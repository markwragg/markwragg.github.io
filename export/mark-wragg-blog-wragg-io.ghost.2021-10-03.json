{
    "db": [
        {
            "meta": {
                "exported_on": 1633269028987,
                "version": "009"
            },
            "data": {
                "posts": [
                    {
                        "id": 6,
                        "uuid": "2f026cbc-5fcb-4f0c-9417-fb38faac9b3f",
                        "title": "Synchronising Git repositories to disk with Chef",
                        "slug": "synchronising-git-repositories-to-disk-with-chef",
                        "markdown": "My team has an ever-growing collection of administration scripts and tools which are organised and maintained within a number of Git repositories. In order to ensure that the copies of these scripts in production always reflect the Master branch in Source Control we set up a Chef cookbook to synchronise the repositories to a network share in the environment. This post details how this functionality was developed and deployed.\n\n**What is Git?**\n\n<div class=\"image-div\" style=\"width: 300px; float: right; margin: 0px 30px 30px 30px;\">\n![XKCD Explains Git](http://imgs.xkcd.com/comics/git.png)\n<p>Image source: <a href=\"https://xkcd.com/1597/\">xkcd.com</a>\n</div>\n\nIn case you're not aware, Git is a version / source control system that has been around since 2005 and is widely used by software developers as a way to manage source code. It allows teams to collaborate on code, ensuring that changes that are made can be tracked and ultimately merged in to the \"master\" branch of the code.\n\nSource Control is typically a relatively new concept for Infrastructure / Operations teams, but there are multiple benefits for working within such a system, not least of which are:\n\n1. Your working behaviour starts to mimic that of your Developer peers, making DevOps collaboration easier and more likely.\n2. All changes are tracked and controlled, making it easy to understand what has changed and roll back those changes.\n3. It encourages the control of infrastructure as code, which ensures a consistent and repeatable architecture.\n\n*-- If Git is new to you, I recommend you follow [this guide](https://guides.github.com/activities/hello-world/) to get to grips with the basic concepts.*\n\n**What is Chef?**\n\nChef is an automation platform that allows you to declare infrastructure as code and have that code applied to systems to ensure they are consistent and compliant (to reach a desired system state and to centralise the control of that state).\n\n*-- If Chef is new to you I recommend you check out the [Windows-based getting started guide](https://learn.chef.io/learn-the-basics/windows/configure-a-resource/).*\n\nIf any of the above is new to you I also strongly recommend you read [The Release Pipeline Model whitepaper](https://msdn.microsoft.com/en-us/powershell/dsc/whitepapers#the-release-pipeline-model) by Steven Murawski and Michael Greene.\n\n**Why use chef to sync git repos?**\n\nOur environment currently lacks a package management/code deployment solution such as [AppVeyor](http://www.appveyor/com) or [Jenkins](https://jenkins.io/) for continuous integration. But we have started to experiment with Chef for configuration management. We are at various levels of maturity with Chef for our products and we're just starting to look at ways to leverage it for our infrastructure. As a result it was a quick-win to use it for this task until we're ready to implement a more suitable solution.\n\n### How it works\nIf you'd like to view the source code and/or reuse this cookbook, you can find it in Git here:\n> https://github.com/markwragg/Chef-GitSync.\n\n**Usage**\n\nThis cookbook is intended to run in a production environment in order to synchronise the master branches of one or more repositories so that when changes are approved and merged they are imminently and automatically promoted to Production. \n\nHowever, as you can configure it to sync any branch you define to any location it could have various other uses.\n\n**The recipe**\n\nYou need to define the following attributes (in attributes/default.rb):\n\n- Repositories: A comma-separated list of URLs to the .git file for each repository\n- Rootpath: The local folder where you want the repositories synced\n- Branch: The name of the branch you want synced\n\nFor example:\n\n```language-ruby\ndefault['stash-to-disk-sync']['repositories'] = ['https://example.com/example.git','https://example2.com/example2.git']\n\ndefault['stash-to-disk-sync']['rootpath'] = 'C:\\\\SourceCode\\\\'\ndefault['stash-to-disk-sync']['branch'] = 'master'\n```\n\nThe recipe then simply loops through the list of repositories and calls Git to sync them to a folder named the same as the repository underneath your defined rootpath and syncing the Branch specified:\n```language-ruby\nnode['stash-to-disk-sync']['repositories'].each do |repositoryname|\n  foldername = repositoryname.scan(%r{([^\\/]+)(?=\\.\\w+$)}).last.first\n\n  git \"#{node['stash-to-disk-sync']['rootpath']}#{foldername}\" do\n    repository repositoryname\n    revision node['stash-to-disk-sync']['branch']\n    action :sync\n  end\nend\n```\n\n### Installation\n\nWe have implemented this using Chef Zero, which allows us to run it without having a Chef Server installation. The below details how we have set this up in our environment but your setup/needs might vary.\n\n> \"Chef Zero is a simple, easy-install, in-memory Chef server that can be useful for Chef Client testing and chef-solo-like tasks that require a full Chef Server. \"\n>\n> Source: https://github.com/chef/chef-zero\n\n**To setup/configure Chef Zero:**\n\n1. Install Chef: https://downloads.chef.io/chef-client/windows/.\n2. Create the following files in C:\\Chef\\ with the contents described below:\n\nC:\\Chef\\ **client.rb**\n\n```language-ruby\ncookbook_path 'C:\\\\Chef\\\\Cookbooks\\\\'\njson_attribs 'C:\\\\Chef\\\\attributes.json'\nchef_zero.enabled true\n```\n\nC:\\Chef\\ **attributes.json**\n\n```language-json\n{\n\t\"run_list\": [\n\t\t\"recipe[chef-client::default]\",\n\t\t\"recipe[git::windows]\",\n\t\t\"recipe[git-to-disk::default]\"\n\t],\n\t\"chef_client\": {\n\t\t\"interval\": \"1800\"\n\t},\n\t\"git-to-disk\": {\n\t\t\"rootpath\": \"\\\\\\\\yourdomain.local\\\\code\\\\\"\n\t}\n\t\t\n}\n```\n**Download the required Cookbooks:**\n\nYou'll now need to download the git-to-disk cookbook and it's dependencies. It requires the git::windows recipe and the chef-client::default recipe, but those each have dependencies on other cookbooks also. As a result the list is quite extensive. \n\nDownload or [Clone](https://git-scm.com/docs/git-clone) each of the following to **C:\\Chef\\Cookbooks** (or to whatever you have specified as your cookbook_path in client.rb):\n\n1. [Chef-GitSync](https://github.com/markwragg/Chef-GitSync).\n- [chef_client](https://github.com/chef-cookbooks/chef-client).\n- [Git](https://github.com/chef-cookbooks/git).\n- [build-essential](https://github.com/chef-cookbooks/build-essential)\n- [chef_handler](https://github.com/chef-cookbooks/chef_handler)\n- [cron](https://github.com/chef-cookbooks/cron)\n- [dmg](https://github.com/chef-cookbooks/dmg)\n- [logrotate](https://github.com/stevendanna/logrotate)\n- [seven_zip](https://github.com/daptiv/seven_zip)\n- [windows](https://github.com/chef-cookbooks/windows)\n- [yum](https://github.com/chef-cookbooks/yum)\n- [yum-epel](https://github.com/chef-cookbooks/yum-epel)\n\n**Execute Chef-Client:**\n\nEnsure the directory exists where you want to clone the repositories to (C:\\Software\\Stash by default). Then to kick off the first run of Chef:\n\n1. Open a **Powershell** or cmd window. \n- Navigate to  to **C:\\Chef\\\\**.\n- Execute **Chef-Client**.\n\n![Chef-Client run of Chef-GitSync](/content/images/2016/06/Chef-Client-Git-Sync1.png)\n\nAfter synchronising the cookbooks, Chef will perform the desired actions. It will check if Chef Client is running as a service, and if it is not will install a service for it. This will then run Chef-Client every 1800 seconds (by default).   \n\nIf a Git Client is not already installed, one will be installed. Git is an obvious dependency as it allows it to clone and pull updates from the repositories, but by having Chef install this it means that if someone were to uninstall Git, Chef would reinstall it automatically on its next run - correcting the desired state: \n\n![](/content/images/2016/06/Chef-Client-Git-Sync2.png)\n\nFinally it will go through each listed repository and use Git to synchronise them to the local folder.\n\n----\nFor my team this is a first small step towards a release pipeline in our environment, which has otherwise been largely governed by business processes. Ultimately this was simple to implement, particularly as we had a pre-existing Source Control to use and because it was possible to access that from our hosting environment. With this in place, we can ensure that any future changes to our automation scripts are only made via Source Control and that those changes are automatically promoted to Production once approved.",
                        "html": "<p>My team has an ever-growing collection of administration scripts and tools which are organised and maintained within a number of Git repositories. In order to ensure that the copies of these scripts in production always reflect the Master branch in Source Control we set up a Chef cookbook to synchronise the repositories to a network share in the environment. This post details how this functionality was developed and deployed.</p>\n\n<p><strong>What is Git?</strong></p>\n\n<div class=\"image-div\" style=\"width: 300px; float: right; margin: 0px 30px 30px 30px;\">  \n<img src=\"http://imgs.xkcd.com/comics/git.png\" alt=\"XKCD Explains Git\" />\n<p>Image source: <a href=\"https://xkcd.com/1597/\">xkcd.com</a>  \n</div>\n\n<p>In case you're not aware, Git is a version / source control system that has been around since 2005 and is widely used by software developers as a way to manage source code. It allows teams to collaborate on code, ensuring that changes that are made can be tracked and ultimately merged in to the \"master\" branch of the code.</p>\n\n<p>Source Control is typically a relatively new concept for Infrastructure / Operations teams, but there are multiple benefits for working within such a system, not least of which are:</p>\n\n<ol>\n<li>Your working behaviour starts to mimic that of your Developer peers, making DevOps collaboration easier and more likely.  </li>\n<li>All changes are tracked and controlled, making it easy to understand what has changed and roll back those changes.  </li>\n<li>It encourages the control of infrastructure as code, which ensures a consistent and repeatable architecture.</li>\n</ol>\n\n<p><em>-- If Git is new to you, I recommend you follow <a href=\"https://guides.github.com/activities/hello-world/\">this guide</a> to get to grips with the basic concepts.</em></p>\n\n<p><strong>What is Chef?</strong></p>\n\n<p>Chef is an automation platform that allows you to declare infrastructure as code and have that code applied to systems to ensure they are consistent and compliant (to reach a desired system state and to centralise the control of that state).</p>\n\n<p><em>-- If Chef is new to you I recommend you check out the <a href=\"https://learn.chef.io/learn-the-basics/windows/configure-a-resource/\">Windows-based getting started guide</a>.</em></p>\n\n<p>If any of the above is new to you I also strongly recommend you read <a href=\"https://msdn.microsoft.com/en-us/powershell/dsc/whitepapers#the-release-pipeline-model\">The Release Pipeline Model whitepaper</a> by Steven Murawski and Michael Greene.</p>\n\n<p><strong>Why use chef to sync git repos?</strong></p>\n\n<p>Our environment currently lacks a package management/code deployment solution such as <a href=\"http://www.appveyor/com\">AppVeyor</a> or <a href=\"https://jenkins.io/\">Jenkins</a> for continuous integration. But we have started to experiment with Chef for configuration management. We are at various levels of maturity with Chef for our products and we're just starting to look at ways to leverage it for our infrastructure. As a result it was a quick-win to use it for this task until we're ready to implement a more suitable solution.</p>\n\n<h3 id=\"howitworks\">How it works</h3>\n\n<p>If you'd like to view the source code and/or reuse this cookbook, you can find it in Git here:  </p>\n\n<blockquote>\n  <p><a href=\"https://github.com/markwragg/Chef-GitSync\">https://github.com/markwragg/Chef-GitSync</a>.</p>\n</blockquote>\n\n<p><strong>Usage</strong></p>\n\n<p>This cookbook is intended to run in a production environment in order to synchronise the master branches of one or more repositories so that when changes are approved and merged they are imminently and automatically promoted to Production. </p>\n\n<p>However, as you can configure it to sync any branch you define to any location it could have various other uses.</p>\n\n<p><strong>The recipe</strong></p>\n\n<p>You need to define the following attributes (in attributes/default.rb):</p>\n\n<ul>\n<li>Repositories: A comma-separated list of URLs to the .git file for each repository</li>\n<li>Rootpath: The local folder where you want the repositories synced</li>\n<li>Branch: The name of the branch you want synced</li>\n</ul>\n\n<p>For example:</p>\n\n<pre><code class=\"language-ruby\">default['stash-to-disk-sync']['repositories'] = ['https://example.com/example.git','https://example2.com/example2.git']\n\ndefault['stash-to-disk-sync']['rootpath'] = 'C:\\\\SourceCode\\\\'  \ndefault['stash-to-disk-sync']['branch'] = 'master'  \n</code></pre>\n\n<p>The recipe then simply loops through the list of repositories and calls Git to sync them to a folder named the same as the repository underneath your defined rootpath and syncing the Branch specified:  </p>\n\n<pre><code class=\"language-ruby\">node['stash-to-disk-sync']['repositories'].each do |repositoryname|  \n  foldername = repositoryname.scan(%r{([^\\/]+)(?=\\.\\w+$)}).last.first\n\n  git \"#{node['stash-to-disk-sync']['rootpath']}#{foldername}\" do\n    repository repositoryname\n    revision node['stash-to-disk-sync']['branch']\n    action :sync\n  end\nend  \n</code></pre>\n\n<h3 id=\"installation\">Installation</h3>\n\n<p>We have implemented this using Chef Zero, which allows us to run it without having a Chef Server installation. The below details how we have set this up in our environment but your setup/needs might vary.</p>\n\n<blockquote>\n  <p>\"Chef Zero is a simple, easy-install, in-memory Chef server that can be useful for Chef Client testing and chef-solo-like tasks that require a full Chef Server. \"</p>\n  \n  <p>Source: <a href=\"https://github.com/chef/chef-zero\">https://github.com/chef/chef-zero</a></p>\n</blockquote>\n\n<p><strong>To setup/configure Chef Zero:</strong></p>\n\n<ol>\n<li>Install Chef: <a href=\"https://downloads.chef.io/chef-client/windows/\">https://downloads.chef.io/chef-client/windows/</a>.  </li>\n<li>Create the following files in C:\\Chef\\ with the contents described below:</li>\n</ol>\n\n<p>C:\\Chef\\ <strong>client.rb</strong></p>\n\n<pre><code class=\"language-ruby\">cookbook_path 'C:\\\\Chef\\\\Cookbooks\\\\'  \njson_attribs 'C:\\\\Chef\\\\attributes.json'  \nchef_zero.enabled true  \n</code></pre>\n\n<p>C:\\Chef\\ <strong>attributes.json</strong></p>\n\n<pre><code class=\"language-json\">{\n    \"run_list\": [\n        \"recipe[chef-client::default]\",\n        \"recipe[git::windows]\",\n        \"recipe[git-to-disk::default]\"\n    ],\n    \"chef_client\": {\n        \"interval\": \"1800\"\n    },\n    \"git-to-disk\": {\n        \"rootpath\": \"\\\\\\\\yourdomain.local\\\\code\\\\\"\n    }\n\n}\n</code></pre>\n\n<p><strong>Download the required Cookbooks:</strong></p>\n\n<p>You'll now need to download the git-to-disk cookbook and it's dependencies. It requires the git::windows recipe and the chef-client::default recipe, but those each have dependencies on other cookbooks also. As a result the list is quite extensive. </p>\n\n<p>Download or <a href=\"https://git-scm.com/docs/git-clone\">Clone</a> each of the following to <strong>C:\\Chef\\Cookbooks</strong> (or to whatever you have specified as your cookbook_path in client.rb):</p>\n\n<ol>\n<li><a href=\"https://github.com/markwragg/Chef-GitSync\">Chef-GitSync</a>.  </li>\n<li><a href=\"https://github.com/chef-cookbooks/chef-client\">chef_client</a>.</li>\n<li><a href=\"https://github.com/chef-cookbooks/git\">Git</a>.</li>\n<li><a href=\"https://github.com/chef-cookbooks/build-essential\">build-essential</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/chef_handler\">chef_handler</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/cron\">cron</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/dmg\">dmg</a></li>\n<li><a href=\"https://github.com/stevendanna/logrotate\">logrotate</a></li>\n<li><a href=\"https://github.com/daptiv/seven_zip\">seven_zip</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/windows\">windows</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/yum\">yum</a></li>\n<li><a href=\"https://github.com/chef-cookbooks/yum-epel\">yum-epel</a></li>\n</ol>\n\n<p><strong>Execute Chef-Client:</strong></p>\n\n<p>Ensure the directory exists where you want to clone the repositories to (C:\\Software\\Stash by default). Then to kick off the first run of Chef:</p>\n\n<ol>\n<li>Open a <strong>Powershell</strong> or cmd window.  </li>\n<li>Navigate to  to <strong>C:\\Chef\\</strong>.</li>\n<li>Execute <strong>Chef-Client</strong>.</li>\n</ol>\n\n<p><img src=\"/content/images/2016/06/Chef-Client-Git-Sync1.png\" alt=\"Chef-Client run of Chef-GitSync\" /></p>\n\n<p>After synchronising the cookbooks, Chef will perform the desired actions. It will check if Chef Client is running as a service, and if it is not will install a service for it. This will then run Chef-Client every 1800 seconds (by default).   </p>\n\n<p>If a Git Client is not already installed, one will be installed. Git is an obvious dependency as it allows it to clone and pull updates from the repositories, but by having Chef install this it means that if someone were to uninstall Git, Chef would reinstall it automatically on its next run - correcting the desired state: </p>\n\n<p><img src=\"/content/images/2016/06/Chef-Client-Git-Sync2.png\" alt=\"\" /></p>\n\n<p>Finally it will go through each listed repository and use Git to synchronise them to the local folder.</p>\n\n<hr />\n\n<p>For my team this is a first small step towards a release pipeline in our environment, which has otherwise been largely governed by business processes. Ultimately this was simple to implement, particularly as we had a pre-existing Source Control to use and because it was possible to access that from our hosting environment. With this in place, we can ensure that any future changes to our automation scripts are only made via Source Control and that those changes are automatically promoted to Production once approved.</p>",
                        "image": "/content/images/2016/06/Chef-Git-Logo-6-1.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Synchronise Git repositories to disk with Chef Cookbook",
                        "meta_description": "This blog post describes how a chef cookbook can be used to keep one or more Git repositories synchronised to a Windows server.",
                        "author_id": 1,
                        "created_at": "2016-05-02 16:43:48",
                        "created_by": 1,
                        "updated_at": "2016-07-08 12:59:57",
                        "updated_by": 1,
                        "published_at": "2016-07-08 12:49:29",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 7,
                        "uuid": "a5f92096-4bd5-4739-a8ce-e89611637cc0",
                        "title": "Subscribe",
                        "slug": "subscribe",
                        "markdown": "If you would like to subscribe to posts from this blog use one of the following options.\n\n## Full RSS feed\n\nTo add all posts from this blog to an RSS reader simply use the following link: \n\n- http://wragg.io/rss/\n\n## Topic specific RSS feed\n\nIf you would like to subscribe to one or more specific topics, modify the above link to be:\n\n- `http://wragg.io/tag/` **[yourtag]** `/rss`\n\nFor example:\n\n- http://wragg.io/tag/aws/rss\n- http://wragg.io/tag/powershell/rss",
                        "html": "<p>If you would like to subscribe to posts from this blog use one of the following options.</p>\n\n<h2 id=\"fullrssfeed\">Full RSS feed</h2>\n\n<p>To add all posts from this blog to an RSS reader simply use the following link: </p>\n\n<ul>\n<li><a href=\"http://wragg.io/rss/\">http://wragg.io/rss/</a></li>\n</ul>\n\n<h2 id=\"topicspecificrssfeed\">Topic specific RSS feed</h2>\n\n<p>If you would like to subscribe to one or more specific topics, modify the above link to be:</p>\n\n<ul>\n<li><code>http://wragg.io/tag/</code> <strong>[yourtag]</strong> <code>/rss</code></li>\n</ul>\n\n<p>For example:</p>\n\n<ul>\n<li><a href=\"http://wragg.io/tag/aws/rss\">http://wragg.io/tag/aws/rss</a></li>\n<li><a href=\"http://wragg.io/tag/powershell/rss\">http://wragg.io/tag/powershell/rss</a></li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 1,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-02 20:39:06",
                        "created_by": 1,
                        "updated_at": "2016-05-03 13:31:21",
                        "updated_by": 1,
                        "published_at": "2016-05-01 12:30:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 8,
                        "uuid": "9c8b7a27-04bc-426e-8360-f71b10152166",
                        "title": "Send notifications to Hipchat with Powershell #ChatOps",
                        "slug": "send-notifications-to-hipchat-with-powershell",
                        "markdown": "I recently implemented a Powershell module to send notifications in to our Hipchat rooms. This post explains how that script works and why this was an important shift for how we handle notifications.\n\nA number of our administrative scripts were historically configured to send emails when they performed key activities to ensure these automations were visible. As with most teams, we receive a lot of email so I suspect (like me) most of the team eventually configured rules to have these emails auto-filed, hiding them and negating the intended effect. \n\nWhile most teams are probably already utilising some form of electronic chat mechanism  (~~Lync~~ Skype for Business etc.) these tools are typically designed to just permit communication. Hipchat (and other tools like it such as Slack) extend this by allowing integrations to be built that make the chat tool more powerful, useful and unavoidably central to how operations occur. Sending notifications in to chat rooms is the most basic of these integrations.\n\n![Hipchat](/content/images/2016/06/HipchatShot-wide-1.png)\n\n*-- These tools are an enabler for a cultural movement known as \"[ChatOps](http://blogs.atlassian.com/2016/01/what-is-chatops-adoption-guide/)\". [Popularised by Github](https://www.youtube.com/watch?v=NST3u-GjjFw), the idea is that the chat room becomes a place where work is not only discussed but occurs and where the result of that work can then be known by the entire team.*\n\nThere are several existing third-party Powershell modules available for sending notifications to Hipchat, but I was interested in the challenge of developing my own. Hipchat themselves provided guidance for using Powershell for sending notifications, but the guidance was based around version 1 of their API, which is now deprecated. My module is named Powershell-HipChat and is available on Github here: \n\n> https://github.com/markwragg/Powershell-Hipchat\n\nThe code operates via a function named `send-hipchat` which is included in the Powershell module file: hipchat.psm1 (which also comes with a Module Manifest psd1 file). \n\nBefore you can make use of the function you'll need two things from Hipchat: the name or ID of the room you want to send notifications in to and an API token for that room.\n\n**To get the ID or name of your room:**\n\n1. Log in to the Hipchat web interface and browse to \"Rooms\". If you're not self-hosting, that will be https://yourname.hipchat.com/rooms\n2. Click on the room you want. You can use the friendly name of that room if you wish (ensure you encapsulate it in speech marks if it includes spaces) but I recommend you instead use the ID of the room, which you see on the Summary page listed as \"API ID\" as well as in address bar for that page. By using the ID you won't be affected if the room is renamed.\n\n**To get an API token:**\n\n*Beware that the script uses version 2 of the API, and you cannot use version 1 API tokens. The version 1 API tokens were global, where as version 2 tokens are specifically created for each individual room.* \n\n1. Within the room's page (per the above steps) navigate to \"Tokens\".\n2. Complete the \"Create a new token\" form. The \"Label\" you choose will appear with each notification, so you may want to use it to describe the source activity of the notification.\n3. The scope should be \"Send notification\".\n\nIf you want to send messages to multiple rooms, you must create a token for each room. You can also create multiple tokens per room, which you can then label for different purposes.\n\n**To use the function:**\n\n1. [Download/clone the files](https://github.com/markwragg/Powershell-Hipchat) to where you wish to use them.\n2. Load the module via the manifest with `Import-module hipchat.psd1`\n3. Execute the command (or include it in your script), e.g: `send-hipchat -message \"It was the best of times. It was the worst of times.\" -color \"random\" -apitoken <your apitoken> -room <your room id> -retry 5 -retrysecs 30`\n\nHere's the result in Hipchat:\n\n![Hipchat notification examples](/content/images/2016/06/Hipchat-Notification-Examples-1.png)\n\n## The code\n\nAfter an initial implementation that relied on cmdlets available from Powershell 3 and newer, I discovered that some of our environment was still running on Powershell 2. While I intend to upgrade those servers in the near future, in the interim I updated the script to remove those dependencies. That does add quite a lot of bloat, so I've made it somewhat explicit where that has occurred so that anyone who is sure that they are running Powershell 3 or above can easily strip out the Powershell 2 sections.\n\nThe first of those is that Powershell 2 does not have a convertto-json function, but after some googling I was able to retro-fit one via the below function that is only declared if the version is less than Powershell 3:\n\n```language-powershell\nif ($PSVersionTable.PSVersion.Major -lt 3 ){\n          \n    function ConvertTo-Json([object] $item){\n        add-type -assembly system.web.extensions\n        $ps_js=new-object system.web.script.serialization.javascriptSerializer\n        return $ps_js.Serialize($item)\n    }\n\n}\n```\n*--- The above function (or the PS3+ cmdlet) is needed because Hipchat requires the message it receives to be formatted as JSON.*\n\nAfter this, the rest of the module is the `send-hipchat` function:\n\n```language-powershell\nfunction Send-Hipchat {\n    [CmdletBinding()]\n```\nThe below parameters have validation and sensible defaults where it seemed appropriate. For example there is only a specific list of colours that are valid.\n\nI added retry functionality to the script because my initial testing was a bit inconsistent in delivering the message first time, but this may well have been as a result of my infrastructure rather than a failing on the part of Hipchat. I left this functionality in in case it was useful in the future, but by default retry = 0 which means it will only try to deliver it once. If you do set a retry, you can configure the delay between tries with retrysecs (30 by default):\n```language-powershell\n    Param(\n        [Parameter(Mandatory = $True)][string]$message,\n        [ValidateSet('yellow', 'green', 'red', 'purple', 'gray','random')][string]$color = 'gray',\n        [switch]$notify,\n        [Parameter(Mandatory = $True)][string]$apitoken,   \n        [Parameter(Mandatory = $True)][string]$room,\n        [int]$retry = 0,\n        [int]$retrysecs = 30\n    )\n```\nThis next part declares a hash table of your notification parameters. The Notify parameter defines whether active users in the room get a notification (i.e an unread message indicator, or other notification per their preferences) that the message has been added to the room. If notify is set to false, the message is effectively silent.\n```language-powershell\n    $messageObj = @{\n        \"message\" = $message;\n        \"color\" = $color;\n        \"notify\" = [string]$notify\n    }\n```\nThe API URL for Hipchat is standard unless you are self-hosting. If you are, change the URL below:\n```language-powershell        \n    $uri = \"https://api.hipchat.com/v2/room/$room/notification?auth_token=$apitoken\"\n    $Body = ConvertTo-Json $messageObj\n    $Post = [System.Text.Encoding]::UTF8.GetBytes($Body)\n        \n    $Retrycount = 0\n    \n    While($RetryCount -le $retry){\n\t    try {\n```\nIf Powershell 3 or above is available, delivering the message is a one line use of `invoke-webrequest`. If not, it's a slightly more complicated case of using a .NET equivalent:\n```language-powershell\nif ($PSVersionTable.PSVersion.Major -gt 2 ){\n   $Response = Invoke-WebRequest -Method Post -Uri $uri -Body $Body -ContentType \"application/json\"\n}else{\n   $Request = [System.Net.WebRequest]::Create($uri)\n   $Request.ContentType = \"application/json\"\n   $Request.ContentLength = $Post.Length\n   $Request.Method = \"POST\"\n\n   $requestStream = $Request.GetRequestStream()\n   $requestStream.Write($Post, 0,$Post.length)\n   $requestStream.Close()\n\n   $Response = $Request.GetResponse()\n   $stream = New-Object IO.StreamReader($Response.GetResponseStream(), $Response.ContentEncoding)\n   $content = $stream.ReadToEnd()\n   $stream.Close()\n   $Response.Close()\n}\nWrite-Verbose \"'$message' sent!\"\nReturn $true\n```\nIf any error has occurred, we report it to the console and if retry has been set, wait retrysecs before looping. The If logic below ensures we don't wait after the last attempt:\n```language-powershell\n        } catch {\n            Write-Error \"Could not send message: `r`n $_.Exception.ToString()\"\n\n             If ($retrycount -lt $retry){\n                Write-Verbose \"retrying in $retrysecs seconds...\"\n                Start-Sleep -Seconds $retrysecs\n            }\n        }\n        $Retrycount++\n    }\n```\nIf we haven't successfully delivered after the maximum retries we inform the user via the console and `return $false`. This is is important as it allows logic to be built around the function in the calling script to determine if it was successful (and to perhaps fall back to email notification if it was not).\n\nNote the below lines don't get executed when we're successful as the earlier `return $true` ends the script immediately.\n```    \n    Write-Verbose \"Could not send after $Retrycount tries. I quit.\"\n    Return $false\n}\n```\n\n## Usage suggestions\n\nYour use cases will vary depending on what suits your environment. It's a good idea to push out notifications from any automation that is affecting change in your environment to ensure these changes are visible to appropriate audiences. Another way in which notifications are commonly used as part of ChatOps is to inform the team on the progress of the automated build/test process of releases.\n\nHere's some automations that we've embedded notifications in to:\n\n- **Windows updates:** We use a script in our environment that triggers Windows updates (rather than directly by Windows) because it allows us to also temporarily pause monitoring. A HipChat notification lets us see exactly when each server has rebooted, so if there is a negative effect we immediately know why.\n- **Software customisations:** Some of our software is customised via a standard installer and our Centralised Operations team can schedule these installations around when customer's want them to occur. We've added notifications that tell us when an installation has been scheduled and then that sends or a red or green notification after the installation reporting failure or success.\n- **Storage vMotions:** It's occasionally necessary to relocate multiple Virtual Machines between datastores. To ensure this doesn't cause disruption we have a script (thanks to [Sam Martin](http://sammart.in) that moves a list of VMs one at a time (saving us having to trigger each move manually). To monitor the progress of this each move drops a notification to Hipchat.",
                        "html": "<p>I recently implemented a Powershell module to send notifications in to our Hipchat rooms. This post explains how that script works and why this was an important shift for how we handle notifications.</p>\n\n<p>A number of our administrative scripts were historically configured to send emails when they performed key activities to ensure these automations were visible. As with most teams, we receive a lot of email so I suspect (like me) most of the team eventually configured rules to have these emails auto-filed, hiding them and negating the intended effect. </p>\n\n<p>While most teams are probably already utilising some form of electronic chat mechanism  (<del>Lync</del> Skype for Business etc.) these tools are typically designed to just permit communication. Hipchat (and other tools like it such as Slack) extend this by allowing integrations to be built that make the chat tool more powerful, useful and unavoidably central to how operations occur. Sending notifications in to chat rooms is the most basic of these integrations.</p>\n\n<p><img src=\"/content/images/2016/06/HipchatShot-wide-1.png\" alt=\"Hipchat\" /></p>\n\n<p><em>-- These tools are an enabler for a cultural movement known as \"<a href=\"http://blogs.atlassian.com/2016/01/what-is-chatops-adoption-guide/\">ChatOps</a>\". <a href=\"https://www.youtube.com/watch?v=NST3u-GjjFw\">Popularised by Github</a>, the idea is that the chat room becomes a place where work is not only discussed but occurs and where the result of that work can then be known by the entire team.</em></p>\n\n<p>There are several existing third-party Powershell modules available for sending notifications to Hipchat, but I was interested in the challenge of developing my own. Hipchat themselves provided guidance for using Powershell for sending notifications, but the guidance was based around version 1 of their API, which is now deprecated. My module is named Powershell-HipChat and is available on Github here: </p>\n\n<blockquote>\n  <p><a href=\"https://github.com/markwragg/Powershell-Hipchat\">https://github.com/markwragg/Powershell-Hipchat</a></p>\n</blockquote>\n\n<p>The code operates via a function named <code>send-hipchat</code> which is included in the Powershell module file: hipchat.psm1 (which also comes with a Module Manifest psd1 file). </p>\n\n<p>Before you can make use of the function you'll need two things from Hipchat: the name or ID of the room you want to send notifications in to and an API token for that room.</p>\n\n<p><strong>To get the ID or name of your room:</strong></p>\n\n<ol>\n<li>Log in to the Hipchat web interface and browse to \"Rooms\". If you're not self-hosting, that will be <a href=\"https://yourname.hipchat.com/rooms\">https://yourname.hipchat.com/rooms</a>  </li>\n<li>Click on the room you want. You can use the friendly name of that room if you wish (ensure you encapsulate it in speech marks if it includes spaces) but I recommend you instead use the ID of the room, which you see on the Summary page listed as \"API ID\" as well as in address bar for that page. By using the ID you won't be affected if the room is renamed.</li>\n</ol>\n\n<p><strong>To get an API token:</strong></p>\n\n<p><em>Beware that the script uses version 2 of the API, and you cannot use version 1 API tokens. The version 1 API tokens were global, where as version 2 tokens are specifically created for each individual room.</em> </p>\n\n<ol>\n<li>Within the room's page (per the above steps) navigate to \"Tokens\".  </li>\n<li>Complete the \"Create a new token\" form. The \"Label\" you choose will appear with each notification, so you may want to use it to describe the source activity of the notification.  </li>\n<li>The scope should be \"Send notification\".</li>\n</ol>\n\n<p>If you want to send messages to multiple rooms, you must create a token for each room. You can also create multiple tokens per room, which you can then label for different purposes.</p>\n\n<p><strong>To use the function:</strong></p>\n\n<ol>\n<li><a href=\"https://github.com/markwragg/Powershell-Hipchat\">Download/clone the files</a> to where you wish to use them.  </li>\n<li>Load the module via the manifest with <code>Import-module hipchat.psd1</code>  </li>\n<li>Execute the command (or include it in your script), e.g: <code>send-hipchat -message \"It was the best of times. It was the worst of times.\" -color \"random\" -apitoken &lt;your apitoken&gt; -room &lt;your room id&gt; -retry 5 -retrysecs 30</code></li>\n</ol>\n\n<p>Here's the result in Hipchat:</p>\n\n<p><img src=\"/content/images/2016/06/Hipchat-Notification-Examples-1.png\" alt=\"Hipchat notification examples\" /></p>\n\n<h2 id=\"thecode\">The code</h2>\n\n<p>After an initial implementation that relied on cmdlets available from Powershell 3 and newer, I discovered that some of our environment was still running on Powershell 2. While I intend to upgrade those servers in the near future, in the interim I updated the script to remove those dependencies. That does add quite a lot of bloat, so I've made it somewhat explicit where that has occurred so that anyone who is sure that they are running Powershell 3 or above can easily strip out the Powershell 2 sections.</p>\n\n<p>The first of those is that Powershell 2 does not have a convertto-json function, but after some googling I was able to retro-fit one via the below function that is only declared if the version is less than Powershell 3:</p>\n\n<pre><code class=\"language-powershell\">if ($PSVersionTable.PSVersion.Major -lt 3 ){\n\n    function ConvertTo-Json([object] $item){\n        add-type -assembly system.web.extensions\n        $ps_js=new-object system.web.script.serialization.javascriptSerializer\n        return $ps_js.Serialize($item)\n    }\n\n}\n</code></pre>\n\n<p><em>--- The above function (or the PS3+ cmdlet) is needed because Hipchat requires the message it receives to be formatted as JSON.</em></p>\n\n<p>After this, the rest of the module is the <code>send-hipchat</code> function:</p>\n\n<pre><code class=\"language-powershell\">function Send-Hipchat {  \n    [CmdletBinding()]\n</code></pre>\n\n<p>The below parameters have validation and sensible defaults where it seemed appropriate. For example there is only a specific list of colours that are valid.</p>\n\n<p>I added retry functionality to the script because my initial testing was a bit inconsistent in delivering the message first time, but this may well have been as a result of my infrastructure rather than a failing on the part of Hipchat. I left this functionality in in case it was useful in the future, but by default retry = 0 which means it will only try to deliver it once. If you do set a retry, you can configure the delay between tries with retrysecs (30 by default):  </p>\n\n<pre><code class=\"language-powershell\">    Param(\n        [Parameter(Mandatory = $True)][string]$message,\n        [ValidateSet('yellow', 'green', 'red', 'purple', 'gray','random')][string]$color = 'gray',\n        [switch]$notify,\n        [Parameter(Mandatory = $True)][string]$apitoken,   \n        [Parameter(Mandatory = $True)][string]$room,\n        [int]$retry = 0,\n        [int]$retrysecs = 30\n    )\n</code></pre>\n\n<p>This next part declares a hash table of your notification parameters. The Notify parameter defines whether active users in the room get a notification (i.e an unread message indicator, or other notification per their preferences) that the message has been added to the room. If notify is set to false, the message is effectively silent.  </p>\n\n<pre><code class=\"language-powershell\">    $messageObj = @{\n        \"message\" = $message;\n        \"color\" = $color;\n        \"notify\" = [string]$notify\n    }\n</code></pre>\n\n<p>The API URL for Hipchat is standard unless you are self-hosting. If you are, change the URL below:  </p>\n\n<pre><code class=\"language-powershell        \">    $uri = \"https://api.hipchat.com/v2/room/$room/notification?auth_token=$apitoken\"\n    $Body = ConvertTo-Json $messageObj\n    $Post = [System.Text.Encoding]::UTF8.GetBytes($Body)\n\n    $Retrycount = 0\n\n    While($RetryCount -le $retry){\n        try {\n</code></pre>\n\n<p>If Powershell 3 or above is available, delivering the message is a one line use of <code>invoke-webrequest</code>. If not, it's a slightly more complicated case of using a .NET equivalent:  </p>\n\n<pre><code class=\"language-powershell\">if ($PSVersionTable.PSVersion.Major -gt 2 ){  \n   $Response = Invoke-WebRequest -Method Post -Uri $uri -Body $Body -ContentType \"application/json\"\n}else{\n   $Request = [System.Net.WebRequest]::Create($uri)\n   $Request.ContentType = \"application/json\"\n   $Request.ContentLength = $Post.Length\n   $Request.Method = \"POST\"\n\n   $requestStream = $Request.GetRequestStream()\n   $requestStream.Write($Post, 0,$Post.length)\n   $requestStream.Close()\n\n   $Response = $Request.GetResponse()\n   $stream = New-Object IO.StreamReader($Response.GetResponseStream(), $Response.ContentEncoding)\n   $content = $stream.ReadToEnd()\n   $stream.Close()\n   $Response.Close()\n}\nWrite-Verbose \"'$message' sent!\"  \nReturn $true  \n</code></pre>\n\n<p>If any error has occurred, we report it to the console and if retry has been set, wait retrysecs before looping. The If logic below ensures we don't wait after the last attempt:  </p>\n\n<pre><code class=\"language-powershell\">        } catch {\n            Write-Error \"Could not send message: `r`n $_.Exception.ToString()\"\n\n             If ($retrycount -lt $retry){\n                Write-Verbose \"retrying in $retrysecs seconds...\"\n                Start-Sleep -Seconds $retrysecs\n            }\n        }\n        $Retrycount++\n    }\n</code></pre>\n\n<p>If we haven't successfully delivered after the maximum retries we inform the user via the console and <code>return $false</code>. This is is important as it allows logic to be built around the function in the calling script to determine if it was successful (and to perhaps fall back to email notification if it was not).</p>\n\n<p>Note the below lines don't get executed when we're successful as the earlier <code>return $true</code> ends the script immediately.  </p>\n\n<pre><code class=\"language-    \">    Write-Verbose \"Could not send after $Retrycount tries. I quit.\"\n    Return $false\n}\n</code></pre>\n\n<h2 id=\"usagesuggestions\">Usage suggestions</h2>\n\n<p>Your use cases will vary depending on what suits your environment. It's a good idea to push out notifications from any automation that is affecting change in your environment to ensure these changes are visible to appropriate audiences. Another way in which notifications are commonly used as part of ChatOps is to inform the team on the progress of the automated build/test process of releases.</p>\n\n<p>Here's some automations that we've embedded notifications in to:</p>\n\n<ul>\n<li><strong>Windows updates:</strong> We use a script in our environment that triggers Windows updates (rather than directly by Windows) because it allows us to also temporarily pause monitoring. A HipChat notification lets us see exactly when each server has rebooted, so if there is a negative effect we immediately know why.</li>\n<li><strong>Software customisations:</strong> Some of our software is customised via a standard installer and our Centralised Operations team can schedule these installations around when customer's want them to occur. We've added notifications that tell us when an installation has been scheduled and then that sends or a red or green notification after the installation reporting failure or success.</li>\n<li><strong>Storage vMotions:</strong> It's occasionally necessary to relocate multiple Virtual Machines between datastores. To ensure this doesn't cause disruption we have a script (thanks to <a href=\"http://sammart.in\">Sam Martin</a> that moves a list of VMs one at a time (saving us having to trigger each move manually). To monitor the progress of this each move drops a notification to Hipchat.</li>\n</ul>",
                        "image": "/content/images/2016/06/hc.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-02 21:55:09",
                        "created_by": 1,
                        "updated_at": "2017-02-28 15:23:00",
                        "updated_by": 1,
                        "published_at": "2016-06-04 13:32:51",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 9,
                        "uuid": "7b017619-0177-471d-a714-8749a3957944",
                        "title": "Source control in operations and the basics of Git",
                        "slug": "source-control-in-operations-and-the-basics-of-git",
                        "markdown": "It is quickly becoming apparent that implementing and using a source control system is an important skill to master. This has long been true in development, but within (Windows) Operations it is (seemingly for many) a relatively new or uknown concept. Many organisations have a change control policy and process which demands that changes be recorded and approved before implemented. However its still generally possible for this policy to be bypassed (deliberately or accidentally). In times of crisis it's often the first thing to go out the window.\n\nFor source control in Operations to be relevant, we need to ensure that changes in the environment are only possible via the source control system. It must not be possible to make changes to the servers directly (or at the very least, it must not be possible to make the changes we've source controlled directly).\n\n## Benefits of source control to Ops\n- Centralised configuration, a precursor to enabling configuration management/auto scaling technologies\n- The ability to see who committed changes, exactly what those changes were and to easily revert changes to the previous revision of the configuration if needed.\n\nGit - \"a version control system that is widely used for software development and other version control tasks\"\n\nThe most widely used system for managing and collaborating on source code is Git. This blog post describes my understanding of some of the basic concepts.\n\nIf you are new to Git yourself, I strongly recommend you follow this guide which takes you through the basic concepts: https://guides.github.com/activities/hello-world/",
                        "html": "<p>It is quickly becoming apparent that implementing and using a source control system is an important skill to master. This has long been true in development, but within (Windows) Operations it is (seemingly for many) a relatively new or uknown concept. Many organisations have a change control policy and process which demands that changes be recorded and approved before implemented. However its still generally possible for this policy to be bypassed (deliberately or accidentally). In times of crisis it's often the first thing to go out the window.</p>\n\n<p>For source control in Operations to be relevant, we need to ensure that changes in the environment are only possible via the source control system. It must not be possible to make changes to the servers directly (or at the very least, it must not be possible to make the changes we've source controlled directly).</p>\n\n<h2 id=\"benefitsofsourcecontroltoops\">Benefits of source control to Ops</h2>\n\n<ul>\n<li>Centralised configuration, a precursor to enabling configuration management/auto scaling technologies</li>\n<li>The ability to see who committed changes, exactly what those changes were and to easily revert changes to the previous revision of the configuration if needed.</li>\n</ul>\n\n<p>Git - \"a version control system that is widely used for software development and other version control tasks\"</p>\n\n<p>The most widely used system for managing and collaborating on source code is Git. This blog post describes my understanding of some of the basic concepts.</p>\n\n<p>If you are new to Git yourself, I strongly recommend you follow this guide which takes you through the basic concepts: <a href=\"https://guides.github.com/activities/hello-world/\">https://guides.github.com/activities/hello-world/</a></p>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-02 21:59:27",
                        "created_by": 1,
                        "updated_at": "2017-02-23 12:49:31",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 10,
                        "uuid": "1b0d1673-3fd0-4a4e-83b0-3cd6191f7e44",
                        "title": "Operational metrics",
                        "slug": "operational-metrics",
                        "markdown": "How should we measure success in Operations? Why is it important to track metrics? What metrics are informative and what metrics are not? What are traps/pitfalls? We need to be careful not to assume doing more with less is necessarily an indicator of success. How do you measure quality?\n\nNumber of 24x7 paging alerts is a good starting point, per recent tweet from John Arundel https://twitter.com/bitfield/status/726344907741401088\n\n(you can display a tweet in line)\nhttps://support.twitter.com/articles/20169559\n\nWhat metrics are dangerous?\nNumber of tickets\nTime taken to resolve tickets (without thought to type/categorisation)\nNumber of changes - rate of change is not a measure of success\nChange success?? Depends on the cause of failure. What constitutes a failed changed - topic for another post.\n\nGood metrics\nCommits to source control? % of configuration in source control? Hard to get a firm number on this tho. Oh % of changes made in source control vs non source control?\n%age of tasks automated (full, partial) vs manual\nPercentage of production on latest version\nTime between builds\nTime between release and production \nTurnover\nCapability\nResponse time\nTime taken to complete builds\nTime taken to complete upgrades\nUptime\nMean time to recovery\nProject tasks completed or % of time devoted to projects?\nCustomer ticket volume -- not total tickets but cust incidents indicates how much pain customers suffer \n\n\nhttps://www.pagerduty.com/blog/operational-metrics/\nRaw incident count\nmttr\nTime to respond (in hrs / out hrs?)\nEscalations -- definitely want to start counting these \n\nMetrics must be\nImportant to the business\nActionable\nMeasured frequently\nRelevant to the audience \n\nDashboards must be\nSimple to interpret\nProvide context\n\nhttps://www.klipfolio.com/resources/articles/kpi-dashboard-operational-metrics-top-10-guidelines\n\nITIL suggests measuring processes in two ways: quantitative and qualitative. Quantity is easy, it's things like how many tickets, how many failed changes. Quality is harder, often requiring you to speak to people on their perception of the service.\n\nQuality measures:\n\nAssess and score the quality of a random sample of changes.\nAssess and score the quality of a random sample of incidents.\nAssess a build request, whether it was delivered on time and how many updates were provided : could do this one programmatically.\n\nCould script the selection of sample tickets.\n\n",
                        "html": "<p>How should we measure success in Operations? Why is it important to track metrics? What metrics are informative and what metrics are not? What are traps/pitfalls? We need to be careful not to assume doing more with less is necessarily an indicator of success. How do you measure quality?</p>\n\n<p>Number of 24x7 paging alerts is a good starting point, per recent tweet from John Arundel <a href=\"https://twitter.com/bitfield/status/726344907741401088\">https://twitter.com/bitfield/status/726344907741401088</a></p>\n\n<p>(you can display a tweet in line)\n<a href=\"https://support.twitter.com/articles/20169559\">https://support.twitter.com/articles/20169559</a></p>\n\n<p>What metrics are dangerous? <br />\nNumber of tickets <br />\nTime taken to resolve tickets (without thought to type/categorisation) <br />\nNumber of changes - rate of change is not a measure of success <br />\nChange success?? Depends on the cause of failure. What constitutes a failed changed - topic for another post.</p>\n\n<p>Good metrics <br />\nCommits to source control? % of configuration in source control? Hard to get a firm number on this tho. Oh % of changes made in source control vs non source control? <br />\n%age of tasks automated (full, partial) vs manual\nPercentage of production on latest version <br />\nTime between builds <br />\nTime between release and production <br />\nTurnover <br />\nCapability <br />\nResponse time <br />\nTime taken to complete builds <br />\nTime taken to complete upgrades <br />\nUptime <br />\nMean time to recovery <br />\nProject tasks completed or % of time devoted to projects? <br />\nCustomer ticket volume -- not total tickets but cust incidents indicates how much pain customers suffer </p>\n\n<p><a href=\"https://www.pagerduty.com/blog/operational-metrics/\">https://www.pagerduty.com/blog/operational-metrics/</a> <br />\nRaw incident count <br />\nmttr <br />\nTime to respond (in hrs / out hrs?) <br />\nEscalations -- definitely want to start counting these </p>\n\n<p>Metrics must be <br />\nImportant to the business <br />\nActionable <br />\nMeasured frequently <br />\nRelevant to the audience </p>\n\n<p>Dashboards must be <br />\nSimple to interpret <br />\nProvide context</p>\n\n<p><a href=\"https://www.klipfolio.com/resources/articles/kpi-dashboard-operational-metrics-top-10-guidelines\">https://www.klipfolio.com/resources/articles/kpi-dashboard-operational-metrics-top-10-guidelines</a></p>\n\n<p>ITIL suggests measuring processes in two ways: quantitative and qualitative. Quantity is easy, it's things like how many tickets, how many failed changes. Quality is harder, often requiring you to speak to people on their perception of the service.</p>\n\n<p>Quality measures:</p>\n\n<p>Assess and score the quality of a random sample of changes. <br />\nAssess and score the quality of a random sample of incidents. <br />\nAssess a build request, whether it was delivered on time and how many updates were provided : could do this one programmatically.</p>\n\n<p>Could script the selection of sample tickets.</p>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-02 22:00:04",
                        "created_by": 1,
                        "updated_at": "2016-09-05 21:54:59",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 11,
                        "uuid": "c89bdbe8-a281-4c60-86c5-74b2d6633e2a",
                        "title": "A PowerShell stopwatch for your Profile.ps1",
                        "slug": "powershell-stopwatch",
                        "markdown": "[Boe Prox](https://twitter.com/proxb) blogged/tweeted recently about [how you can start a stopwatch in PowerShell with one line](https://learn-powershell.net/2016/04/29/quick-hits-create-and-start-a-stopwatch-in-one-line/) using the command `[System.Diagnostics.Stopwatch]::StartNew()`. While there are plenty of other ways to measure time in PowerShell (e.g measure-object, new-timespan), I could see an adhoc stopwatch being useful on occasion. However the raw class name is a bit clunky and forgettable. Therefore as a bit of fun I have created it as a series of functions in my PowerShell Profile.ps1 with friendly names I could call when needed.\n\n-- *The irony of the fact that I've turned his one-liner in to approx. 30 lines of code is not lost on me btw.*\n\n---\n\n- In case you're not aware, the PowerShell profile is a place to store functions or commands that you want executed or available automatically each time you start PowerShell. There are actually [several different kinds of Profile](https://blogs.technet.microsoft.com/heyscriptingguy/2012/05/21/understanding-the-six-powershell-profiles/).\n- Another example of something I put in my PowerShell Profile is set-location (essentially a \"cd\") to my Scripts directory, as its where I tend to want to start as my working directory.\n\n---\n\n## Stopwatch functions\n\nIn my profile.ps1 I've created the following functions:\n```language-powershell\nfunction start-stopwatch {\n    $script:StopWatch = [System.Diagnostics.Stopwatch]::StartNew()\n    write-output \"Stopwatch started at $(get-date). Use stop-stopwatch or esw to stop.\"\n}\n```\nThis starts the stopwatch running and writes to the console a message with the date it started. Note the use of $script scope for my Stopwatch variable, to make it accessible to the other functions. There may be a better/more responsible way to do this.\n\nNow it's off and running, we obviously need a way to stop it:\n\n```language-powershell\nfunction stop-stopwatch {\n    $script:StopWatch.Stop()\n    write-output \"Stopwatch stopped at $(get-date). Elapsed time is $($script:StopWatch.elapsed.tostring())\"\n    write-output $script:Stopwatch.Elapsed\n}\n```\nThis function allows us to continue the stopwatch without resetting its current time value:\n```language-powershell\nfunction continue-stopwatch {\n    $script:StopWatch.Start()\n    write-output \"Stopwatch continuing from $($script:StopWatch.elapsed.tostring())\"\n}\n```\nAnd this function allows you to get the current value from the stopwatch without interrupting it:\n```language-powershell\nfunction get-stopwatch {\n    If ($script:Stopwatch.IsRunning) {$state = \"running\"}Else{$state = \"stopped\"}\n    write-output \"Stopwatch is currently $state. Elapsed time is $($script:StopWatch.elapsed.tostring())\"\n    write-output $script:Stopwatch.Elapsed\n}\n```\nAlthough these function names are easier to remember/recall, I still found myself wanting to be able to control it more quickly. As such the last part of my PowerShell Profile creates some short aliases for these new functions:\n```language-powershell\nnew-alias ssw start-stopwatch\nnew-alias esw stop-stopwatch\nnew-alias csw continue-stopwatch\nnew-alias gsw get-stopwatch\n```\nAs an aside I used get-alias to check that these 3 letter aliases were not in use for anything else, which they don't seem to be currently but beware they could be in the future.\n\nHere's what the output looks like:\n\n![](/content/images/2016/05/stopwatch-1.png)",
                        "html": "<p><a href=\"https://twitter.com/proxb\">Boe Prox</a> blogged/tweeted recently about <a href=\"https://learn-powershell.net/2016/04/29/quick-hits-create-and-start-a-stopwatch-in-one-line/\">how you can start a stopwatch in PowerShell with one line</a> using the command <code>[System.Diagnostics.Stopwatch]::StartNew()</code>. While there are plenty of other ways to measure time in PowerShell (e.g measure-object, new-timespan), I could see an adhoc stopwatch being useful on occasion. However the raw class name is a bit clunky and forgettable. Therefore as a bit of fun I have created it as a series of functions in my PowerShell Profile.ps1 with friendly names I could call when needed.</p>\n\n<p>-- <em>The irony of the fact that I've turned his one-liner in to approx. 30 lines of code is not lost on me btw.</em></p>\n\n<hr />\n\n<ul>\n<li>In case you're not aware, the PowerShell profile is a place to store functions or commands that you want executed or available automatically each time you start PowerShell. There are actually <a href=\"https://blogs.technet.microsoft.com/heyscriptingguy/2012/05/21/understanding-the-six-powershell-profiles/\">several different kinds of Profile</a>.</li>\n<li>Another example of something I put in my PowerShell Profile is set-location (essentially a \"cd\") to my Scripts directory, as its where I tend to want to start as my working directory.</li>\n</ul>\n\n<hr />\n\n<h2 id=\"stopwatchfunctions\">Stopwatch functions</h2>\n\n<p>In my profile.ps1 I've created the following functions:  </p>\n\n<pre><code class=\"language-powershell\">function start-stopwatch {  \n    $script:StopWatch = [System.Diagnostics.Stopwatch]::StartNew()\n    write-output \"Stopwatch started at $(get-date). Use stop-stopwatch or esw to stop.\"\n}\n</code></pre>\n\n<p>This starts the stopwatch running and writes to the console a message with the date it started. Note the use of $script scope for my Stopwatch variable, to make it accessible to the other functions. There may be a better/more responsible way to do this.</p>\n\n<p>Now it's off and running, we obviously need a way to stop it:</p>\n\n<pre><code class=\"language-powershell\">function stop-stopwatch {  \n    $script:StopWatch.Stop()\n    write-output \"Stopwatch stopped at $(get-date). Elapsed time is $($script:StopWatch.elapsed.tostring())\"\n    write-output $script:Stopwatch.Elapsed\n}\n</code></pre>\n\n<p>This function allows us to continue the stopwatch without resetting its current time value:  </p>\n\n<pre><code class=\"language-powershell\">function continue-stopwatch {  \n    $script:StopWatch.Start()\n    write-output \"Stopwatch continuing from $($script:StopWatch.elapsed.tostring())\"\n}\n</code></pre>\n\n<p>And this function allows you to get the current value from the stopwatch without interrupting it:  </p>\n\n<pre><code class=\"language-powershell\">function get-stopwatch {  \n    If ($script:Stopwatch.IsRunning) {$state = \"running\"}Else{$state = \"stopped\"}\n    write-output \"Stopwatch is currently $state. Elapsed time is $($script:StopWatch.elapsed.tostring())\"\n    write-output $script:Stopwatch.Elapsed\n}\n</code></pre>\n\n<p>Although these function names are easier to remember/recall, I still found myself wanting to be able to control it more quickly. As such the last part of my PowerShell Profile creates some short aliases for these new functions:  </p>\n\n<pre><code class=\"language-powershell\">new-alias ssw start-stopwatch  \nnew-alias esw stop-stopwatch  \nnew-alias csw continue-stopwatch  \nnew-alias gsw get-stopwatch  \n</code></pre>\n\n<p>As an aside I used get-alias to check that these 3 letter aliases were not in use for anything else, which they don't seem to be currently but beware they could be in the future.</p>\n\n<p>Here's what the output looks like:</p>\n\n<p><img src=\"/content/images/2016/05/stopwatch-1.png\" alt=\"\" /></p>",
                        "image": "/content/images/2016/05/stopwatch-3.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-02 22:00:40",
                        "created_by": 1,
                        "updated_at": "2016-05-10 09:02:40",
                        "updated_by": 1,
                        "published_at": "2016-05-03 20:14:13",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 12,
                        "uuid": "22feac66-8483-4d9a-a869-7eb96ddeac38",
                        "title": "About me",
                        "slug": "about-me",
                        "markdown": "<div class=\"image-div\" style=\"width: 300px; float: left; margin: 20px 20px  20px 20px;\n}\">\n![](/content/images/2018/02/Me-BW-Square-1.jpg)\n</div>\n\n~ I live in the South East of the UK with my wife, three children, Labrador and two cats. I have been working in IT Engineering roles since graduating with a Degree in Computing in 2003. Much of my initial career was working in front-line operations roles, supporting infrastructure and SaaS applications hosted in private datacentres and the cloud. For the past 3 years I've been working as a DevOps Engineer and more recently via the contract market. My experience is predominantly with Microsoft technologies but I can generally turn a quick hand to any new tool and I have worked previously with both AWS and Azure as well as a variety of DevOps toolsets.\n\nYou can follow me on Twitter [@markwragg](https://twitter.com/markwragg).\n\nI am particularly passionate about PowerShell and have spoken twice at [PSDay.UK](https://psday.uk/), the UK's annual PowerShell Conference. If you'd like to view my talks they can be found here:\n\n#### Public Speaking\n\n- [Measure all the things with Influx, Grafana and PowerShell](https://www.youtube.com/watch?v=V7PYt9tWFw8) *-- PSDay Birmingham 2019*\n- [Mastering PowerShell Testing with Pester](https://www.youtube.com/watch?v=BbOiQCgDDR8&feature=youtu.be) *-- PSDay London 2018*\n- [How to Develop Cross-Platform Compatible Modules using PowerShell Code](https://github.com/markwragg/Presentations/tree/master/20180925_Southampton-PSUG) *-- Southampton PowerShell Usergroup 2018 (slides only, not recorded)*\n\n#### Books\n\nI have also made contributions to the following books:\n\n- [The PowerShell Conference Book volume 1](https://leanpub.com/powershell-conference-book) *-- contributed a chapter on PowerShell code testing with Pester*\n- [The PowerShell Conference Book volume 2](https://leanpub.com/psconfbook2) *-- contributed a chapter on monitoring with Influx, Grafana and PowerShell*\n\n#### Open Source\n\nI maintain a number of Open Source code repositories for tools that I've developed that I felt could be useful to the wider community. You can find those in my profile on [GitHub](https://github.com/markwragg). Some that are particularly popular include:\n\n- [PowerShell-Influx](https://github.com/markwragg/PowerShell-Influx) *-- a PowerShell module for sending metrics into the time-series platform Influx.*\n- [Test-ActiveDirectory](https://github.com/markwragg/Test-ActiveDirectory) *-- a set of Pester tests for validating the health of a deployment of Active Directory.*\n- [PowerShell-SlackBot](https://github.com/markwragg/Powershell-SlackBot) *-- a PowerShell module for hosting a chat bot for Slack using PowerShell and TCP Sockets. The chat bot can then be configured to listen to and respond to specific messages and run PowerShell commands that are initiated from within a chat window.*\n\n\n\n\n\n",
                        "html": "<div class=\"image-div\" style=\"width: 300px; float: left; margin: 20px 20px  20px 20px;  \n}\">\n<img src=\"/content/images/2018/02/Me-BW-Square-1.jpg\" alt=\"\" />\n</div>\n\n<p>~ I live in the South East of the UK with my wife, three children, Labrador and two cats. I have been working in IT Engineering roles since graduating with a Degree in Computing in 2003. Much of my initial career was working in front-line operations roles, supporting infrastructure and SaaS applications hosted in private datacentres and the cloud. For the past 3 years I've been working as a DevOps Engineer and more recently via the contract market. My experience is predominantly with Microsoft technologies but I can generally turn a quick hand to any new tool and I have worked previously with both AWS and Azure as well as a variety of DevOps toolsets.</p>\n\n<p>You can follow me on Twitter <a href=\"https://twitter.com/markwragg\">@markwragg</a>.</p>\n\n<p>I am particularly passionate about PowerShell and have spoken twice at <a href=\"https://psday.uk/\">PSDay.UK</a>, the UK's annual PowerShell Conference. If you'd like to view my talks they can be found here:</p>\n\n<h4 id=\"publicspeaking\">Public Speaking</h4>\n\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=V7PYt9tWFw8\">Measure all the things with Influx, Grafana and PowerShell</a> <em>-- PSDay Birmingham 2019</em></li>\n<li><a href=\"https://www.youtube.com/watch?v=BbOiQCgDDR8&amp;feature=youtu.be\">Mastering PowerShell Testing with Pester</a> <em>-- PSDay London 2018</em></li>\n<li><a href=\"https://github.com/markwragg/Presentations/tree/master/20180925_Southampton-PSUG\">How to Develop Cross-Platform Compatible Modules using PowerShell Code</a> <em>-- Southampton PowerShell Usergroup 2018 (slides only, not recorded)</em></li>\n</ul>\n\n<h4 id=\"books\">Books</h4>\n\n<p>I have also made contributions to the following books:</p>\n\n<ul>\n<li><a href=\"https://leanpub.com/powershell-conference-book\">The PowerShell Conference Book volume 1</a> <em>-- contributed a chapter on PowerShell code testing with Pester</em></li>\n<li><a href=\"https://leanpub.com/psconfbook2\">The PowerShell Conference Book volume 2</a> <em>-- contributed a chapter on monitoring with Influx, Grafana and PowerShell</em></li>\n</ul>\n\n<h4 id=\"opensource\">Open Source</h4>\n\n<p>I maintain a number of Open Source code repositories for tools that I've developed that I felt could be useful to the wider community. You can find those in my profile on <a href=\"https://github.com/markwragg\">GitHub</a>. Some that are particularly popular include:</p>\n\n<ul>\n<li><a href=\"https://github.com/markwragg/PowerShell-Influx\">PowerShell-Influx</a> <em>-- a PowerShell module for sending metrics into the time-series platform Influx.</em></li>\n<li><a href=\"https://github.com/markwragg/Test-ActiveDirectory\">Test-ActiveDirectory</a> <em>-- a set of Pester tests for validating the health of a deployment of Active Directory.</em></li>\n<li><a href=\"https://github.com/markwragg/Powershell-SlackBot\">PowerShell-SlackBot</a> <em>-- a PowerShell module for hosting a chat bot for Slack using PowerShell and TCP Sockets. The chat bot can then be configured to listen to and respond to specific messages and run PowerShell commands that are initiated from within a chat window.</em></li>\n</ul>",
                        "image": "/content/images/2020/08/markwragg-psdayuk-2019.JPG",
                        "featured": 0,
                        "page": 1,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-03 13:24:06",
                        "created_by": 1,
                        "updated_at": "2020-08-10 16:17:49",
                        "updated_by": 1,
                        "published_at": "2020-08-10 08:00:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 13,
                        "uuid": "bdc41f65-f2dc-44a8-a96b-ddf13d7b0fdc",
                        "title": "Resources",
                        "slug": "resources",
                        "markdown": "This page lists various media, websites and other resources that I've found interesting and useful.\n\n## Blogs\n\nI am using [Feedly](https://feedly.com/) to aggregate the RSS feeds from a number of technology blogs. I recommend those listed below.\n\n- [Sam Martin](http://sammart.in) encouraged me to write this blog. I hired Sam in 2012 upon his return to IT after a short break and he quickly became indispensable both within my team and organisation. Sam uses his blog to chronicle new technologies and skills he has developed in areas such as AWS, Azure, Chef etc.\n\n- [Richard Siddaway](https://richardspowershellblog.wordpress.com/) blogs and speaks about Powershell and other related topics. Richard is a veteran IT architect, server administrator, support engineer, and PowerShell MVP. He's the author of PowerShell in Practice, published in 2010 by Manning.\n\n## Podcasts\n\nI have found Podcasts to be a great way to stay informed of emerging technologies and practices and a useful way to get some value from my commute. This page lists some of the Podcasts that I recommend to anyone working in Windows-server based Ops.\n\n- [RunAs Radio](http://runasradio.com/) is a weekly podcast aimed at Microsoft IT Professionals. It focuses solely on Microsoft technologies and features a different guest each week to discuss various topics including: Windows, Powershell, SQL and more.\n\n- [The Stack Exchange Podcast](https://blog.stackoverflow.com/tags/podcasts/) is hosted by the founders of Stack Exchange. Typically they discuss new features and news relevant to the Stack Exchange community, but the detail in which they discuss their projects and the rationale of their decisions makes for compelling, relatable listening.\n\n- [Ops All The Things](https://opsallthethings.com/) is a podcast from Steven Murawski and Christopher Webber. While not frequently updated, its fairly unique in that it considers IT from the perspective of Operations.\n\n- [Manager Tools](https://www.manager-tools.com/podcasts) is a great resource for anyone responsible for managing others. Not necessarily worth listening to on a weekly basis, instead review their back catalogue and pick topics relevant to you. They keep to a strict 30 minute format so the information is generally delivered succinctly.\n\n## Other\n\nOther sites or links you may find useful.\n\n- [@MorpheusData](https://medium.com/@MorpheusData) recently shared this list of [400+ Free Resources for DevOps and SysAdmins](https://medium.com/@MorpheusData/400-free-resources-for-devops-2e2ecf52e64a#.ujhkxfha5) on Medium.",
                        "html": "<p>This page lists various media, websites and other resources that I've found interesting and useful.</p>\n\n<h2 id=\"blogs\">Blogs</h2>\n\n<p>I am using <a href=\"https://feedly.com/\">Feedly</a> to aggregate the RSS feeds from a number of technology blogs. I recommend those listed below.</p>\n\n<ul>\n<li><p><a href=\"http://sammart.in\">Sam Martin</a> encouraged me to write this blog. I hired Sam in 2012 upon his return to IT after a short break and he quickly became indispensable both within my team and organisation. Sam uses his blog to chronicle new technologies and skills he has developed in areas such as AWS, Azure, Chef etc.</p></li>\n<li><p><a href=\"https://richardspowershellblog.wordpress.com/\">Richard Siddaway</a> blogs and speaks about Powershell and other related topics. Richard is a veteran IT architect, server administrator, support engineer, and PowerShell MVP. He's the author of PowerShell in Practice, published in 2010 by Manning.</p></li>\n</ul>\n\n<h2 id=\"podcasts\">Podcasts</h2>\n\n<p>I have found Podcasts to be a great way to stay informed of emerging technologies and practices and a useful way to get some value from my commute. This page lists some of the Podcasts that I recommend to anyone working in Windows-server based Ops.</p>\n\n<ul>\n<li><p><a href=\"http://runasradio.com/\">RunAs Radio</a> is a weekly podcast aimed at Microsoft IT Professionals. It focuses solely on Microsoft technologies and features a different guest each week to discuss various topics including: Windows, Powershell, SQL and more.</p></li>\n<li><p><a href=\"https://blog.stackoverflow.com/tags/podcasts/\">The Stack Exchange Podcast</a> is hosted by the founders of Stack Exchange. Typically they discuss new features and news relevant to the Stack Exchange community, but the detail in which they discuss their projects and the rationale of their decisions makes for compelling, relatable listening.</p></li>\n<li><p><a href=\"https://opsallthethings.com/\">Ops All The Things</a> is a podcast from Steven Murawski and Christopher Webber. While not frequently updated, its fairly unique in that it considers IT from the perspective of Operations.</p></li>\n<li><p><a href=\"https://www.manager-tools.com/podcasts\">Manager Tools</a> is a great resource for anyone responsible for managing others. Not necessarily worth listening to on a weekly basis, instead review their back catalogue and pick topics relevant to you. They keep to a strict 30 minute format so the information is generally delivered succinctly.</p></li>\n</ul>\n\n<h2 id=\"other\">Other</h2>\n\n<p>Other sites or links you may find useful.</p>\n\n<ul>\n<li><a href=\"https://medium.com/@MorpheusData\">@MorpheusData</a> recently shared this list of <a href=\"https://medium.com/@MorpheusData/400-free-resources-for-devops-2e2ecf52e64a#.ujhkxfha5\">400+ Free Resources for DevOps and SysAdmins</a> on Medium.</li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 1,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-03 13:25:46",
                        "created_by": 1,
                        "updated_at": "2016-05-03 13:26:01",
                        "updated_by": 1,
                        "published_at": "2016-05-01 12:00:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 14,
                        "uuid": "1be599e2-81a9-44c6-a5f2-22fc6672dcfe",
                        "title": "Auto Scaling across multiple Availability Zones with CloudFormation",
                        "slug": "configuring-ec2-auto-scaling-groups-to-scale-instances-across-multiple-availability-zones-with-cloudformation",
                        "markdown": "A colleague and I recently implemented an improvement to our CloudFormation scripts which enabled the Auto Scaling Group to launch instances across more than one Availability Zone. This blog post documents the changes we made.\n\nIf you're not using CloudFormation, its likely a simple case of defining the Availability Zones when you manually create your Auto Scaling Group. As far as I know, you do this at point of creation and cannot change this later (without creating a new ASG) as well as ensuring you have subnets within your VPC for each AZ.\n\n> **Caveats / Lessons learnt**\n>\n- I am fairly new to AWS and have a basic understanding of CloudFormation. This guidance might not be applicable to every scenario or best practice. We had existing CloudFormation scripts which I simply modified.\n- You need to have a VPC Subnet defined for each Availability Zone.\n- Reserved Instances are purchased per Availability Zone, so beware if you have existing RIs or wish to use RIs you need to divide your purchases across the AZs you intend to use.\n- When you update your CloudFormation stack, your EC2 instances will be rebalanced immediately as a result. This means half of your instances will be terminated and relaunched in the other AZ. If you want to protect yourself from the sudden loss of half your instances, you could temporarily scale up to twice your existing volume first.\n\n### Pre-requisites\n\nAs listed above, its first important to ensure that you have a VPC Subnet defined per Availability Zone you wish to use. This is a simple case of navigating to VPC > Subnets and creating a new Subnet within your VPC that is associated with the new AZ you wish to use. You should likely also ensure that this subnet is associated with the same Route Table as the existing subnet used by the Auto Scaling Group (as defined under VPC > Route Tables > Subnet Associations. If you just use a default main Route Table this may occur by default.\n\n### Modifying the CloudFormation script\n\nPreviously our CloudFormation had a single \"AvailabilityZone\" node under \"Parameters\". We duplicated this and renamed one to be \"AvailabilityZoneA\" and the other \"AvailabilityZoneB\". Previously this was a String type, but we took the opportunity at the same time to update the Type to be: \"AWS::EC2::AvailabilityZone::Name\". This makes CloudFormation prompt you with a drop down list of the AZs when you are launching or configuring the stack.\n\n```language-json\n    \"AvailabilityZoneA\": {\n\t\t\"Type\": \"AWS::EC2::AvailabilityZone::Name\",\n\t\t\"Default\": \"eu-west-1a\",\n\t\t\"Description\": \"Name of the first availability zone in which the servers will be created.\"\n    },\n  \t  \n     \"AvailabilityZoneB\": {\n\t\t\"Type\": \"AWS::EC2::AvailabilityZone::Name\",\n\t\t\"Default\": \"eu-west-1b\",\n\t\t\"Description\": \"Name of the second availability zone in which the servers will be created.\"\n    },\n```\nWe next made the same modification to the VPCSubnet parameter, replacing the single parameter with two:\n```language-json\n    \"VPCSubnetA\": {\n\t  \t\"Type\": \"AWS::EC2::Subnet::Id\",\n\t  \t\"Description\": \"The ID of the subnet in the first AvailabilityZone as specified in AvailabilityZoneA.\"\n    },\n\t  \n    \"VPCSubnetB\": {\n\t  \t\"Type\": \"AWS::EC2::Subnet::Id\",\n\t  \t\"Description\": \"The ID of the subnet in the second AvailabilityZone as specified in AvailabilityZoneB.\"\n    },\n```\n\t  \nIn the resources section we modified the \"WebServerGroup\" such that the AvailabililityZones and VPCZoneIdentifier properties referenced the values of newly defined parameters.\n```language-json\n    \"WebServerGroup\": {\n\t\t\t\"Type\": \"AWS::AutoScaling::AutoScalingGroup\",\n\t\t\t\"Properties\": {\n\t\t\t  \"AvailabilityZones\" : [{ \"Ref\" : \"AvailabilityZoneA\"}, { \"Ref\" : \"AvailabilityZoneB\"}],\n\t\t\t ...\t\n\t\t\t \"VPCZoneIdentifier\":[{ \"Ref\" : \"VPCSubnetA\" },{ \"Ref\" : \"VPCSubnetB\" }]\n\t\t\t}\n    },\n```\nWe modified the \"ElasticLoadBalancer\" resource to also reference the two VPC parameters:\n```language-json\n    \"ElasticLoadBalancer\": {\n\t\t\t\"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n\t\t\t\"Properties\": {\n\t\t\t\t...\n\t\t\t\t\"Subnets\" : [{ \"Ref\" : \"VPCSubnetA\" },{ \"Ref\" : \"VPCSubnetB\" }],\n    },\n```\nWe made one final optional change, which was to include the following property in the ElasticLoadBalancer configuration:\n```language-json\n    \"CrossZone\": \"True\",\n```\nThis setting is described here in the AWS documentation: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/enable-disable-crosszone-lb.html but the description is somewhat difficult to interpret. My interpretation is that this ensures when your ELB determines which Instance to send traffic to, it is considering the load of all instances in all AZs. I believe without this being enabled, it will first pick an AZ (assumedly in a round-robin fashion) and then decide where to send traffic within that AZ based on load. \n\n### Pushing to production\n\nAfter committing the .json file changes to Source Control, we scheduled the change and via CloudFormation performed an \"Update Stack\" operation to bring the changes in to Production. As noted earlier, in each region this caused half of the instances to be immediately terminated and relaunched. After we had confirmed that they were each marked as \"InService\" (healthy) in the LoadBalancer, we terminated the other pre-existing instances to ensure the ASG was fully working as well as to relaunch them with the latest AMI. The final result was the instances spread equally across two Availability Zones in each of our regions.",
                        "html": "<p>A colleague and I recently implemented an improvement to our CloudFormation scripts which enabled the Auto Scaling Group to launch instances across more than one Availability Zone. This blog post documents the changes we made.</p>\n\n<p>If you're not using CloudFormation, its likely a simple case of defining the Availability Zones when you manually create your Auto Scaling Group. As far as I know, you do this at point of creation and cannot change this later (without creating a new ASG) as well as ensuring you have subnets within your VPC for each AZ.</p>\n\n<blockquote>\n  <p><strong>Caveats / Lessons learnt</strong></p>\n  \n  <ul>\n  <li>I am fairly new to AWS and have a basic understanding of CloudFormation. This guidance might not be applicable to every scenario or best practice. We had existing CloudFormation scripts which I simply modified.</li>\n  <li>You need to have a VPC Subnet defined for each Availability Zone.</li>\n  <li>Reserved Instances are purchased per Availability Zone, so beware if you have existing RIs or wish to use RIs you need to divide your purchases across the AZs you intend to use.</li>\n  <li>When you update your CloudFormation stack, your EC2 instances will be rebalanced immediately as a result. This means half of your instances will be terminated and relaunched in the other AZ. If you want to protect yourself from the sudden loss of half your instances, you could temporarily scale up to twice your existing volume first.</li>\n  </ul>\n</blockquote>\n\n<h3 id=\"prerequisites\">Pre-requisites</h3>\n\n<p>As listed above, its first important to ensure that you have a VPC Subnet defined per Availability Zone you wish to use. This is a simple case of navigating to VPC > Subnets and creating a new Subnet within your VPC that is associated with the new AZ you wish to use. You should likely also ensure that this subnet is associated with the same Route Table as the existing subnet used by the Auto Scaling Group (as defined under VPC > Route Tables > Subnet Associations. If you just use a default main Route Table this may occur by default.</p>\n\n<h3 id=\"modifyingthecloudformationscript\">Modifying the CloudFormation script</h3>\n\n<p>Previously our CloudFormation had a single \"AvailabilityZone\" node under \"Parameters\". We duplicated this and renamed one to be \"AvailabilityZoneA\" and the other \"AvailabilityZoneB\". Previously this was a String type, but we took the opportunity at the same time to update the Type to be: \"AWS::EC2::AvailabilityZone::Name\". This makes CloudFormation prompt you with a drop down list of the AZs when you are launching or configuring the stack.</p>\n\n<pre><code class=\"language-json\">    \"AvailabilityZoneA\": {\n        \"Type\": \"AWS::EC2::AvailabilityZone::Name\",\n        \"Default\": \"eu-west-1a\",\n        \"Description\": \"Name of the first availability zone in which the servers will be created.\"\n    },\n\n     \"AvailabilityZoneB\": {\n        \"Type\": \"AWS::EC2::AvailabilityZone::Name\",\n        \"Default\": \"eu-west-1b\",\n        \"Description\": \"Name of the second availability zone in which the servers will be created.\"\n    },\n</code></pre>\n\n<p>We next made the same modification to the VPCSubnet parameter, replacing the single parameter with two:  </p>\n\n<pre><code class=\"language-json\">    \"VPCSubnetA\": {\n        \"Type\": \"AWS::EC2::Subnet::Id\",\n        \"Description\": \"The ID of the subnet in the first AvailabilityZone as specified in AvailabilityZoneA.\"\n    },\n\n    \"VPCSubnetB\": {\n        \"Type\": \"AWS::EC2::Subnet::Id\",\n        \"Description\": \"The ID of the subnet in the second AvailabilityZone as specified in AvailabilityZoneB.\"\n    },\n</code></pre>\n\n<p>In the resources section we modified the \"WebServerGroup\" such that the AvailabililityZones and VPCZoneIdentifier properties referenced the values of newly defined parameters.  </p>\n\n<pre><code class=\"language-json\">    \"WebServerGroup\": {\n            \"Type\": \"AWS::AutoScaling::AutoScalingGroup\",\n            \"Properties\": {\n              \"AvailabilityZones\" : [{ \"Ref\" : \"AvailabilityZoneA\"}, { \"Ref\" : \"AvailabilityZoneB\"}],\n             ...    \n             \"VPCZoneIdentifier\":[{ \"Ref\" : \"VPCSubnetA\" },{ \"Ref\" : \"VPCSubnetB\" }]\n            }\n    },\n</code></pre>\n\n<p>We modified the \"ElasticLoadBalancer\" resource to also reference the two VPC parameters:  </p>\n\n<pre><code class=\"language-json\">    \"ElasticLoadBalancer\": {\n            \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n            \"Properties\": {\n                ...\n                \"Subnets\" : [{ \"Ref\" : \"VPCSubnetA\" },{ \"Ref\" : \"VPCSubnetB\" }],\n    },\n</code></pre>\n\n<p>We made one final optional change, which was to include the following property in the ElasticLoadBalancer configuration:  </p>\n\n<pre><code class=\"language-json\">    \"CrossZone\": \"True\",\n</code></pre>\n\n<p>This setting is described here in the AWS documentation: <a href=\"http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/enable-disable-crosszone-lb.html\">http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/enable-disable-crosszone-lb.html</a> but the description is somewhat difficult to interpret. My interpretation is that this ensures when your ELB determines which Instance to send traffic to, it is considering the load of all instances in all AZs. I believe without this being enabled, it will first pick an AZ (assumedly in a round-robin fashion) and then decide where to send traffic within that AZ based on load. </p>\n\n<h3 id=\"pushingtoproduction\">Pushing to production</h3>\n\n<p>After committing the .json file changes to Source Control, we scheduled the change and via CloudFormation performed an \"Update Stack\" operation to bring the changes in to Production. As noted earlier, in each region this caused half of the instances to be immediately terminated and relaunched. After we had confirmed that they were each marked as \"InService\" (healthy) in the LoadBalancer, we terminated the other pre-existing instances to ensure the ASG was fully working as well as to relaunch them with the latest AMI. The final result was the instances spread equally across two Availability Zones in each of our regions.</p>",
                        "image": "/content/images/2016/05/Autoscaling_MultiAZ_s.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-03 13:26:52",
                        "created_by": 1,
                        "updated_at": "2016-05-04 09:43:38",
                        "updated_by": 1,
                        "published_at": "2016-05-02 14:00:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 15,
                        "uuid": "dd85a3e7-6163-4fce-b8ff-b57473526dda",
                        "title": "Three things you might not know about Import-module",
                        "slug": "three-things-you-might-not-know-about-import-module",
                        "markdown": "In Powershell the Import-Module cmdlet allows you to extend the cmdlets available to you within your script or console. This can be official extensions, such as the activedirectory module that is included with AD Tools, or custom modules you have written yourself to group together a useful set of functions or commands. This post briefly covers a few tricks i've discovered about import-module that i've found repeatedly useful.\n\nIf you want to see which modules are available to you (and where they are located on the system) execute `Get-Module -ListAvailable`.\n\n### 1. Re-importing a module that is already loaded does nothing, unless you use the -force\n\nWhen developing your own modules, you may wish to test changes to your code by re-importing your module via repeatedly using the import-module command. What Powershell doesn't make obvious is that if a module has been previously loaded, import-module does nothing. This can be frustrating during development as the console returns no indication that it has not reloaded your module. To ensure your module does get reloaded each time, simply include the -force parameter.\n\n`Import-module mymodule -force`\n\nThis is an alternative to executing remove-module then import-module again, or you could close and relaunch your Powershell session.\n\n### 2. You can limit the cmdlets that are loaded by using -cmdlet\n\nIf you want to limit specifically which cmdlets are loaded when calling import-module, simply use the -cmdlet parameter and then a comma separated list of cmdlets. I find this useful when working with the ActiveDirectory module in a script where I only want one or two commands (such as get-aduser). I suspect there is a small speed benefit to specifying which cmdlets to load, but this also has the security benefit of ensuring the script is only using the cmdlet you intended.\n\n`Import-module activedirectory -cmdlet get-aduser,get-adcomputer`\n\n### 3.  You can use -verbose to detail the cmdlets and other items loaded from the module\n\nIf you want to see exactly what is loaded when an import-module command is used add the -verbose parameter and get verbose output to the console listing (among other things) each cmdlet that is loaded.\n\n`Import-module activedirectory -verbose`",
                        "html": "<p>In Powershell the Import-Module cmdlet allows you to extend the cmdlets available to you within your script or console. This can be official extensions, such as the activedirectory module that is included with AD Tools, or custom modules you have written yourself to group together a useful set of functions or commands. This post briefly covers a few tricks i've discovered about import-module that i've found repeatedly useful.</p>\n\n<p>If you want to see which modules are available to you (and where they are located on the system) execute <code>Get-Module -ListAvailable</code>.</p>\n\n<h3 id=\"1reimportingamodulethatisalreadyloadeddoesnothingunlessyouusetheforce\">1. Re-importing a module that is already loaded does nothing, unless you use the -force</h3>\n\n<p>When developing your own modules, you may wish to test changes to your code by re-importing your module via repeatedly using the import-module command. What Powershell doesn't make obvious is that if a module has been previously loaded, import-module does nothing. This can be frustrating during development as the console returns no indication that it has not reloaded your module. To ensure your module does get reloaded each time, simply include the -force parameter.</p>\n\n<p><code>Import-module mymodule -force</code></p>\n\n<p>This is an alternative to executing remove-module then import-module again, or you could close and relaunch your Powershell session.</p>\n\n<h3 id=\"2youcanlimitthecmdletsthatareloadedbyusingcmdlet\">2. You can limit the cmdlets that are loaded by using -cmdlet</h3>\n\n<p>If you want to limit specifically which cmdlets are loaded when calling import-module, simply use the -cmdlet parameter and then a comma separated list of cmdlets. I find this useful when working with the ActiveDirectory module in a script where I only want one or two commands (such as get-aduser). I suspect there is a small speed benefit to specifying which cmdlets to load, but this also has the security benefit of ensuring the script is only using the cmdlet you intended.</p>\n\n<p><code>Import-module activedirectory -cmdlet get-aduser,get-adcomputer</code></p>\n\n<h3 id=\"3youcanuseverbosetodetailthecmdletsandotheritemsloadedfromthemodule\">3.  You can use -verbose to detail the cmdlets and other items loaded from the module</h3>\n\n<p>If you want to see exactly what is loaded when an import-module command is used add the -verbose parameter and get verbose output to the console listing (among other things) each cmdlet that is loaded.</p>\n\n<p><code>Import-module activedirectory -verbose</code></p>",
                        "image": "/content/images/2016/05/media-20160429-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-03 13:28:26",
                        "created_by": 1,
                        "updated_at": "2016-05-03 20:19:13",
                        "updated_by": 1,
                        "published_at": "2016-05-01 15:00:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 16,
                        "uuid": "b5bda27a-d584-472f-8c24-afe4b84f4238",
                        "title": "Using JIRA and Agile Project Management",
                        "slug": "using-jira-and-agile-project-management",
                        "markdown": "> https://www.youtube.com/watch?v=NrHpXvDXVrw\n\n### Why Agile?\n\n- Better visibility (solves a lot of problems)\n- Better prioritisation\n- Less downtime between tasks\n- Improved Teamwork\n- More predictability - estimating the work you can do (can I do 8 things by next Friday?) over and over again you start to get a realistic understanding.\n\n### Creating tasks\n#### What's a task?\nA task is something that takes more than 30 minutes to do (anything less is too small to write down). If it's bigger than 30 mins it's worth recording. Don't make a task that takes more than 3 days (or even more than 1 day).\n\n#### Waiting on others\nWaiting is a good sign that your task is really 3 tasks.\nWrite Blog Post becomes Write Draft > Review Draft > Revise Draft\n\n#### How long will it take?\nEstimating is hard. Jira uses \"Story Points\" not hours. \n30mins = 1 story point (the smallest size task you put in Jira).\n1 day = 16 points (8 hour day)\n\n> \"Story point\" is an agile concept. Avoiding estimating directly in hours avoids people from using it to judge performance (Didn't complete a 30 min task? working too slow).\n\n#### Scrum vs Kanban\nSubsets of Agile. Scrum is used by teams with a target deadline they're trying to work towards. The least important variable for a Scrub team is the quantity of work that gets done.\n\nKanban teams are like Scrum teams but without a target date. Support and Service teams. Work from a prioritised list as fast as you can. No plan mode, no sprints. Kanban doesn't have a plan more (greyed out in JIRA).\n\n### Epics - Group together work of a certain theme. Like a tag.\n> A waiting task is really two tasks\nIf you have to wait, split the task in two. You can't control others time, so your part of the task is complete, the second part becomes a separate task.\n\n### Burndown\nCreate a ton of tasks, put them in a Sprint, get those things done.\nEstimate how much work you can do in the next Sprint based on how much work you got done in the previous sprint.\n",
                        "html": "<blockquote>\n  <p><a href=\"https://www.youtube.com/watch?v=NrHpXvDXVrw\">https://www.youtube.com/watch?v=NrHpXvDXVrw</a></p>\n</blockquote>\n\n<h3 id=\"whyagile\">Why Agile?</h3>\n\n<ul>\n<li>Better visibility (solves a lot of problems)</li>\n<li>Better prioritisation</li>\n<li>Less downtime between tasks</li>\n<li>Improved Teamwork</li>\n<li>More predictability - estimating the work you can do (can I do 8 things by next Friday?) over and over again you start to get a realistic understanding.</li>\n</ul>\n\n<h3 id=\"creatingtasks\">Creating tasks</h3>\n\n<h4 id=\"whatsatask\">What's a task?</h4>\n\n<p>A task is something that takes more than 30 minutes to do (anything less is too small to write down). If it's bigger than 30 mins it's worth recording. Don't make a task that takes more than 3 days (or even more than 1 day).</p>\n\n<h4 id=\"waitingonothers\">Waiting on others</h4>\n\n<p>Waiting is a good sign that your task is really 3 tasks. <br />\nWrite Blog Post becomes Write Draft > Review Draft > Revise Draft</p>\n\n<h4 id=\"howlongwillittake\">How long will it take?</h4>\n\n<p>Estimating is hard. Jira uses \"Story Points\" not hours. <br />\n30mins = 1 story point (the smallest size task you put in Jira). <br />\n1 day = 16 points (8 hour day)</p>\n\n<blockquote>\n  <p>\"Story point\" is an agile concept. Avoiding estimating directly in hours avoids people from using it to judge performance (Didn't complete a 30 min task? working too slow).</p>\n</blockquote>\n\n<h4 id=\"scrumvskanban\">Scrum vs Kanban</h4>\n\n<p>Subsets of Agile. Scrum is used by teams with a target deadline they're trying to work towards. The least important variable for a Scrub team is the quantity of work that gets done.</p>\n\n<p>Kanban teams are like Scrum teams but without a target date. Support and Service teams. Work from a prioritised list as fast as you can. No plan mode, no sprints. Kanban doesn't have a plan more (greyed out in JIRA).</p>\n\n<h3 id=\"epicsgrouptogetherworkofacertainthemelikeatag\">Epics - Group together work of a certain theme. Like a tag.</h3>\n\n<blockquote>\n  <p>A waiting task is really two tasks\n  If you have to wait, split the task in two. You can't control others time, so your part of the task is complete, the second part becomes a separate task.</p>\n</blockquote>\n\n<h3 id=\"burndown\">Burndown</h3>\n\n<p>Create a ton of tasks, put them in a Sprint, get those things done. <br />\nEstimate how much work you can do in the next Sprint based on how much work you got done in the previous sprint.</p>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-05 10:35:28",
                        "created_by": 1,
                        "updated_at": "2016-09-05 21:55:08",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 17,
                        "uuid": "12616886-bbdc-4454-9009-b206e01696e9",
                        "title": "Containers in Windows Server 2016 TP5",
                        "slug": "containers-in-windows-server-2016-tp5",
                        "markdown": "I'm just starting to get my head around the concept of Containers and as such decided to take Windows Server 2016 Technical Preview 5 for a spin, which includes Docker Containers as a feature. It's potentially worth being more explicit here (for anyone not aware) that this isn't Microsoft's version of Containers, this is [actually Docker baked in to Server 2016](https://www.docker.com/microsoft). That's a pretty cool move on Microsoft's part.\n\nThings were already getting pretty meta in trying to run a Container inside a Virtual Machine on my desktop, but an early challenge I experienced in testing this out was getting the Hyper-V feature to install within a VM running under VirtualBox. The short answer seems to be you can't, due to a lack of support for Nested Virtualisation [despite someone asking for the feature seven years ago](https://www.virtualbox.org/ticket/4032).\n\n*-- This doesn't mean you can't test Server 2016 via VirtualBox btw, you just definitely can't test out Hyper-V containers. What's a Hyper-V container you ask?*\n\n![](/content/images/2016/05/0216-diagram.png)\n\n> **Windows Containers**\n>\n> - Multiple container instances can run concurrently on a host, with isolation provided through namespace, resource control, and process isolation technologies. Windows Server containers share the same kernel with the host, as well as each other.\n>\n> **Hyper-V Containers**\n>\n>  - Multiple container instances can run concurrently on a host; however, each container runs inside of a special virtual machine. This provides kernel level isolation between each Hyper-V container and the container host.\n>\nSource: [MSDN: Hyper-V Containers](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/hyperv_container)\n\n*-- In summary, a Hyper-V container is more isolated. Containers are like living in a gated community, but a Hyper-V Container also lives in a cave in the woods.*\n\nAs I've noted previously, I suspect [the Hyper-V feature on Windows 8/10 may support nested virtualisation](http://www.altaro.com/hyper-v/nested-virtualization-hyper-v-windows-server-2016/), but as i'm currently driving Windows 7 that wasn't an option for me, so instead I set up my Virtual Machine using VMWare Workstation 12 Player. \n\nIf you haven't yet installed Windows Server 2016 [see my previous post on how I went about it](http://wragg.io/installing-windows-2016-technical-preview-5-tp5/) and [this post for a little more detail on basic setup if you want to use Server Core](http://wragg.io/installing-windows-server-2016-tp5-server-core).\n\nEither way (to core or not to core) the next steps are as follows:\n\n*-- The steps i'm about to describe are also covered in the [MSDN Windows Containers Quick Start guide](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start) and I strongly advise you check there first to ensure you have the latest information.*\n\n**1. Configure your server to be a Container Host**\n\nThere's a little more to doing this than just installing the Containers feature surprisingly (although that is the first step). However Microsoft have made this very easy by providing a Powershell script that does all the work (yay automation!) per [Step 2 of the Quick Start guide](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start_configure_host).\n\nLog in to your machine and run `Powershell`. We're going to deploy to an existing system, so we should use the \"Existing System\" part of the guide. First you can download the Powershell script by executing this command:\n\n````language-powershell\nwget -uri https://aka.ms/tp5/Install-ContainerHost -OutFile C:\\Install-ContainerHost.ps1\n```\n\nAnd then this to configure the system to host Hyper-V and Windows Containers:\n\n```language-powershell\npowershell.exe -NoProfile -ExecutionPolicy Bypass C:\\Install-ContainerHost.ps1 -HyperV\n```\n\n*-- If you don't want to test out Hyper-V containers you can omit the -HyperV switch and I believe it will configure the server for Windows Containers instead.*\n\nThis installs the required features and performs reboots. It will automatically resume when you login after boot (it sets up a logon task to do so). This takes a short while (depending on your bandwidth) as it downloads a Windows Nano Server image file via [OneGet](https://github.com/OneGet/oneget). Here's what it looks like while it's downloading:\n\n![](/content/images/2016/05/Containers-downloading.png)\n\nWhen it's done the script will retun \"Script complete!\" as well as list the available Docker images (which you can get at any time with `docker images`). If you're testing out Hyper-V containers you should see a 'nanoserver' image with a a tag of latest:\n\n```\nBase image is now tagged:\nnanoserver   10.0.14300.1010   9db95268a387   6 weeks ago   817.1 MB\nnanoserver   latest            9db95268a387   6 weeks ago   817.1 MB\n```\nIf you're testing Windows Containers, it will have downloaded you a 'windowsservercore' image instead.\n\nTo see Docker system info (and confirm its working), type `docker info`:\n\n![](/content/images/2016/05/server-2016-docker-info.png)\n\nTo see what else you can do with Docker type `Docker --help`.\n\n**2. Configure a Container**\n\nIf you've configured your Container Host to run Hyper-V containers, you should now move to [part 4 of the Quick Start guide](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/manage_docker_hyperv) (part 3 covers Windows Containers). Again I will summarise the steps, but I recommend checking the guide directly for the latest info.\n\n*-- This is particularly relevant advice, because this [documentation is open source and in GitHub where anyone can submit improvements](https://github.com/Microsoft/Virtualization-Documentation), so it's likely to see regular improvements.*\n\nCreate a directory named c:\\share\\en-us. If you're working in Server Core you can do this with Powershell by executing:\n\n```language-powershell\nNew-Item -Type Directory c:\\share\\en-us\n```\n\n> *\"Hyper-V containers use the Nano Server base OS image. Because Nano Server is light weight operating system and does not include the IIS package, this needs to be obtained in order to complete this exercise. This can be found on the Window Server 2016 technical preview media under the NanoServer\\Packages directory.\"*\n\n**Ensure you have the Server 2016 TP5 ISO media still mounted to a CD Drive as D:\\\\.** Then copy the IIS package files to your container host by executing:\n\n```language-powershell\nCopy-Item D:\\NanoServer\\Packages\\Microsoft-NanoServer-IIS-Package.cab C:\\share\nCopy-Item D:\\NanoServer\\Packages\\en-us\\Microsoft-NanoServer-IIS-Package_en-us.cab C:\\share\\en-us\n```\n\nCreate a file in the c:\\share folder named unattend.xml and copy the following text into the unattend.xml file:\n\n```language-xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n    <unattend xmlns=\"urn:schemas-microsoft-com:unattend\">\n    <servicing>\n        <package action=\"install\">\n            <assemblyIdentity name=\"Microsoft-NanoServer-IIS-Feature-Package\" version=\"10.0.14300.1000\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" />\n            <source location=\"c:\\iisinstall\\Microsoft-NanoServer-IIS-Package.cab\" />\n        </package>\n        <package action=\"install\">\n            <assemblyIdentity name=\"Microsoft-NanoServer-IIS-Feature-Package\" version=\"10.0.14300.1000\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"en-US\" />\n            <source location=\"c:\\iisinstall\\en-us\\Microsoft-NanoServer-IIS-Package_en-us.cab\" />\n        </package>\n    </servicing>\n    <cpi:offlineImage cpi:source=\"\" xmlns:cpi=\"urn:schemas-microsoft-com:cpi\" />\n</unattend>\n```\n\n*-- Even in Server Core you can do this by just running `notepad`.*\n\nYour C:\\Share directory should now look like this:\n\n```\nc:\\share\n|-- en-us\n|    |-- Microsoft-NanoServer-IIS-Package_en-us.cab\n|\n|-- Microsoft-NanoServer-IIS-Package.cab\n|-- unattend.xml\n```\n\n**3. Start a Container**\n\nPer the quickstart guide:\n\n> *\"To create a Hyper-V container using docker, specify the --isolation=hyperv parameter. This example mounts the c:\\share directory from the host, to the c:\\iisinstall directory of the container, and then creates an interactive shell session with the container.\"*\n\nExecute this:\n\n```language-powershell\ndocker run --name iisnanobase -it -p 80:80 -v c:\\share:c:\\iisinstall --isolation=hyperv nanoserver cmd\n```\n\n*-- Potential Issues:*\n\nIf you get an error similar to the below, you may need to ensure your host Virtual Machine has at least 2 CPU cores:\n\n<div style=\"background-color:#ffcccc; color:red; font-family:'Roboto Mono', 'Menlo', 'Monaco', 'Consolas', monospace; font-weight:400; padding:10px\">\nError response from daemon: HCSShim::CreateComputeSystem failed in Win32: Unspecified error (0x80004005)</div>\n\nIf it doesn't have at least 2 cores, edit the VM to increase the cores and reboot. After rebooting, you then need to delete your container with `docker rm iisnanobase` before rerunning the `docker run` command above.\n\nIf you're using Hyper-V under Windows 10 and get this error, you may be running a Build of Windows 10 that does not support Nested Virtualisation.\n\n<div style=\"background-color:#ffcccc; color:red; font-family:'Roboto Mono', 'Menlo', 'Monaco', 'Consolas', monospace; font-weight:400; padding:10px\">\nError response from daemon: HCSShim::CreateComputeSystem failed in Win32: Not enough storage is available to complete this operation. (0xe)</div>\n\nUse winver to check if you have Build 10565 or later. If you don't, update via Windows Update.\n\n*-- End of issues.*\n\nIf you now find yourself in a cmd prompt and `hostname` returns minwinpc, congratulations, you're officially inside a running container!\n\n![](/content/images/2016/05/2016-hyperv-container-nano-server.png)\n\n**4. Create IIS web server container image**\n\n> *\"From within the container shell session, IIS can be installed using dism. Run the following commands to install IIS in the container.\"*\n\nThe quick start guide next takes you through configuring IIS within your nano server. This is done by accessing the share we mounted when we ran the container. Execute the following commands:\n\n```\ndism /online /apply-unattend:c:\\iisinstall\\unattend.xml\ndel C:\\inetpub\\wwwroot\\iisstart.htm\necho \"Hello World! You are less likely to be eaten by a grue inside a Hyper-V Container\" > C:\\inetpub\\wwwroot\\index.html\nnet start w3svc\n```\n\n![](/content/images/2016/05/server-2016-hyperv-container-configure-image.png)\n\n*-- What's a continer?*\n\nExecute `ipconfig` to get the IP address of your *container*, then run Internet Explorer on your Container Host to check IIS is serving content on that IP address. If (like me) your Container Host is server core, IE will not be an option. Instead, open a cmd prompt back on your container host (you can use CTRL+ALT+DEL > Task Manager to do this without exiting from the cmd shell of your container), run `Powershell` and execute:\n\n```language-powershell\ninvoke-webrequest <your ipaddress> -usebasicparsing \n```\n\n![](/content/images/2016/05/server-2016-container-iis-web-response.png)\n\n**5. Commit your configured container to the local repository**\n\nYou can now commit your configured container to the local image repository, so that each time you want to start it it's preconfigured in this way. Before you can do this the container needs to be in a stopped state. \n\nEnter `exit` in the cmd shell of your running container. This will return it to a stopped state, as you can see by entering `docker info`.\n\nFollowing this you can commit the container to the image repository by entering:\n```\ndocker commit iisnanobase windowsnanoiis\n```\nYou can see your committed container is in the repository by entering:\n```\ndocker images\n```\n\n![](/content/images/2016/05/2016-server-docker-commit-image.png)\n\n```\nAnd you can start your container running again (if you wish) by entering:\n\ndocker start iisnanobase\n```\n*-- I had an issue getting this to work until I rebooted. Its not clear whether this was an issue with my virtual environment or a bug in TP5.*\n\nYou can see your running containers with `docker ps` (include the -a switch to see all containers, including those not running).\n\n![](/content/images/2016/05/2016-server-docker-start-container.png)\n\n## Further Reading\n\n- [MSDN article on managing Hyper-V Containers](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/hyperv_container)\n- [MSDN article on managing Windows Containers](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/manage_images)\n- [Get started with Docker for Windows](https://docs.docker.com/windows/) a good guide that explains the basic concepts.",
                        "html": "<p>I'm just starting to get my head around the concept of Containers and as such decided to take Windows Server 2016 Technical Preview 5 for a spin, which includes Docker Containers as a feature. It's potentially worth being more explicit here (for anyone not aware) that this isn't Microsoft's version of Containers, this is <a href=\"https://www.docker.com/microsoft\">actually Docker baked in to Server 2016</a>. That's a pretty cool move on Microsoft's part.</p>\n\n<p>Things were already getting pretty meta in trying to run a Container inside a Virtual Machine on my desktop, but an early challenge I experienced in testing this out was getting the Hyper-V feature to install within a VM running under VirtualBox. The short answer seems to be you can't, due to a lack of support for Nested Virtualisation <a href=\"https://www.virtualbox.org/ticket/4032\">despite someone asking for the feature seven years ago</a>.</p>\n\n<p><em>-- This doesn't mean you can't test Server 2016 via VirtualBox btw, you just definitely can't test out Hyper-V containers. What's a Hyper-V container you ask?</em></p>\n\n<p><img src=\"/content/images/2016/05/0216-diagram.png\" alt=\"\" /></p>\n\n<blockquote>\n  <p><strong>Windows Containers</strong></p>\n  \n  <ul>\n  <li>Multiple container instances can run concurrently on a host, with isolation provided through namespace, resource control, and process isolation technologies. Windows Server containers share the same kernel with the host, as well as each other.</li>\n  </ul>\n  \n  <p><strong>Hyper-V Containers</strong></p>\n  \n  <ul>\n  <li>Multiple container instances can run concurrently on a host; however, each container runs inside of a special virtual machine. This provides kernel level isolation between each Hyper-V container and the container host.</li>\n  </ul>\n  \n  <p>Source: <a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/hyperv_container\">MSDN: Hyper-V Containers</a></p>\n</blockquote>\n\n<p><em>-- In summary, a Hyper-V container is more isolated. Containers are like living in a gated community, but a Hyper-V Container also lives in a cave in the woods.</em></p>\n\n<p>As I've noted previously, I suspect <a href=\"http://www.altaro.com/hyper-v/nested-virtualization-hyper-v-windows-server-2016/\">the Hyper-V feature on Windows 8/10 may support nested virtualisation</a>, but as i'm currently driving Windows 7 that wasn't an option for me, so instead I set up my Virtual Machine using VMWare Workstation 12 Player. </p>\n\n<p>If you haven't yet installed Windows Server 2016 <a href=\"http://wragg.io/installing-windows-2016-technical-preview-5-tp5/\">see my previous post on how I went about it</a> and <a href=\"http://wragg.io/installing-windows-server-2016-tp5-server-core\">this post for a little more detail on basic setup if you want to use Server Core</a>.</p>\n\n<p>Either way (to core or not to core) the next steps are as follows:</p>\n\n<p><em>-- The steps i'm about to describe are also covered in the <a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start\">MSDN Windows Containers Quick Start guide</a> and I strongly advise you check there first to ensure you have the latest information.</em></p>\n\n<p><strong>1. Configure your server to be a Container Host</strong></p>\n\n<p>There's a little more to doing this than just installing the Containers feature surprisingly (although that is the first step). However Microsoft have made this very easy by providing a Powershell script that does all the work (yay automation!) per <a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start_configure_host\">Step 2 of the Quick Start guide</a>.</p>\n\n<p>Log in to your machine and run <code>Powershell</code>. We're going to deploy to an existing system, so we should use the \"Existing System\" part of the guide. First you can download the Powershell script by executing this command:</p>\n\n<pre><code class=\"language-`language-powershell\">wget -uri https://aka.ms/tp5/Install-ContainerHost -OutFile C:\\Install-ContainerHost.ps1  \n</code></pre>\n\n<p>And then this to configure the system to host Hyper-V and Windows Containers:</p>\n\n<pre><code class=\"language-powershell\">powershell.exe -NoProfile -ExecutionPolicy Bypass C:\\Install-ContainerHost.ps1 -HyperV  \n</code></pre>\n\n<p><em>-- If you don't want to test out Hyper-V containers you can omit the -HyperV switch and I believe it will configure the server for Windows Containers instead.</em></p>\n\n<p>This installs the required features and performs reboots. It will automatically resume when you login after boot (it sets up a logon task to do so). This takes a short while (depending on your bandwidth) as it downloads a Windows Nano Server image file via <a href=\"https://github.com/OneGet/oneget\">OneGet</a>. Here's what it looks like while it's downloading:</p>\n\n<p><img src=\"/content/images/2016/05/Containers-downloading.png\" alt=\"\" /></p>\n\n<p>When it's done the script will retun \"Script complete!\" as well as list the available Docker images (which you can get at any time with <code>docker images</code>). If you're testing out Hyper-V containers you should see a 'nanoserver' image with a a tag of latest:</p>\n\n<pre><code>Base image is now tagged:  \nnanoserver   10.0.14300.1010   9db95268a387   6 weeks ago   817.1 MB  \nnanoserver   latest            9db95268a387   6 weeks ago   817.1 MB  \n</code></pre>\n\n<p>If you're testing Windows Containers, it will have downloaded you a 'windowsservercore' image instead.</p>\n\n<p>To see Docker system info (and confirm its working), type <code>docker info</code>:</p>\n\n<p><img src=\"/content/images/2016/05/server-2016-docker-info.png\" alt=\"\" /></p>\n\n<p>To see what else you can do with Docker type <code>Docker --help</code>.</p>\n\n<p><strong>2. Configure a Container</strong></p>\n\n<p>If you've configured your Container Host to run Hyper-V containers, you should now move to <a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/manage_docker_hyperv\">part 4 of the Quick Start guide</a> (part 3 covers Windows Containers). Again I will summarise the steps, but I recommend checking the guide directly for the latest info.</p>\n\n<p><em>-- This is particularly relevant advice, because this <a href=\"https://github.com/Microsoft/Virtualization-Documentation\">documentation is open source and in GitHub where anyone can submit improvements</a>, so it's likely to see regular improvements.</em></p>\n\n<p>Create a directory named c:\\share\\en-us. If you're working in Server Core you can do this with Powershell by executing:</p>\n\n<pre><code class=\"language-powershell\">New-Item -Type Directory c:\\share\\en-us  \n</code></pre>\n\n<blockquote>\n  <p><em>\"Hyper-V containers use the Nano Server base OS image. Because Nano Server is light weight operating system and does not include the IIS package, this needs to be obtained in order to complete this exercise. This can be found on the Window Server 2016 technical preview media under the NanoServer\\Packages directory.\"</em></p>\n</blockquote>\n\n<p><strong>Ensure you have the Server 2016 TP5 ISO media still mounted to a CD Drive as D:\\.</strong> Then copy the IIS package files to your container host by executing:</p>\n\n<pre><code class=\"language-powershell\">Copy-Item D:\\NanoServer\\Packages\\Microsoft-NanoServer-IIS-Package.cab C:\\share  \nCopy-Item D:\\NanoServer\\Packages\\en-us\\Microsoft-NanoServer-IIS-Package_en-us.cab C:\\share\\en-us  \n</code></pre>\n\n<p>Create a file in the c:\\share folder named unattend.xml and copy the following text into the unattend.xml file:</p>\n\n<pre><code class=\"language-xml\">&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;  \n    &lt;unattend xmlns=\"urn:schemas-microsoft-com:unattend\"&gt;\n    &lt;servicing&gt;\n        &lt;package action=\"install\"&gt;\n            &lt;assemblyIdentity name=\"Microsoft-NanoServer-IIS-Feature-Package\" version=\"10.0.14300.1000\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"neutral\" /&gt;\n            &lt;source location=\"c:\\iisinstall\\Microsoft-NanoServer-IIS-Package.cab\" /&gt;\n        &lt;/package&gt;\n        &lt;package action=\"install\"&gt;\n            &lt;assemblyIdentity name=\"Microsoft-NanoServer-IIS-Feature-Package\" version=\"10.0.14300.1000\" processorArchitecture=\"amd64\" publicKeyToken=\"31bf3856ad364e35\" language=\"en-US\" /&gt;\n            &lt;source location=\"c:\\iisinstall\\en-us\\Microsoft-NanoServer-IIS-Package_en-us.cab\" /&gt;\n        &lt;/package&gt;\n    &lt;/servicing&gt;\n    &lt;cpi:offlineImage cpi:source=\"\" xmlns:cpi=\"urn:schemas-microsoft-com:cpi\" /&gt;\n&lt;/unattend&gt;  \n</code></pre>\n\n<p><em>-- Even in Server Core you can do this by just running <code>notepad</code>.</em></p>\n\n<p>Your C:\\Share directory should now look like this:</p>\n\n<pre><code>c:\\share  \n|-- en-us\n|    |-- Microsoft-NanoServer-IIS-Package_en-us.cab\n|\n|-- Microsoft-NanoServer-IIS-Package.cab\n|-- unattend.xml\n</code></pre>\n\n<p><strong>3. Start a Container</strong></p>\n\n<p>Per the quickstart guide:</p>\n\n<blockquote>\n  <p><em>\"To create a Hyper-V container using docker, specify the --isolation=hyperv parameter. This example mounts the c:\\share directory from the host, to the c:\\iisinstall directory of the container, and then creates an interactive shell session with the container.\"</em></p>\n</blockquote>\n\n<p>Execute this:</p>\n\n<pre><code class=\"language-powershell\">docker run --name iisnanobase -it -p 80:80 -v c:\\share:c:\\iisinstall --isolation=hyperv nanoserver cmd  \n</code></pre>\n\n<p><em>-- Potential Issues:</em></p>\n\n<p>If you get an error similar to the below, you may need to ensure your host Virtual Machine has at least 2 CPU cores:</p>\n\n<div style=\"background-color:#ffcccc; color:red; font-family:'Roboto Mono', 'Menlo', 'Monaco', 'Consolas', monospace; font-weight:400; padding:10px\">  \nError response from daemon: HCSShim::CreateComputeSystem failed in Win32: Unspecified error (0x80004005)</div>\n\n<p>If it doesn't have at least 2 cores, edit the VM to increase the cores and reboot. After rebooting, you then need to delete your container with <code>docker rm iisnanobase</code> before rerunning the <code>docker run</code> command above.</p>\n\n<p>If you're using Hyper-V under Windows 10 and get this error, you may be running a Build of Windows 10 that does not support Nested Virtualisation.</p>\n\n<div style=\"background-color:#ffcccc; color:red; font-family:'Roboto Mono', 'Menlo', 'Monaco', 'Consolas', monospace; font-weight:400; padding:10px\">  \nError response from daemon: HCSShim::CreateComputeSystem failed in Win32: Not enough storage is available to complete this operation. (0xe)</div>\n\n<p>Use winver to check if you have Build 10565 or later. If you don't, update via Windows Update.</p>\n\n<p><em>-- End of issues.</em></p>\n\n<p>If you now find yourself in a cmd prompt and <code>hostname</code> returns minwinpc, congratulations, you're officially inside a running container!</p>\n\n<p><img src=\"/content/images/2016/05/2016-hyperv-container-nano-server.png\" alt=\"\" /></p>\n\n<p><strong>4. Create IIS web server container image</strong></p>\n\n<blockquote>\n  <p><em>\"From within the container shell session, IIS can be installed using dism. Run the following commands to install IIS in the container.\"</em></p>\n</blockquote>\n\n<p>The quick start guide next takes you through configuring IIS within your nano server. This is done by accessing the share we mounted when we ran the container. Execute the following commands:</p>\n\n<pre><code>dism /online /apply-unattend:c:\\iisinstall\\unattend.xml  \ndel C:\\inetpub\\wwwroot\\iisstart.htm  \necho \"Hello World! You are less likely to be eaten by a grue inside a Hyper-V Container\" &gt; C:\\inetpub\\wwwroot\\index.html  \nnet start w3svc  \n</code></pre>\n\n<p><img src=\"/content/images/2016/05/server-2016-hyperv-container-configure-image.png\" alt=\"\" /></p>\n\n<p><em>-- What's a continer?</em></p>\n\n<p>Execute <code>ipconfig</code> to get the IP address of your <em>container</em>, then run Internet Explorer on your Container Host to check IIS is serving content on that IP address. If (like me) your Container Host is server core, IE will not be an option. Instead, open a cmd prompt back on your container host (you can use CTRL+ALT+DEL > Task Manager to do this without exiting from the cmd shell of your container), run <code>Powershell</code> and execute:</p>\n\n<pre><code class=\"language-powershell\">invoke-webrequest &lt;your ipaddress&gt; -usebasicparsing  \n</code></pre>\n\n<p><img src=\"/content/images/2016/05/server-2016-container-iis-web-response.png\" alt=\"\" /></p>\n\n<p><strong>5. Commit your configured container to the local repository</strong></p>\n\n<p>You can now commit your configured container to the local image repository, so that each time you want to start it it's preconfigured in this way. Before you can do this the container needs to be in a stopped state. </p>\n\n<p>Enter <code>exit</code> in the cmd shell of your running container. This will return it to a stopped state, as you can see by entering <code>docker info</code>.</p>\n\n<p>Following this you can commit the container to the image repository by entering:  </p>\n\n<pre><code>docker commit iisnanobase windowsnanoiis  \n</code></pre>\n\n<p>You can see your committed container is in the repository by entering:  </p>\n\n<pre><code>docker images  \n</code></pre>\n\n<p><img src=\"/content/images/2016/05/2016-server-docker-commit-image.png\" alt=\"\" /></p>\n\n<pre><code>And you can start your container running again (if you wish) by entering:\n\ndocker start iisnanobase  \n</code></pre>\n\n<p><em>-- I had an issue getting this to work until I rebooted. Its not clear whether this was an issue with my virtual environment or a bug in TP5.</em></p>\n\n<p>You can see your running containers with <code>docker ps</code> (include the -a switch to see all containers, including those not running).</p>\n\n<p><img src=\"/content/images/2016/05/2016-server-docker-start-container.png\" alt=\"\" /></p>\n\n<h2 id=\"furtherreading\">Further Reading</h2>\n\n<ul>\n<li><a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/hyperv_container\">MSDN article on managing Hyper-V Containers</a></li>\n<li><a href=\"https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/manage_images\">MSDN article on managing Windows Containers</a></li>\n<li><a href=\"https://docs.docker.com/windows/\">Get started with Docker for Windows</a> a good guide that explains the basic concepts.</li>\n</ul>",
                        "image": "/content/images/2016/05/inception-1024-wallpaper-829789-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2016-05-05 19:01:09",
                        "created_by": 1,
                        "updated_at": "2016-06-06 11:39:51",
                        "updated_by": 1,
                        "published_at": "2016-05-07 17:57:12",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 18,
                        "uuid": "1afd90ec-cd78-40b1-a542-d22f894abf81",
                        "title": "Installing Windows Server 2016 TP5 (Technical Preview 5)",
                        "slug": "installing-windows-2016-technical-preview-5-tp5",
                        "markdown": "On the 27th of April 2016 Microsoft released [Technical Preview 5 of Windows Server 2016](https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview). I was keen to see the new version in action, particularly since attending a talk recently on Windows Containers, which have been a feature available for testing since TP3. I am going to explore Containers in a later post, but here i'm going to cover how I chose to get the OS up and running, as when I came to do it I struggled to find a recommendation on where to test it.\n \n**1. Choose a Hypervisor**\n\nThere are a number of desktop virtualisation tools to choose from. Initially I got Windows Server 2016 up and running under Oracle's VirtualBox, but I later discovered that it did not support Nested Virtualisation which means you can't install the Hyper-V feature within your VM and test out Hyper-V containers. As such I instead recommend VMWare Workstation 12 Player (which is free for non-commerical use).\n\n[Download VMWare Workstation 12 Player from here](http://www.vmware.com/uk/products/player) and install.\n\n> I suspect (but have not confirmed) that if you're running Windows 8 or 10, you could use the desktop Hyper-V feature instead, but as i'm running Windows 7 this wasn't an option.\n\n**2. Download the ISO**\n\nDownload the [Windows Server 2016 TP5 Full ISO](https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview). You'll need to register to get access to the link. \n\n**3. Create a Virtual Machine**\n\nIn VMWare Workstation select Create a new Virtual Machine. Point to the ISO, click Next. Select Windows and Server 2016 (obviously --but its nice Workstation has this as an option, VirtualBox did not). Name your VM and choose how large you want the disk. Finally click \"customise hardware\", click processors and tick \"Virtualize Intel VT-X/EPT or AMD-V/RVI\". Also ensure you set the VM to have at least 2 cores. Click OK and Finish. Select your VM and click \"Play virtual machine\". \n\n**4. Install Windows**\n\nThe installation process for Windows is fairly simple. Select your Language and follow through the Wizard. Beware there's no \"back\" in the interface so if you move on to the next stage and change your mind you'll need to start over.\n\nMost critically [beware that since TP3 you cannot turn the Windows GUI on and off with the Powershell Install-WindowsFeature command](https://blogs.technet.microsoft.com/windowsserver/2015/08/27/windows-server-2016-installation-option-changes/). So if you want to check out either the GUI or Server Core make sure you choose either the Windows with or without \"Desktop Experience\". \n\nThe license keys can be found on [the page where you downloaded the ISO](https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview#preinstall_21937) by expanding the Preinstall Information section. Also take a look at the info in the Description section to see the difference between Standard and Datacenter editions.\n\n![](/content/images/2016/05/Windows-server-2016-tp5-install.png)\n\nAfter Windows has completed installing you'll be prompted to set up a password for the default Admin account. If you chose Server Core, this will be via a command prompt interface. Weirdly you don't hit enter to move down to the next line but the arrow keys instead (i'd like to know what code drives this kind of interface, it seems useful).\n\nAfter this it's just a case of logging in:\n\n![](/content/images/2016/05/Windows-server-2016-tp5-login.png)\n\nAnd (in case you haven't seen it) here's what it looks like with a GUI:\n\n![](/content/images/2016/05/server-2016-gui.png)\n\n**5. Install updates**\n\nPer the [release notes](https://technet.microsoft.com/library/dn765470.aspx), its important that you install the [Cumulative Update for TP5](https://support.microsoft.com/en-gb/kb/3157663) and reboot before installing any roles or features. The easiest way to do this is to go to Settings > Update & security > Windows Update where it should be listed as an available update:\n\n![](/content/images/2016/05/2016-Updates.png)\n\nOptionally (and if you've used VMWare Workstation like me) you might now want to install VMWare tools to improve the experience of interacting with the VM. This installed cleanly for me with no issues and I recommend doing it.\n\nI will be installing and exploring Containers in a future post.\n\n## Further reading\n\nFor a full list of the features of Windows Server 2016, including those bundled in each of the Technical Previews, see the [Windows Server 2016 wikipedia page](https://en.wikipedia.org/wiki/Windows_Server_2016#Preview_releases).\n\nIf you already have or plan to install a Windows Server Core (no Desktop Experience) I have now written a follow up post that covers the [basic post-installation setup of Server 2016 Core](http://wragg.io/installing-windows-server-2016-tp5-server-core/).",
                        "html": "<p>On the 27th of April 2016 Microsoft released <a href=\"https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview\">Technical Preview 5 of Windows Server 2016</a>. I was keen to see the new version in action, particularly since attending a talk recently on Windows Containers, which have been a feature available for testing since TP3. I am going to explore Containers in a later post, but here i'm going to cover how I chose to get the OS up and running, as when I came to do it I struggled to find a recommendation on where to test it.</p>\n\n<p><strong>1. Choose a Hypervisor</strong></p>\n\n<p>There are a number of desktop virtualisation tools to choose from. Initially I got Windows Server 2016 up and running under Oracle's VirtualBox, but I later discovered that it did not support Nested Virtualisation which means you can't install the Hyper-V feature within your VM and test out Hyper-V containers. As such I instead recommend VMWare Workstation 12 Player (which is free for non-commerical use).</p>\n\n<p><a href=\"http://www.vmware.com/uk/products/player\">Download VMWare Workstation 12 Player from here</a> and install.</p>\n\n<blockquote>\n  <p>I suspect (but have not confirmed) that if you're running Windows 8 or 10, you could use the desktop Hyper-V feature instead, but as i'm running Windows 7 this wasn't an option.</p>\n</blockquote>\n\n<p><strong>2. Download the ISO</strong></p>\n\n<p>Download the <a href=\"https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview\">Windows Server 2016 TP5 Full ISO</a>. You'll need to register to get access to the link. </p>\n\n<p><strong>3. Create a Virtual Machine</strong></p>\n\n<p>In VMWare Workstation select Create a new Virtual Machine. Point to the ISO, click Next. Select Windows and Server 2016 (obviously --but its nice Workstation has this as an option, VirtualBox did not). Name your VM and choose how large you want the disk. Finally click \"customise hardware\", click processors and tick \"Virtualize Intel VT-X/EPT or AMD-V/RVI\". Also ensure you set the VM to have at least 2 cores. Click OK and Finish. Select your VM and click \"Play virtual machine\". </p>\n\n<p><strong>4. Install Windows</strong></p>\n\n<p>The installation process for Windows is fairly simple. Select your Language and follow through the Wizard. Beware there's no \"back\" in the interface so if you move on to the next stage and change your mind you'll need to start over.</p>\n\n<p>Most critically <a href=\"https://blogs.technet.microsoft.com/windowsserver/2015/08/27/windows-server-2016-installation-option-changes/\">beware that since TP3 you cannot turn the Windows GUI on and off with the Powershell Install-WindowsFeature command</a>. So if you want to check out either the GUI or Server Core make sure you choose either the Windows with or without \"Desktop Experience\". </p>\n\n<p>The license keys can be found on <a href=\"https://www.microsoft.com/en-gb/evalcenter/evaluate-windows-server-technical-preview#preinstall_21937\">the page where you downloaded the ISO</a> by expanding the Preinstall Information section. Also take a look at the info in the Description section to see the difference between Standard and Datacenter editions.</p>\n\n<p><img src=\"/content/images/2016/05/Windows-server-2016-tp5-install.png\" alt=\"\" /></p>\n\n<p>After Windows has completed installing you'll be prompted to set up a password for the default Admin account. If you chose Server Core, this will be via a command prompt interface. Weirdly you don't hit enter to move down to the next line but the arrow keys instead (i'd like to know what code drives this kind of interface, it seems useful).</p>\n\n<p>After this it's just a case of logging in:</p>\n\n<p><img src=\"/content/images/2016/05/Windows-server-2016-tp5-login.png\" alt=\"\" /></p>\n\n<p>And (in case you haven't seen it) here's what it looks like with a GUI:</p>\n\n<p><img src=\"/content/images/2016/05/server-2016-gui.png\" alt=\"\" /></p>\n\n<p><strong>5. Install updates</strong></p>\n\n<p>Per the <a href=\"https://technet.microsoft.com/library/dn765470.aspx\">release notes</a>, its important that you install the <a href=\"https://support.microsoft.com/en-gb/kb/3157663\">Cumulative Update for TP5</a> and reboot before installing any roles or features. The easiest way to do this is to go to Settings > Update &amp; security > Windows Update where it should be listed as an available update:</p>\n\n<p><img src=\"/content/images/2016/05/2016-Updates.png\" alt=\"\" /></p>\n\n<p>Optionally (and if you've used VMWare Workstation like me) you might now want to install VMWare tools to improve the experience of interacting with the VM. This installed cleanly for me with no issues and I recommend doing it.</p>\n\n<p>I will be installing and exploring Containers in a future post.</p>\n\n<h2 id=\"furtherreading\">Further reading</h2>\n\n<p>For a full list of the features of Windows Server 2016, including those bundled in each of the Technical Previews, see the <a href=\"https://en.wikipedia.org/wiki/Windows_Server_2016#Preview_releases\">Windows Server 2016 wikipedia page</a>.</p>\n\n<p>If you already have or plan to install a Windows Server Core (no Desktop Experience) I have now written a follow up post that covers the <a href=\"http://wragg.io/installing-windows-server-2016-tp5-server-core/\">basic post-installation setup of Server 2016 Core</a>.</p>",
                        "image": "/content/images/2016/05/windows_10__hero__background__correct_scale__by_gamerverise-d903d5f.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "How to install Windows Server 2016 TP5 (Technical Preview 5)",
                        "meta_description": "This article covers how to install the TP5 release of Windows Server 2016 on to a Virtual Machine within VMWare  Workstation 12 Player.",
                        "author_id": 1,
                        "created_at": "2016-05-05 20:24:20",
                        "created_by": 1,
                        "updated_at": "2016-05-10 09:00:13",
                        "updated_by": 1,
                        "published_at": "2016-05-05 21:57:16",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 19,
                        "uuid": "4a2d6894-9bee-4558-9f4c-187fa63917dc",
                        "title": "Installing Windows Server 2016 TP5 Server Core",
                        "slug": "installing-windows-server-2016-tp5-server-core",
                        "markdown": "Previously I covered [performing the installation of Windows Server 2016 Technical Preview 5](http://wragg.io/installing-windows-2016-technical-preview-5-tp5/) in to a desktop VM with the Desktop Experience (GUI) mode enabled. This post covers the installation of the GUI-less Server Core mode and subsequently how to manage and maintain it.\n\n> Beware that since TP3 they have [removed the ability to turn the Windows GUI on and off after installation](https://technet.microsoft.com/en-us/library/mt427865.aspx) that was possible in previous versions, so like Indiana Jones, [choose wisely](https://www.youtube.com/watch?v=0H3rdfI28s0).\n\n**1. Install Windows**\n\nTo get Windows installed simply [follow the first 4 steps of my previous article](http://wragg.io/installing-windows-2016-technical-preview-5-tp5/), ensuring that on this occasion you choose to install a version of Windows *without* Desktop Experience. \n\nThat should get you here:\n\n![](/content/images/2016/05/2016-password.png)\n\nAnd after setting a password and signing in, here:\n\n![](/content/images/2016/05/2016-core.png)\n\n*-- It is pitch black. You are likely to be eaten by a grue.*\n\n**2. Install Updates**\n\nYou should first ensure you install the [Cumulative Update for TP5](**http://**). The release notes state its important to do this before installing any Roles or Features or bugs will occur (and you can't just install it later).  There's a handy tool to do server configuration and updates from the command-line. \n\nSimply enter `sconfig` to access this interface:\n\n![](/content/images/2016/05/sconfig.png)\n\nSelect option 6 and then A to search for all updates.\n\n![](/content/images/2016/05/Updates-1.png)\n\nInstall the updates and reboot.\n\nIf (like me) you're using VMWare Workstation or similar now is a good time to install VMWare tools (or similar). With VMWare tools you mount the ISO to the CD drive via the client as usual (Player > Manage > Install VMWare tools) then enter:\n```\ncd D:\nsetup64.exe /S /v \"/qn REBOOT=Y\"\n```\nThe server will reboot automatically.\n\n**3. Install Remote Management tools**\n\nThe easiest way to manage your server from this point forward is to use the Remote Tools. These have been a Windows Feature that you can install for some time, but it seems formal support for managing Windows Server 2016 is currently only provided by the [RSAT tools for Windows 10, which you can download here](https://www.microsoft.com/en-gb/download/details.aspx?id=45520).\n\n**4. Install Roles/Features**\n\nYou can install roles and features with the Server Manager remote tool (in the same way you can locally when you have a GUI), but if you'd like to do it more directly from the server you can use Powershell. Bear in mind by default you're in a cmd shell, so to access powershell simply run `powershell`. Your prompt will change to be prefixed with PS.\n\nTo see the features currently installed execute `get-windowsfeature`. To install a feature enter `install-windowsfeature <name>`. Remove a feature with `remove-windowsfeature <name>`.\n\n![](/content/images/2016/05/powershell-windowsfeature.png)\n\n## Further Reading\n\nBruce Adamczak has published a very thorough [Server Core Survival Guide](https://blogs.technet.microsoft.com/bruce_adamczak/2013/01/15/2012-core-survival-guide/) for Windows 2012 which I suspect is (for the most part) still applicable to 2016.",
                        "html": "<p>Previously I covered <a href=\"http://wragg.io/installing-windows-2016-technical-preview-5-tp5/\">performing the installation of Windows Server 2016 Technical Preview 5</a> in to a desktop VM with the Desktop Experience (GUI) mode enabled. This post covers the installation of the GUI-less Server Core mode and subsequently how to manage and maintain it.</p>\n\n<blockquote>\n  <p>Beware that since TP3 they have <a href=\"https://technet.microsoft.com/en-us/library/mt427865.aspx\">removed the ability to turn the Windows GUI on and off after installation</a> that was possible in previous versions, so like Indiana Jones, <a href=\"https://www.youtube.com/watch?v=0H3rdfI28s0\">choose wisely</a>.</p>\n</blockquote>\n\n<p><strong>1. Install Windows</strong></p>\n\n<p>To get Windows installed simply <a href=\"http://wragg.io/installing-windows-2016-technical-preview-5-tp5/\">follow the first 4 steps of my previous article</a>, ensuring that on this occasion you choose to install a version of Windows <em>without</em> Desktop Experience. </p>\n\n<p>That should get you here:</p>\n\n<p><img src=\"/content/images/2016/05/2016-password.png\" alt=\"\" /></p>\n\n<p>And after setting a password and signing in, here:</p>\n\n<p><img src=\"/content/images/2016/05/2016-core.png\" alt=\"\" /></p>\n\n<p><em>-- It is pitch black. You are likely to be eaten by a grue.</em></p>\n\n<p><strong>2. Install Updates</strong></p>\n\n<p>You should first ensure you install the <a href=\"**http://**\">Cumulative Update for TP5</a>. The release notes state its important to do this before installing any Roles or Features or bugs will occur (and you can't just install it later).  There's a handy tool to do server configuration and updates from the command-line. </p>\n\n<p>Simply enter <code>sconfig</code> to access this interface:</p>\n\n<p><img src=\"/content/images/2016/05/sconfig.png\" alt=\"\" /></p>\n\n<p>Select option 6 and then A to search for all updates.</p>\n\n<p><img src=\"/content/images/2016/05/Updates-1.png\" alt=\"\" /></p>\n\n<p>Install the updates and reboot.</p>\n\n<p>If (like me) you're using VMWare Workstation or similar now is a good time to install VMWare tools (or similar). With VMWare tools you mount the ISO to the CD drive via the client as usual (Player > Manage > Install VMWare tools) then enter:  </p>\n\n<pre><code>cd D:  \nsetup64.exe /S /v \"/qn REBOOT=Y\"  \n</code></pre>\n\n<p>The server will reboot automatically.</p>\n\n<p><strong>3. Install Remote Management tools</strong></p>\n\n<p>The easiest way to manage your server from this point forward is to use the Remote Tools. These have been a Windows Feature that you can install for some time, but it seems formal support for managing Windows Server 2016 is currently only provided by the <a href=\"https://www.microsoft.com/en-gb/download/details.aspx?id=45520\">RSAT tools for Windows 10, which you can download here</a>.</p>\n\n<p><strong>4. Install Roles/Features</strong></p>\n\n<p>You can install roles and features with the Server Manager remote tool (in the same way you can locally when you have a GUI), but if you'd like to do it more directly from the server you can use Powershell. Bear in mind by default you're in a cmd shell, so to access powershell simply run <code>powershell</code>. Your prompt will change to be prefixed with PS.</p>\n\n<p>To see the features currently installed execute <code>get-windowsfeature</code>. To install a feature enter <code>install-windowsfeature &lt;name&gt;</code>. Remove a feature with <code>remove-windowsfeature &lt;name&gt;</code>.</p>\n\n<p><img src=\"/content/images/2016/05/powershell-windowsfeature.png\" alt=\"\" /></p>\n\n<h2 id=\"furtherreading\">Further Reading</h2>\n\n<p>Bruce Adamczak has published a very thorough <a href=\"https://blogs.technet.microsoft.com/bruce_adamczak/2013/01/15/2012-core-survival-guide/\">Server Core Survival Guide</a> for Windows 2012 which I suspect is (for the most part) still applicable to 2016.</p>",
                        "image": "/content/images/2016/05/you_are_likely_to_be_eaten_by_a_grue_by_nmajmani-d4qbkrg-3.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-06 11:32:17",
                        "created_by": 1,
                        "updated_at": "2016-05-10 09:00:28",
                        "updated_by": 1,
                        "published_at": "2016-05-06 16:39:57",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 20,
                        "uuid": "ef850c92-ef92-4180-874c-669aa2d28a23",
                        "title": "Managing and maintaining Containers in Windows Server 2016",
                        "slug": "managing-and-maintaining-containers-in-windows-server-2016",
                        "markdown": "Continuing my previous post which got a container running under win 2016 tp5 this post will further explore the commands and options available to you.\n\n# Topics to cover\n\n- stop start pause containers \n- docker search\n- top\n- configuring network access for a container in a VM\n",
                        "html": "<p>Continuing my previous post which got a container running under win 2016 tp5 this post will further explore the commands and options available to you.</p>\n\n<h1 id=\"topicstocover\">Topics to cover</h1>\n\n<ul>\n<li>stop start pause containers </li>\n<li>docker search</li>\n<li>top</li>\n<li>configuring network access for a container in a VM</li>\n</ul>",
                        "image": "/content/images/2016/06/Inception.png",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-09 19:59:28",
                        "created_by": 1,
                        "updated_at": "2016-09-05 21:54:36",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 21,
                        "uuid": "28086c01-bb5b-42cf-a56a-ebce49ed1e3c",
                        "title": "Automate yourself a better job with Powershell",
                        "slug": "automate-yourself-a-better-job-with-powershell",
                        "markdown": "The phrase \"automate yourself out of a job\" is too easily misinterpreted as a negative. Without any context it's possible to view it as to mean *you shouldn't automate things. If you do you'll end up unemployed or could make others unemployed.* In IT at least, this is unlikely to be true. Powershell is 10 years old later this year and if you're not using it, here's why you should.\n\nI've never met anyone that has told me they scripted themselves redundant. Instead automation should free you from mundane tasks and give you time back to focus on other improvement activities and projects. But many people still prefer to perform tasks manually and (in the Windows space at least) I've encountered a lot of people that do not script on a regular basis as part of their role and are getting along just fine (for now) with little to no Powershell ability.\n\n## Fear of automation\n\nI doubt most people genuinely believe automating things would lead to their job loss. In fact its more than likely to achieve the opposite. So what are some of the reasons people are afraid to automate?\n\n![Skynet](/content/images/2016/05/terminator.jpg)\n\n*-- Other than inadvertandly inventing Skynet of course.*\n\n- **It's too hard.** If you're new to scripting, or to a particular scripting language, the barrier to entry can seem quite high. There is a steep learning curve that is offputting, particularly where the manual steps of the task are well known and are quicker to achieve when performing the task in isolation. \n- **I could break more things faster.** *\"If i'd scripted this, i'd have broken everything!\"*, it's certainly much easier to have a wider impact when using automation scripts and if an error is introduced that causes a problem, that problem could be many times worse than if the task was being done manually. \n- **Permanent ownership.** If the environment you are in has little to no automation, you could worry that you would find yourself solely responsible for the ongoing maintenance of your script, as well as effectively adopting the automated task as an individual. If the script goes wrong, you will always be called upon to investigate why.\n\n## Conquering these fears\n\nIf you find any of the above relatable, consider these tips:\n\n- **Write small scripts often.** Use code whenever possible. Powershell is an administration language as well as a scripting language. This means it's just as valid to fire up the console and write one of two commands as it is to write a multi-line script. Need to restart a service? Stop opening that mmc. Get-service servicename | restart-service.\n- **Dedicate time to focus.** When you are ready to do some longer script development, shut down your email. Close your chat client. Block out your calendar. Developers know you need a few solid hours uninterrupted to make real progress.\n- **Build a network of resources.** The internet is teeming with resources to help you and it's ever growing by it's very nature. That doesn't have to be limited to websites, there are podcasts, meet ups, communities, conferences, hackathons as well as (gulp) books. See my [Resources](http://wragg.io/Resources/) page for a starting point and consider building your own. Also, check out the [Microsoft Virtual Academy](https://mva.microsoft.com/en-us/training-courses/getting-started-with-powershell-30-jump-start-8276).\n- **Test often and use safety switches.** Powershell has some great built in options to keep you from making mistakes. Use [-whatif and -confirm switches](http://www.computerperformance.co.uk/powershell/powershell_whatif_confirm.htm) to get a console output of what a command will do before it does it, or a console prompt. Use these often in your initial test. Write [Pester](http://www.powershellmagazine.com/2014/03/12/get-started-with-pester-powershell-unit-testing-framework/) scripts. Since PowerShell version 4 [Pester](http://www.powershellmagazine.com/2014/03/12/get-started-with-pester-powershell-unit-testing-framework/) has existed as a framework to allow you to code validation tests for your scripts. Consider writing your tests first, then code. This is called [Test Driven Development](http://www.hurryupandwait.io/blog/why-tdd-for-powershell-or-why-pester-or-why-unit-test-scripting-language). If you're using Source Control, check your code in and then have it peer reviewed. Even if you're not, get a second person to review and critique your code. If you're doing complex commands (particularly when using the pipeline) consider temporarily breaking your code up and run parts in isolation to check it is returning the results you expect. Finally, if you can, always deploy your code first in a test environment.\n- **Inspire and train others.** To ensure you don't end up a Single Point of Failure, you need to ensure others are as invested in automation as you are. Share your resources with them. Help them to develop scripting skills. Run some training sessions in-house, or seek external training. Getting people over that initial hump is the hardest part but once they are invested, enthusiasm multiplies.\n\n## Benefits of automation\n\n![](/content/images/2016/05/grass-1.jpg)\n\n- **Job satisfaction**. Repetitive tasks are boring. It might feel like job security now but it's not guaranteed to be in the future. Scripting is fun and seeing work complete itself is deeply satisfying.\n- **Consistent delivery**. Failure is not an option. Automation can ensure a consistent result every time. No more risk of human error introducing a small unnoticed problem, or resulting in hours of troubleshooting. And if you develop your scripts in a small and often way (Continuous Delivery) and use Source Control rolling back from a problem is easy due to version control.\n- **Free your time to do more interesting things**. Manual tasks suck time. Free time is precious and gives rise to great opportunity, such as automating something else :).\n\nThere will always be more things to do in IT than there is time to complete. Automation gives us more opportunity to make smart choices about what those things are.",
                        "html": "<p>The phrase \"automate yourself out of a job\" is too easily misinterpreted as a negative. Without any context it's possible to view it as to mean <em>you shouldn't automate things. If you do you'll end up unemployed or could make others unemployed.</em> In IT at least, this is unlikely to be true. Powershell is 10 years old later this year and if you're not using it, here's why you should.</p>\n\n<p>I've never met anyone that has told me they scripted themselves redundant. Instead automation should free you from mundane tasks and give you time back to focus on other improvement activities and projects. But many people still prefer to perform tasks manually and (in the Windows space at least) I've encountered a lot of people that do not script on a regular basis as part of their role and are getting along just fine (for now) with little to no Powershell ability.</p>\n\n<h2 id=\"fearofautomation\">Fear of automation</h2>\n\n<p>I doubt most people genuinely believe automating things would lead to their job loss. In fact its more than likely to achieve the opposite. So what are some of the reasons people are afraid to automate?</p>\n\n<p><img src=\"/content/images/2016/05/terminator.jpg\" alt=\"Skynet\" /></p>\n\n<p><em>-- Other than inadvertandly inventing Skynet of course.</em></p>\n\n<ul>\n<li><strong>It's too hard.</strong> If you're new to scripting, or to a particular scripting language, the barrier to entry can seem quite high. There is a steep learning curve that is offputting, particularly where the manual steps of the task are well known and are quicker to achieve when performing the task in isolation. </li>\n<li><strong>I could break more things faster.</strong> <em>\"If i'd scripted this, i'd have broken everything!\"</em>, it's certainly much easier to have a wider impact when using automation scripts and if an error is introduced that causes a problem, that problem could be many times worse than if the task was being done manually. </li>\n<li><strong>Permanent ownership.</strong> If the environment you are in has little to no automation, you could worry that you would find yourself solely responsible for the ongoing maintenance of your script, as well as effectively adopting the automated task as an individual. If the script goes wrong, you will always be called upon to investigate why.</li>\n</ul>\n\n<h2 id=\"conqueringthesefears\">Conquering these fears</h2>\n\n<p>If you find any of the above relatable, consider these tips:</p>\n\n<ul>\n<li><strong>Write small scripts often.</strong> Use code whenever possible. Powershell is an administration language as well as a scripting language. This means it's just as valid to fire up the console and write one of two commands as it is to write a multi-line script. Need to restart a service? Stop opening that mmc. Get-service servicename | restart-service.</li>\n<li><strong>Dedicate time to focus.</strong> When you are ready to do some longer script development, shut down your email. Close your chat client. Block out your calendar. Developers know you need a few solid hours uninterrupted to make real progress.</li>\n<li><strong>Build a network of resources.</strong> The internet is teeming with resources to help you and it's ever growing by it's very nature. That doesn't have to be limited to websites, there are podcasts, meet ups, communities, conferences, hackathons as well as (gulp) books. See my <a href=\"http://wragg.io/Resources/\">Resources</a> page for a starting point and consider building your own. Also, check out the <a href=\"https://mva.microsoft.com/en-us/training-courses/getting-started-with-powershell-30-jump-start-8276\">Microsoft Virtual Academy</a>.</li>\n<li><strong>Test often and use safety switches.</strong> Powershell has some great built in options to keep you from making mistakes. Use <a href=\"http://www.computerperformance.co.uk/powershell/powershell_whatif_confirm.htm\">-whatif and -confirm switches</a> to get a console output of what a command will do before it does it, or a console prompt. Use these often in your initial test. Write <a href=\"http://www.powershellmagazine.com/2014/03/12/get-started-with-pester-powershell-unit-testing-framework/\">Pester</a> scripts. Since PowerShell version 4 <a href=\"http://www.powershellmagazine.com/2014/03/12/get-started-with-pester-powershell-unit-testing-framework/\">Pester</a> has existed as a framework to allow you to code validation tests for your scripts. Consider writing your tests first, then code. This is called <a href=\"http://www.hurryupandwait.io/blog/why-tdd-for-powershell-or-why-pester-or-why-unit-test-scripting-language\">Test Driven Development</a>. If you're using Source Control, check your code in and then have it peer reviewed. Even if you're not, get a second person to review and critique your code. If you're doing complex commands (particularly when using the pipeline) consider temporarily breaking your code up and run parts in isolation to check it is returning the results you expect. Finally, if you can, always deploy your code first in a test environment.</li>\n<li><strong>Inspire and train others.</strong> To ensure you don't end up a Single Point of Failure, you need to ensure others are as invested in automation as you are. Share your resources with them. Help them to develop scripting skills. Run some training sessions in-house, or seek external training. Getting people over that initial hump is the hardest part but once they are invested, enthusiasm multiplies.</li>\n</ul>\n\n<h2 id=\"benefitsofautomation\">Benefits of automation</h2>\n\n<p><img src=\"/content/images/2016/05/grass-1.jpg\" alt=\"\" /></p>\n\n<ul>\n<li><strong>Job satisfaction</strong>. Repetitive tasks are boring. It might feel like job security now but it's not guaranteed to be in the future. Scripting is fun and seeing work complete itself is deeply satisfying.</li>\n<li><strong>Consistent delivery</strong>. Failure is not an option. Automation can ensure a consistent result every time. No more risk of human error introducing a small unnoticed problem, or resulting in hours of troubleshooting. And if you develop your scripts in a small and often way (Continuous Delivery) and use Source Control rolling back from a problem is easy due to version control.</li>\n<li><strong>Free your time to do more interesting things</strong>. Manual tasks suck time. Free time is precious and gives rise to great opportunity, such as automating something else :).</li>\n</ul>\n\n<p>There will always be more things to do in IT than there is time to complete. Automation gives us more opportunity to make smart choices about what those things are.</p>",
                        "image": "/content/images/2016/05/Robot-Hands-Keyboard.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-09 20:03:08",
                        "created_by": 1,
                        "updated_at": "2016-05-10 19:33:04",
                        "updated_by": 1,
                        "published_at": "2016-05-10 00:00:06",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 23,
                        "uuid": "4081696d-8db0-40cd-9fcf-80d44e287149",
                        "title": "Automating ITIL Change",
                        "slug": "automating-itil-change",
                        "markdown": "ITIL is a standard set of IT service definitions and best practices that have been adopted by many organisations. How an organisation implements ITIL processes is up to them and this can vary significantly. ITIL is generally considered the antithesis  of Agile, which prefers speed above all else, but in theory (from my perspective currently) there are ways to continue to utilise the and conform with ITIL processes while working in an Agile way and permitting a culture of Continuous Deployment. The most significant process that we need to consider is how we manage Change.\n\nChange Management is attractive because it offers to bring order to what is typically an anarchic IT organisation. Prior to implementing Change Management, many organisations suffer with rampant uncontrolled change which brings with it the risk of long periods of disruption. Change Management provides a somewhat solution to this by insisting all changes are tracked and reviewed. This has one excellent benefit if adhered to by all participants which is that all changes are at least recorded. As such when disruption occurs its at least simple to determine why and downtime as a result is generally reduced.\n\nUnfortunately the downside to this is that changes generally take longer to occur. Even in its simplest form, the act of the process slows down the engineers ability to make changes. To the extreme, this can be by a factor of days, where changes may need to go via multiple individuals for approval or be presented as a CAB (Change Approval Board) meeting for review. This is sometimes suitable particularly for complex changes and as long as testing has been possible, not a huge inconvenience. However, if you're not able to fully and realistically test your changes, the change process (however small) delays implementation, particularly as each change failure results in more process.\n\nSo, ITIL isn't going way. Agile and Continuous Deployment shouldn't be ignored. What's the answer?\n\n## Standard Changes\n\nITIL already considers a method for expediting certain changes through the definition of a \"standard\" change. This allows for a change to be recorded but pre-approved, so eliminates the delay that comes with change approvals. However, typically it's still a manual process for an engineer to record that they are performing the change (which standard change they are performing, to which system/customer/product, at what time etc.). And manual processes come with the usual risks:\n\n- They take time and interrupt flow.\n- They might not be followed completely (resulting in inaccuracy).\n- They might not be followed at all.\n\n**What if you could automate change?**\n\nThe act of recording Standard changes is just begging to be automated. There's options for how to achieve this:\n\n- If the change itself is code driven, have the code record the change as a first step. This would require wherever the code is being executed to be able to communicate with the change system, but otherwise we have all the information we need: who's executing the change? (who's running the script) when is it occurring? (right now -- or at a scheduled time, which spawns a scheduled task).\n- If the change is not code driven, then we could execute a separate script as part of the process. That would be particularly possible if the process is in an online system where clicking a link could cause code to be executed. At least that then doesn't break flow.",
                        "html": "<p>ITIL is a standard set of IT service definitions and best practices that have been adopted by many organisations. How an organisation implements ITIL processes is up to them and this can vary significantly. ITIL is generally considered the antithesis  of Agile, which prefers speed above all else, but in theory (from my perspective currently) there are ways to continue to utilise the and conform with ITIL processes while working in an Agile way and permitting a culture of Continuous Deployment. The most significant process that we need to consider is how we manage Change.</p>\n\n<p>Change Management is attractive because it offers to bring order to what is typically an anarchic IT organisation. Prior to implementing Change Management, many organisations suffer with rampant uncontrolled change which brings with it the risk of long periods of disruption. Change Management provides a somewhat solution to this by insisting all changes are tracked and reviewed. This has one excellent benefit if adhered to by all participants which is that all changes are at least recorded. As such when disruption occurs its at least simple to determine why and downtime as a result is generally reduced.</p>\n\n<p>Unfortunately the downside to this is that changes generally take longer to occur. Even in its simplest form, the act of the process slows down the engineers ability to make changes. To the extreme, this can be by a factor of days, where changes may need to go via multiple individuals for approval or be presented as a CAB (Change Approval Board) meeting for review. This is sometimes suitable particularly for complex changes and as long as testing has been possible, not a huge inconvenience. However, if you're not able to fully and realistically test your changes, the change process (however small) delays implementation, particularly as each change failure results in more process.</p>\n\n<p>So, ITIL isn't going way. Agile and Continuous Deployment shouldn't be ignored. What's the answer?</p>\n\n<h2 id=\"standardchanges\">Standard Changes</h2>\n\n<p>ITIL already considers a method for expediting certain changes through the definition of a \"standard\" change. This allows for a change to be recorded but pre-approved, so eliminates the delay that comes with change approvals. However, typically it's still a manual process for an engineer to record that they are performing the change (which standard change they are performing, to which system/customer/product, at what time etc.). And manual processes come with the usual risks:</p>\n\n<ul>\n<li>They take time and interrupt flow.</li>\n<li>They might not be followed completely (resulting in inaccuracy).</li>\n<li>They might not be followed at all.</li>\n</ul>\n\n<p><strong>What if you could automate change?</strong></p>\n\n<p>The act of recording Standard changes is just begging to be automated. There's options for how to achieve this:</p>\n\n<ul>\n<li>If the change itself is code driven, have the code record the change as a first step. This would require wherever the code is being executed to be able to communicate with the change system, but otherwise we have all the information we need: who's executing the change? (who's running the script) when is it occurring? (right now -- or at a scheduled time, which spawns a scheduled task).</li>\n<li>If the change is not code driven, then we could execute a separate script as part of the process. That would be particularly possible if the process is in an online system where clicking a link could cause code to be executed. At least that then doesn't break flow.</li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-14 17:21:19",
                        "created_by": 1,
                        "updated_at": "2016-09-05 21:54:50",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 24,
                        "uuid": "b164eb59-23df-409f-ad2f-167200aece01",
                        "title": "(sort of) Updating the Powershell ISE",
                        "slug": "sort-of-updating-the-powershell-ise",
                        "markdown": "The Powershell ISE is now being developed independently of the Windows Management Framework. What does this mean?\n\n**1. Install Windows Management Framework 5.**\n\n**2. Run Install-Module PowerShellISE-preview.**\n\n**3. Run it with isep**\n\n**4. For start menu shortcuts run Install-ISEPreviewShortcuts**\n\n> https://blogs.msdn.microsoft.com/powershell/2016/01/20/introducing-the-windows-powershell-ise-preview/\n\n- Follow David Wilson: https://twitter.com/daviwil\n- Hear him talk about it here: http://powershell.org/wp/2016/02/08/up-next-david-wilson-from-microsoft-talks-about-powershell-ise-vs-code/\n\n\n# Visual Studio Code\n\nhttps://code.visualstudio.com/Docs/?dv=win\n\nCTRL+K, M -> set language to Powershell\n\nInstall powershell extension: https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell",
                        "html": "<p>The Powershell ISE is now being developed independently of the Windows Management Framework. What does this mean?</p>\n\n<p><strong>1. Install Windows Management Framework 5.</strong></p>\n\n<p><strong>2. Run Install-Module PowerShellISE-preview.</strong></p>\n\n<p><strong>3. Run it with isep</strong></p>\n\n<p><strong>4. For start menu shortcuts run Install-ISEPreviewShortcuts</strong></p>\n\n<blockquote>\n  <p><a href=\"https://blogs.msdn.microsoft.com/powershell/2016/01/20/introducing-the-windows-powershell-ise-preview/\">https://blogs.msdn.microsoft.com/powershell/2016/01/20/introducing-the-windows-powershell-ise-preview/</a></p>\n</blockquote>\n\n<ul>\n<li>Follow David Wilson: <a href=\"https://twitter.com/daviwil\">https://twitter.com/daviwil</a></li>\n<li>Hear him talk about it here: <a href=\"http://powershell.org/wp/2016/02/08/up-next-david-wilson-from-microsoft-talks-about-powershell-ise-vs-code/\">http://powershell.org/wp/2016/02/08/up-next-david-wilson-from-microsoft-talks-about-powershell-ise-vs-code/</a></li>\n</ul>\n\n<h1 id=\"visualstudiocode\">Visual Studio Code</h1>\n\n<p><a href=\"https://code.visualstudio.com/Docs/?dv=win\">https://code.visualstudio.com/Docs/?dv=win</a></p>\n\n<p>CTRL+K, M -> set language to Powershell</p>\n\n<p>Install powershell extension: <a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell\">https://marketplace.visualstudio.com/items?itemName=ms-vscode.PowerShell</a></p>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-15 14:16:45",
                        "created_by": 1,
                        "updated_at": "2016-09-05 21:55:15",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 25,
                        "uuid": "5baf0a08-a8b6-4e4d-b25f-6bff4b7ad772",
                        "title": "Get uptime from multiple servers with Powershell",
                        "slug": "get-uptime-from-multiple-servers",
                        "markdown": "The following script can be used to get the current uptime from a collection of servers in Active Directory using WMI. I used it as a way to audit our estate, keen to understand how long servers have been operational for, in part to identify those which were potentially not routinely receiving Windows Updates.\n\n![](/content/images/2016/08/20130306-005558.jpg)\n\n> Note that as this queries WMI remotely you'll need to ensure the ports for WMI are open on any firewalls in between. By default WMI uses port 135 then a randomly assigned high numbered port, but you can make that static per this guidance: https://msdn.microsoft.com/en-us/library/bb219447(v=vs.85).aspx\n\n**The script**\n\nThe crux of the script is the `Get-WmiObject -ComputerName $computer.name win32_operatingsystem` part which we then use to get the `lastbootuptime` parameter, but there are some optional constructs which I have detailed after the script below.\n\n<script src=\"https://gist.github.com/markwragg/e873167fccd09656bf36d848f7995bd0.js\"></script>\n\nThe script uses `write-progress` to show progress. In order to ensure any verbose or warning messages do not appear underneath the progress bar, I use `write-host` to output 5 blank lines when the script first runs.\n\n![](/content/images/2016/08/get-uptime2-1.png)\n\nIf you run the script with the `-verbose` parameter, you will see each uptime printed to the console as it's obtained (this is not shown above). \n\n**Script framework**\n\nAs with most scripts, I've put a framework around the core functionality to add flexibility, capture errors and show progress. These are explained below.\n\n- Line 4: I have servers organised in to OUs named after their location, so my `$location` parameter allows me to filter the results to one or more of these locations (note you can enter more than one at once, just by comma separating). This isn't a mandatory Parameter so not specifying this just searches all of AD, however you may want to run the script in different locations close to the servers for speed if you can also filter in this way.\n- Line 10: I have servers in my estate which are non-Windows but are Domain joined, so I also filter on Operating System name, by default looking for ones with \"Windows\" in their name (you could remove this if it's not relevant to you).\n- Line 19: I use Write-Progress to show which server is currently being polled and how far through the set of servers we are.\n- Line 21: I use Test-Connection to check the server is responsive, this speeds up the script for any dead or inaccessible servers.\n- Lines 36-42 & 49: I trap any errors that occur in to an object, and at the end of the script spit these out to a separate file for analysis.\n- Line 46: I output my results sorted by most to least uptime.\n",
                        "html": "<p>The following script can be used to get the current uptime from a collection of servers in Active Directory using WMI. I used it as a way to audit our estate, keen to understand how long servers have been operational for, in part to identify those which were potentially not routinely receiving Windows Updates.</p>\n\n<p><img src=\"/content/images/2016/08/20130306-005558.jpg\" alt=\"\" /></p>\n\n<blockquote>\n  <p>Note that as this queries WMI remotely you'll need to ensure the ports for WMI are open on any firewalls in between. By default WMI uses port 135 then a randomly assigned high numbered port, but you can make that static per this guidance: <a href=\"https://msdn.microsoft.com/en-us/library/bb219447(v=vs.85).aspx\">https://msdn.microsoft.com/en-us/library/bb219447(v=vs.85).aspx</a></p>\n</blockquote>\n\n<p><strong>The script</strong></p>\n\n<p>The crux of the script is the <code>Get-WmiObject -ComputerName $computer.name win32_operatingsystem</code> part which we then use to get the <code>lastbootuptime</code> parameter, but there are some optional constructs which I have detailed after the script below.</p>\n\n<script src=\"https://gist.github.com/markwragg/e873167fccd09656bf36d848f7995bd0.js\"></script>\n\n<p>The script uses <code>write-progress</code> to show progress. In order to ensure any verbose or warning messages do not appear underneath the progress bar, I use <code>write-host</code> to output 5 blank lines when the script first runs.</p>\n\n<p><img src=\"/content/images/2016/08/get-uptime2-1.png\" alt=\"\" /></p>\n\n<p>If you run the script with the <code>-verbose</code> parameter, you will see each uptime printed to the console as it's obtained (this is not shown above). </p>\n\n<p><strong>Script framework</strong></p>\n\n<p>As with most scripts, I've put a framework around the core functionality to add flexibility, capture errors and show progress. These are explained below.</p>\n\n<ul>\n<li>Line 4: I have servers organised in to OUs named after their location, so my <code>$location</code> parameter allows me to filter the results to one or more of these locations (note you can enter more than one at once, just by comma separating). This isn't a mandatory Parameter so not specifying this just searches all of AD, however you may want to run the script in different locations close to the servers for speed if you can also filter in this way.</li>\n<li>Line 10: I have servers in my estate which are non-Windows but are Domain joined, so I also filter on Operating System name, by default looking for ones with \"Windows\" in their name (you could remove this if it's not relevant to you).</li>\n<li>Line 19: I use Write-Progress to show which server is currently being polled and how far through the set of servers we are.</li>\n<li>Line 21: I use Test-Connection to check the server is responsive, this speeds up the script for any dead or inaccessible servers.</li>\n<li>Lines 36-42 &amp; 49: I trap any errors that occur in to an object, and at the end of the script spit these out to a separate file for analysis.</li>\n<li>Line 46: I output my results sorted by most to least uptime.</li>\n</ul>",
                        "image": "/content/images/2016/05/flip-clock.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-23 20:16:09",
                        "created_by": 1,
                        "updated_at": "2016-09-04 20:53:45",
                        "updated_by": 1,
                        "published_at": "2016-08-29 10:00:11",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 26,
                        "uuid": "2b8d06e4-263e-4369-bb1a-921424e6e9db",
                        "title": "Handling Powershell exceptions with Try..Catch..Finally",
                        "slug": "powershell-try-catch",
                        "markdown": "Recently while writing a script I expected two commands I was calling to throw exceptions because I expected my script to be unable to remotely connect to some of my servers. Initially I handled the result of these exceptions with `If..Else` blocks, but it felt like a `Try..Catch` might be more appropriate.\n\n*-- Spoiler alert: On this occasion it actually turned out it wasn't, but it's a useful technique regardless.*\n\n![](/content/images/2016/05/yoda-wisdom-1.gif)\n\nWhile Master Yoda does not believe in a \"Try\", Powershell fortunately does. But there are some caveats to using this technique which I will detail below. \n\nIn case you're not aware performing a `Try..Catch` is as simple as:\n\n```\nTry{\n    Do-something\n}Catch{\n    Do-something else, but only if a terminating error has occurred.\n}\n```\nAdditionally you can extend this to include:\n```\n}finally{\n    Do-something always, whether a terminating error occurred or not.\n}\n```\n\nA Finally block can be useful as it will always execute when an error does not occur, but it will also execute if CTRL+C or an Exit keyword is used to stop the script from within a `Catch` block.\n\nHere are some other useful things to know about `Try..Catch`:\n\n**#1: The Catch block will only execute if a terminating error has occurred**\n\nPowershell errors come in two forms, terminating and non-terminating:\n\n>**An error is a terminating error if:**\n\n>- it prevents your cmdlet from continuing to process the current object or from successfully processing any further input objects, regardless of their content.\n>- you do not want your cmdlet to continue processing the current object or any further input objects, regardless of their content.\n>- it occurs in a cmdlet that does not accept or return an object or if it occurs in a cmdlet that accepts or returns only one object.\n\n>**An error is a non-terminating error if:**\n\n>- you want your cmdlet to continue processing the current object and any further input objects.\n>- it is related to a specific input object or subset of input objects.\n>\n>*Source: https://msdn.microsoft.com/en-us/library/ms714414(v=vs.85).aspx*\n\nIf your catch block is not executing, your error is either non-terminating or not throwing an error at all. But..\n\n**#2: You can force a cmdlet to throw a terminating error by using the -erroraction parameter**\n\nThe `-erroraction` parameter is available for any cmdlet that supports [common parameters](https://technet.microsoft.com/en-us/library/hh847884.aspx). There are the following options for this parameter:\n\n- 0 : SilentlyContinue. This suppresses the error message and continues execution.\n- 1 : Stop. This displays the error message and stops executing the specific command.\n- 2 : Continue. Displays the error message and continues executing the command. \"Continue\" is the default value.\n- 3 : Inquire. Displays the error message and prompts you for confirmation before continuing. This is probably only useful when debugging.\n- 4 : Ignore. Suppresses the error message and continues executing the command. Unlike SilentlyContinue, Ignore does not add the error message to the $Error automatic variable. \n\n*-- Tip: You can use the numbers above as a shortcut to these states. Additionally an alias of `-erroraction` is `-ea`. So for example, if you want a cmdlet to silently continue when an error occurs, you can add the parameter `-ea 0`. Beware that this potentially makes your code a little less explicit to others.*\n\nNote that when you use this parameter on a cmdlet it only applies to that specific command. As noted above, the default is \"Continue\", but you can override this by setting the $ErrorActionPreference variable. \n\nYou can set $ErrorActionPreference multiple times, so for example you could change the state to -SilentlyContinue for a block of code, then change it back.\n\n**#3: When a terminating error occurs, nothing else in the Try block executes**\n\nThis is one way to use a `Try..Catch`, as you can create a `Try` block of code that you only want to fully execute when no error occurs. For example, you could use the `test-connection` cmdlet to check if a server pings and if it fails skip anything else afterwards that relied on that connectivity. Without the `Try..Catch` the specific cmdlet would throw an error and then all other subsequent lines would try to execute.\n\nIt's worth noting that it interrupts the pipeline, which means if you're piping multiple inputs to a single cmdlet, no further input is sent to the cmdlet as soon as one of the inputs causes a terminating error to occur.\n\n**#4: Any code after the Try..Catch will execute**\n\nIt's worth stating, because you might expect a **terminating** error to stop the script entirely, but as stated above, it only applies to the command. This means that if you put any lines of code after (and outside) of your `try..catch` it will execute.\n\n**#5: You can specify multiple catch blocks to handle different exceptions**\n\nYou're not limited to a single Catch block following a Try, you can specific multiple blocks for different exceptions. For example:\n\n```\nTry{\n    Pizza\n}Catch[Brocolli.topping.eww]\n    Throw-Bin\n}Catch[Pineapple.topping.eww]\n    Give-Steve\n}Catch{\n    Any-other-pizza-error\n}\n```\n\nThe last generic catch block handles any other error not specified. It can be tricky to work out from the default error message what Exception name to use, but [Boe Prox has written a great article on how to get the Exception type for an error that has occurred so that you can build it in to a Catch block.](https://learn-powershell.net/2015/04/09/quick-hits-finding-exception-types-with-powershell/)\n\n## So.. why/why not do this?\n\nSuppressing error messages is generally considered an anti-pattern. By doing so during development you're making debugging harder. After development, you might be misleading the user on the success of the script. The default \"Continue\" behaviour ensures error messages are presented to the user. \"Stop\" allows you to utilise a Catch block for a non-terminating error to do something more than just report the error to the user (such as log to a file, send an email notification, or initiate a clean up process). \n\nUsing \"SilentlyContinue\" is dangerous (particularly when used as the default preference), but equally an error that you *expect* to get could mislead a user to think a script has failed when it has not. Here's the issue, if you want to suppress the default error output from the user, you can't use a `Try..Catch` because as far as Powershell is concerned no error has occurred.\n\nFor the use case that had me explore this topic, I ultimately reverted to an `If..Else` construct, and used `-ErrorAction \"SilentlyContinue\"` for the specific cmdlets that I expected to error. In my `Else` block I used Write-Warning to let the user know what failure had occurred, because I wanted a cleaner overall result than the default error output. If the script had any other issues, those errors would still be displayed. ",
                        "html": "<p>Recently while writing a script I expected two commands I was calling to throw exceptions because I expected my script to be unable to remotely connect to some of my servers. Initially I handled the result of these exceptions with <code>If..Else</code> blocks, but it felt like a <code>Try..Catch</code> might be more appropriate.</p>\n\n<p><em>-- Spoiler alert: On this occasion it actually turned out it wasn't, but it's a useful technique regardless.</em></p>\n\n<p><img src=\"/content/images/2016/05/yoda-wisdom-1.gif\" alt=\"\" /></p>\n\n<p>While Master Yoda does not believe in a \"Try\", Powershell fortunately does. But there are some caveats to using this technique which I will detail below. </p>\n\n<p>In case you're not aware performing a <code>Try..Catch</code> is as simple as:</p>\n\n<pre><code>Try{  \n    Do-something\n}Catch{\n    Do-something else, but only if a terminating error has occurred.\n}\n</code></pre>\n\n<p>Additionally you can extend this to include:  </p>\n\n<pre><code>}finally{\n    Do-something always, whether a terminating error occurred or not.\n}\n</code></pre>\n\n<p>A Finally block can be useful as it will always execute when an error does not occur, but it will also execute if CTRL+C or an Exit keyword is used to stop the script from within a <code>Catch</code> block.</p>\n\n<p>Here are some other useful things to know about <code>Try..Catch</code>:</p>\n\n<p><strong>#1: The Catch block will only execute if a terminating error has occurred</strong></p>\n\n<p>Powershell errors come in two forms, terminating and non-terminating:</p>\n\n<blockquote>\n  <p><strong>An error is a terminating error if:</strong></p>\n  \n  <ul>\n  <li>it prevents your cmdlet from continuing to process the current object or from successfully processing any further input objects, regardless of their content.</li>\n  <li>you do not want your cmdlet to continue processing the current object or any further input objects, regardless of their content.</li>\n  <li>it occurs in a cmdlet that does not accept or return an object or if it occurs in a cmdlet that accepts or returns only one object.</li>\n  </ul>\n  \n  <p><strong>An error is a non-terminating error if:</strong></p>\n  \n  <ul>\n  <li>you want your cmdlet to continue processing the current object and any further input objects.</li>\n  <li>it is related to a specific input object or subset of input objects.</li>\n  </ul>\n  \n  <p><em>Source: <a href=\"https://msdn.microsoft.com/en-us/library/ms714414(v=vs.85).aspx\">https://msdn.microsoft.com/en-us/library/ms714414(v=vs.85).aspx</a></em></p>\n</blockquote>\n\n<p>If your catch block is not executing, your error is either non-terminating or not throwing an error at all. But..</p>\n\n<p><strong>#2: You can force a cmdlet to throw a terminating error by using the -erroraction parameter</strong></p>\n\n<p>The <code>-erroraction</code> parameter is available for any cmdlet that supports <a href=\"https://technet.microsoft.com/en-us/library/hh847884.aspx\">common parameters</a>. There are the following options for this parameter:</p>\n\n<ul>\n<li>0 : SilentlyContinue. This suppresses the error message and continues execution.</li>\n<li>1 : Stop. This displays the error message and stops executing the specific command.</li>\n<li>2 : Continue. Displays the error message and continues executing the command. \"Continue\" is the default value.</li>\n<li>3 : Inquire. Displays the error message and prompts you for confirmation before continuing. This is probably only useful when debugging.</li>\n<li>4 : Ignore. Suppresses the error message and continues executing the command. Unlike SilentlyContinue, Ignore does not add the error message to the $Error automatic variable. </li>\n</ul>\n\n<p><em>-- Tip: You can use the numbers above as a shortcut to these states. Additionally an alias of <code>-erroraction</code> is <code>-ea</code>. So for example, if you want a cmdlet to silently continue when an error occurs, you can add the parameter <code>-ea 0</code>. Beware that this potentially makes your code a little less explicit to others.</em></p>\n\n<p>Note that when you use this parameter on a cmdlet it only applies to that specific command. As noted above, the default is \"Continue\", but you can override this by setting the $ErrorActionPreference variable. </p>\n\n<p>You can set $ErrorActionPreference multiple times, so for example you could change the state to -SilentlyContinue for a block of code, then change it back.</p>\n\n<p><strong>#3: When a terminating error occurs, nothing else in the Try block executes</strong></p>\n\n<p>This is one way to use a <code>Try..Catch</code>, as you can create a <code>Try</code> block of code that you only want to fully execute when no error occurs. For example, you could use the <code>test-connection</code> cmdlet to check if a server pings and if it fails skip anything else afterwards that relied on that connectivity. Without the <code>Try..Catch</code> the specific cmdlet would throw an error and then all other subsequent lines would try to execute.</p>\n\n<p>It's worth noting that it interrupts the pipeline, which means if you're piping multiple inputs to a single cmdlet, no further input is sent to the cmdlet as soon as one of the inputs causes a terminating error to occur.</p>\n\n<p><strong>#4: Any code after the Try..Catch will execute</strong></p>\n\n<p>It's worth stating, because you might expect a <strong>terminating</strong> error to stop the script entirely, but as stated above, it only applies to the command. This means that if you put any lines of code after (and outside) of your <code>try..catch</code> it will execute.</p>\n\n<p><strong>#5: You can specify multiple catch blocks to handle different exceptions</strong></p>\n\n<p>You're not limited to a single Catch block following a Try, you can specific multiple blocks for different exceptions. For example:</p>\n\n<pre><code>Try{  \n    Pizza\n}Catch[Brocolli.topping.eww]\n    Throw-Bin\n}Catch[Pineapple.topping.eww]\n    Give-Steve\n}Catch{\n    Any-other-pizza-error\n}\n</code></pre>\n\n<p>The last generic catch block handles any other error not specified. It can be tricky to work out from the default error message what Exception name to use, but <a href=\"https://learn-powershell.net/2015/04/09/quick-hits-finding-exception-types-with-powershell/\">Boe Prox has written a great article on how to get the Exception type for an error that has occurred so that you can build it in to a Catch block.</a></p>\n\n<h2 id=\"sowhywhynotdothis\">So.. why/why not do this?</h2>\n\n<p>Suppressing error messages is generally considered an anti-pattern. By doing so during development you're making debugging harder. After development, you might be misleading the user on the success of the script. The default \"Continue\" behaviour ensures error messages are presented to the user. \"Stop\" allows you to utilise a Catch block for a non-terminating error to do something more than just report the error to the user (such as log to a file, send an email notification, or initiate a clean up process). </p>\n\n<p>Using \"SilentlyContinue\" is dangerous (particularly when used as the default preference), but equally an error that you <em>expect</em> to get could mislead a user to think a script has failed when it has not. Here's the issue, if you want to suppress the default error output from the user, you can't use a <code>Try..Catch</code> because as far as Powershell is concerned no error has occurred.</p>\n\n<p>For the use case that had me explore this topic, I ultimately reverted to an <code>If..Else</code> construct, and used <code>-ErrorAction \"SilentlyContinue\"</code> for the specific cmdlets that I expected to error. In my <code>Else</code> block I used Write-Warning to let the user know what failure had occurred, because I wanted a cleaner overall result than the default error output. If the script had any other issues, those errors would still be displayed. </p>",
                        "image": "/content/images/2016/05/yoda-do_or_do_not.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-05-25 18:41:05",
                        "created_by": 1,
                        "updated_at": "2016-05-25 22:35:45",
                        "updated_by": 1,
                        "published_at": "2016-05-25 21:58:44",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 27,
                        "uuid": "acff0521-04ef-47cd-a718-01dc4404283b",
                        "title": "Implementing a release pipeline for Windows-based infrastructure",
                        "slug": "implementing-a-release-pipeline-for-windows-based-infrastructure",
                        "markdown": "Article to summarise the Release Pipeline Model whitepaper and to create a plan of action for how to start to implement.\n\n> https://msdn.microsoft.com/en-us/powershell/dsc/whitepapers#the-release-pipeline-model\n\n",
                        "html": "<p>Article to summarise the Release Pipeline Model whitepaper and to create a plan of action for how to start to implement.</p>\n\n<blockquote>\n  <p><a href=\"https://msdn.microsoft.com/en-us/powershell/dsc/whitepapers#the-release-pipeline-model\">https://msdn.microsoft.com/en-us/powershell/dsc/whitepapers#the-release-pipeline-model</a></p>\n</blockquote>",
                        "image": "/content/images/2016/06/Continuous-Narrow.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-06-05 18:15:11",
                        "created_by": 1,
                        "updated_at": "2017-01-20 11:48:56",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 28,
                        "uuid": "c7119384-abd1-439c-9571-53d70d17997b",
                        "title": "Testing Active Directory with Pester and Powershell",
                        "slug": "testing-active-directory-with-pester-and-powershell",
                        "markdown": "[Irwin Strachan](https://twitter.com/IrwinStrachan) published a Pester script for [Operational Testing of Active Directory](https://pshirwin.wordpress.com/2016/04/08/active-directory-operations-test/) back in April which I was keen to try out. Afterwards I extended the script to add some additional health checks of Active Directory and this post explains how the resultant combination of our work can be used to validate your Active Directory.\n\nYou can find my version of this tool here: \n\n> https://github.com/markwragg/Test-ActiveDirectory\n\nOr get it from the Powershell Gallery via:\n\n> `Install-Module ADAudit` (to grab the code to view it before installing you can use `Save-Module ADAudit` instead).\n\nThis was my second outing with Pester (previously I authored a [Pester script for my Hipchat module](https://github.com/markwragg/Powershell-Hipchat/blob/master/hipchat.Tests.ps1) which did some basic functionality tests) and it's a fantastic framework for building both unit tests of your scripts to ensure they are functioning correctly as well as performing operational testing. If you're not familiar with Pester [check out the wiki](https://github.com/pester/Pester/wiki) to help you get started.\n\n*-- If you have Windows Management Framework 5 installed (or are running Windows 10) you may already have Pester (or you can easily install it with `Install-Module Pester`). Prior to that you can [download it from Github](https://github.com/pester/Pester/) and install it as a module.*\n\n## Testing and validating your Active Directory\n\nIrwin's script was a follow up to his earlier post which [captures your Active Directory configuration and stores it as an XML file](https://pshirwin.wordpress.com/2016/03/25/active-directory-configuration-snapshot/). He then realised this could be leveraged to perform Pester tests against a later capture of the configuration to determine if any drift had occurred.\n\nAfter you have [downloaded the scripts from Git](https://github.com/markwragg/Test-ActiveDirectory) or [PSGallery](https://www.powershellgallery.com/packages/ADAudit/1.1), perform the following:\n\n**1. Execute: 'Get-ADConfig.ps1'**\n\nYou need to download and execute [Get-ADConfig.ps1](https://github.com/markwragg/Test-ActiveDirectory/blob/master/Get-ADConfig.ps1) within your domain with suitable rights to create an XML file of your current Active Directory configuration.  You should do this one time initially at a point where you are satisfied that the configuration of AD is correct, then store that file as **ADGoldConfig-{date}.xml** to be used for future comparisons. \n\n*-- When you later use the Pester script, If you don't specify a path to this file as a parameter it by default looks for the latest version in the directory it is run:*\n\n```language-powershell\n[CmdletBinding()]\nParam(\n    [string]$ADFile,\n    [string]$ADGoldFile = $(Get-ChildItem (\"ADGoldConfig-*.xml\") | Select name -last 1).name\n)\n```\nYou can run `Get-ADConfig.ps1` again manually at a later date to generate a snapshot of the Active Directory configuration for comparison. Store this in the same location as the Pester script and it will be used as the comparative config (or specify the path of a config explicitly via the `ADFile` param per the above). If one is not found by (or provided to) Pester, it will try to run Get-ADConfig.ps1 for you to generate one on the fly. \n\n**2. Having generated a Gold configuration and (optionally) a current configuration XML file you now execute: 'ActiveDirectory.tests.ps1', but do so by running 'invoke-pester'**\n\nThis is the Pester script and you should execute it by changing to the directory that it's in and then running `invoke-pester`. This command will look for any script in the current directory named *.tests.ps1 and execute them. While you can run the PS1 file directly, executing it with `invoke-pester` gives you a summary count at the end of tests passed/failed/skipped and allows you to optionally use other parameters such as storing the result as an XML file, or using tags to skip include or exclude certain tests.\n\nThe Pester script performs the following tests:\n\n**Active Directory configuration checks:**\n\nA check of the config between what has been stored previously and what has been captured now to detect configuration drift. This is almost entirely the set of tests Irwin defined in his posts.\n\n*-- Beware there is currently a limitation in this comparison in that it doesn't validate that sets are the same. E.g if a Domain Controller is added or removed it may not trigger a fail. [Irwin explores this issue in a later post](https://pshirwin.wordpress.com/2016/04/29/operational-readiness-validation-gotchas/) and I recommend you check it out. I may modify the script in the future to account for this limitation.*\n\n**Active Directory health checks:**\n\nThese are the tests that I added, intended to perform operational healthchecks of Active Directory in a similar way as someone might do manually. Some of these tests leverage legacy command-line tools that have been around for many years for diagnosing AD. As they are not Powershell cmdlets their results are generally not analysable as objects, so Pester tests these by looking for the presence or exclusion of certain string values.\n\nThese tests are as follows:\n\n1. Executes `NLTest /Query` and checks the output for \"Success\"\n- Executes `DCDiag -a` and checks the output doesn't include any mention of \"failed\"\n- Executes `RepAdmin /showrepl /CSV` and checks each replication to ensure the \"Number of Failures\" count is 0\n- Performs a Ping (`test-connection`) against each listed Domain Controller (as known via the configuration file)\n- Uses `test-netconnection` (may require Server 2012 R2 or newer and WMF5 to work) to confirm that the following common Active Directory related TCP ports respond locally: \n - 53, 88, 135, 139, 389, 445, 464, 636, 3268, 3269, 9389\n- Checks that common Active Directory related services are running:\n - \"ADWS\", \"BITS\", \"CertPropSvc\", \"CryptSvc\", \"Dfs\", \"DFSR\", \"DNS\", \"Dnscache\", \"eventlog\", \"gpsvc\", \"kdc\", \"LanmanServer\", \"LanmanWorkstation\", \"Netlogon\", \"NTDS\", \"NtFrs\", \"RpcEptMapper\", \"RpcSs\", \"SamSs\", \"W32Time\"\n\n## Conclusion\n\n![](/content/images/2016/06/Pester-AD-Test-Outcome.png)\n\nPester will conclude indicating how many tests passed and failed. The number of tests will vary depending on the complexity of your Active Directory but in my environment 344 tests were performed in about 4 minutes, which is outstanding when you consider the effort that would be required to make the same evaluation manually.\n\nIf you have any ideas for extending these tests further, [Pull requests are welcomed](https://github.com/markwragg/Test-ActiveDirectory/pulls).",
                        "html": "<p><a href=\"https://twitter.com/IrwinStrachan\">Irwin Strachan</a> published a Pester script for <a href=\"https://pshirwin.wordpress.com/2016/04/08/active-directory-operations-test/\">Operational Testing of Active Directory</a> back in April which I was keen to try out. Afterwards I extended the script to add some additional health checks of Active Directory and this post explains how the resultant combination of our work can be used to validate your Active Directory.</p>\n\n<p>You can find my version of this tool here: </p>\n\n<blockquote>\n  <p><a href=\"https://github.com/markwragg/Test-ActiveDirectory\">https://github.com/markwragg/Test-ActiveDirectory</a></p>\n</blockquote>\n\n<p>Or get it from the Powershell Gallery via:</p>\n\n<blockquote>\n  <p><code>Install-Module ADAudit</code> (to grab the code to view it before installing you can use <code>Save-Module ADAudit</code> instead).</p>\n</blockquote>\n\n<p>This was my second outing with Pester (previously I authored a <a href=\"https://github.com/markwragg/Powershell-Hipchat/blob/master/hipchat.Tests.ps1\">Pester script for my Hipchat module</a> which did some basic functionality tests) and it's a fantastic framework for building both unit tests of your scripts to ensure they are functioning correctly as well as performing operational testing. If you're not familiar with Pester <a href=\"https://github.com/pester/Pester/wiki\">check out the wiki</a> to help you get started.</p>\n\n<p><em>-- If you have Windows Management Framework 5 installed (or are running Windows 10) you may already have Pester (or you can easily install it with <code>Install-Module Pester</code>). Prior to that you can <a href=\"https://github.com/pester/Pester/\">download it from Github</a> and install it as a module.</em></p>\n\n<h2 id=\"testingandvalidatingyouractivedirectory\">Testing and validating your Active Directory</h2>\n\n<p>Irwin's script was a follow up to his earlier post which <a href=\"https://pshirwin.wordpress.com/2016/03/25/active-directory-configuration-snapshot/\">captures your Active Directory configuration and stores it as an XML file</a>. He then realised this could be leveraged to perform Pester tests against a later capture of the configuration to determine if any drift had occurred.</p>\n\n<p>After you have <a href=\"https://github.com/markwragg/Test-ActiveDirectory\">downloaded the scripts from Git</a> or <a href=\"https://www.powershellgallery.com/packages/ADAudit/1.1\">PSGallery</a>, perform the following:</p>\n\n<p><strong>1. Execute: 'Get-ADConfig.ps1'</strong></p>\n\n<p>You need to download and execute <a href=\"https://github.com/markwragg/Test-ActiveDirectory/blob/master/Get-ADConfig.ps1\">Get-ADConfig.ps1</a> within your domain with suitable rights to create an XML file of your current Active Directory configuration.  You should do this one time initially at a point where you are satisfied that the configuration of AD is correct, then store that file as <strong>ADGoldConfig-{date}.xml</strong> to be used for future comparisons. </p>\n\n<p><em>-- When you later use the Pester script, If you don't specify a path to this file as a parameter it by default looks for the latest version in the directory it is run:</em></p>\n\n<pre><code class=\"language-powershell\">[CmdletBinding()]\nParam(  \n    [string]$ADFile,\n    [string]$ADGoldFile = $(Get-ChildItem (\"ADGoldConfig-*.xml\") | Select name -last 1).name\n)\n</code></pre>\n\n<p>You can run <code>Get-ADConfig.ps1</code> again manually at a later date to generate a snapshot of the Active Directory configuration for comparison. Store this in the same location as the Pester script and it will be used as the comparative config (or specify the path of a config explicitly via the <code>ADFile</code> param per the above). If one is not found by (or provided to) Pester, it will try to run Get-ADConfig.ps1 for you to generate one on the fly. </p>\n\n<p><strong>2. Having generated a Gold configuration and (optionally) a current configuration XML file you now execute: 'ActiveDirectory.tests.ps1', but do so by running 'invoke-pester'</strong></p>\n\n<p>This is the Pester script and you should execute it by changing to the directory that it's in and then running <code>invoke-pester</code>. This command will look for any script in the current directory named *.tests.ps1 and execute them. While you can run the PS1 file directly, executing it with <code>invoke-pester</code> gives you a summary count at the end of tests passed/failed/skipped and allows you to optionally use other parameters such as storing the result as an XML file, or using tags to skip include or exclude certain tests.</p>\n\n<p>The Pester script performs the following tests:</p>\n\n<p><strong>Active Directory configuration checks:</strong></p>\n\n<p>A check of the config between what has been stored previously and what has been captured now to detect configuration drift. This is almost entirely the set of tests Irwin defined in his posts.</p>\n\n<p><em>-- Beware there is currently a limitation in this comparison in that it doesn't validate that sets are the same. E.g if a Domain Controller is added or removed it may not trigger a fail. <a href=\"https://pshirwin.wordpress.com/2016/04/29/operational-readiness-validation-gotchas/\">Irwin explores this issue in a later post</a> and I recommend you check it out. I may modify the script in the future to account for this limitation.</em></p>\n\n<p><strong>Active Directory health checks:</strong></p>\n\n<p>These are the tests that I added, intended to perform operational healthchecks of Active Directory in a similar way as someone might do manually. Some of these tests leverage legacy command-line tools that have been around for many years for diagnosing AD. As they are not Powershell cmdlets their results are generally not analysable as objects, so Pester tests these by looking for the presence or exclusion of certain string values.</p>\n\n<p>These tests are as follows:</p>\n\n<ol>\n<li>Executes <code>NLTest /Query</code> and checks the output for \"Success\"  </li>\n<li>Executes <code>DCDiag -a</code> and checks the output doesn't include any mention of \"failed\"</li>\n<li>Executes <code>RepAdmin /showrepl /CSV</code> and checks each replication to ensure the \"Number of Failures\" count is 0</li>\n<li>Performs a Ping (<code>test-connection</code>) against each listed Domain Controller (as known via the configuration file)</li>\n<li>Uses <code>test-netconnection</code> (may require Server 2012 R2 or newer and WMF5 to work) to confirm that the following common Active Directory related TCP ports respond locally: \n<ul><li>53, 88, 135, 139, 389, 445, 464, 636, 3268, 3269, 9389</li></ul></li>\n<li>Checks that common Active Directory related services are running:\n<ul><li>\"ADWS\", \"BITS\", \"CertPropSvc\", \"CryptSvc\", \"Dfs\", \"DFSR\", \"DNS\", \"Dnscache\", \"eventlog\", \"gpsvc\", \"kdc\", \"LanmanServer\", \"LanmanWorkstation\", \"Netlogon\", \"NTDS\", \"NtFrs\", \"RpcEptMapper\", \"RpcSs\", \"SamSs\", \"W32Time\"</li></ul></li>\n</ol>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p><img src=\"/content/images/2016/06/Pester-AD-Test-Outcome.png\" alt=\"\" /></p>\n\n<p>Pester will conclude indicating how many tests passed and failed. The number of tests will vary depending on the complexity of your Active Directory but in my environment 344 tests were performed in about 4 minutes, which is outstanding when you consider the effort that would be required to make the same evaluation manually.</p>\n\n<p>If you have any ideas for extending these tests further, <a href=\"https://github.com/markwragg/Test-ActiveDirectory/pulls\">Pull requests are welcomed</a>.</p>",
                        "image": "/content/images/2016/06/Compliance-Checkbox.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Testing Active Directory with Pester and Powershell",
                        "meta_description": "This blog post details how Powershell and Pester can be used to validate your Active Directory configuration and perform automated functional tests.",
                        "author_id": 1,
                        "created_at": "2016-06-06 13:22:01",
                        "created_by": 1,
                        "updated_at": "2016-06-13 08:53:56",
                        "updated_by": 1,
                        "published_at": "2016-06-13 08:53:56",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 29,
                        "uuid": "aee35c80-e555-41f7-bbda-311f07a7862f",
                        "title": "Using Test-NetConnection to diagnose network connectivity",
                        "slug": "using-test-netconnection-to-diagnose-network-connectivity",
                        "markdown": "Powershell version 4 and Windows 8.1 / 2012 R2 introduced the `Test-NetConnection` command as a tool for performing network connectivity tests with Powershell. `Test-NetConnection` allows you to perform ping, traceroute and TCP port tests and from Windows 10 and Server 2016 onward introduces the ability to do \"Diagnose Routing\" tests with the same cmdlet.\n\n> The Test-NetConnection cmdlet displays diagnostic information for a connection. It supports ping test, TCP test, route tracing, and route selection diagnostics. Depending on the input parameters, the output can include the DNS lookup results, a list of IP interfaces, IPsec rules, route/source address selection results, and/or confirmation of connection establishment.\n>\n> *Source: https://technet.microsoft.com/en-us/library/dn372891.aspx*\n\nThe official documentation indicates that there is no \"default value\" for the `-ComputerName` parameter (the destination of your tests), but it does seem to actually default to  internetbeacon.msedge.net if not specified. Other parameters for this command vary depending on which type of testing you want to do.\n\n# Test-NetConnection\n\n- Uses `Write-Progress` for status messages and `Write-Warning` to report failure states (Ping or TCP connect failed) to the console.\n- There is no way to define a timeout value for any of the tests.\n- The cmdlet always does a ping test and name resolution, even if you are using it to test a TCP port or to perform a traceroute (you get both results, with ping and name resolution seemingly being performed first which can slow the performance of the cmdlet overall).\n- You can use `-InformationLevel Quiet` to simplify the result to true/false:\n\n> `-InformationLevel`: If you set this parameter to Quiet, the cmdlet returns basic information. For example, for a ping test, this cmdlet returns a Boolean value that indicates whether the attempt to ping a host or port is successful.\n\n#### Ping / ICMP testing\n\nPerform a simple ping test (although as noted below in my section on tools for other Windows versions, test-connection might be a better choice for this as it's more flexible):\n```\nTest-NetConnection google.com\n```\n#### TCP port testing\n\nPerform a test to a Common TCP port by name using the `-CommonTCPPort` parameter (options are only the following: HTTP, RDP, SMB, WINRM) or to any numbered TCP port by using the `-Port` parameter, examples:\n```\nTest-NetConnection wragg.io -CommonTCPPort HTTP\nTest-NetConnection twitter.com -Port 443\nTest-NetConnection localhost -Port 445\n```\n\n![](/content/images/2016/09/test-netconnection-port443.png)\n\n#### Traceroute testing \nPerform a traceroute (from Windows 10 / Server 2016 only) you can use the `-Hops` parameter to define a maximum number of hops:\n```\nTest-NetConnection sammart.in -TraceRoute\n```\n![](/content/images/2016/09/test-netconnection-traceroute.png)\n\nHere's how you might use this to get the second to last hop of a traceroute:\n```\n(Test-NetConnection google.com -TraceRoute).TraceRoute | Select -SkipLast 1 | Select -Last 1\n```\n# Examples\n\nHere's a Gist with various other examples expanding on those described above:\n\n<script src=\"https://gist.github.com/markwragg/78e923e9e7a1d5aa8a6acdcc2dc6bdce.js\"></script>\n\n# Tools for other Win / Server versions\n\nThere are some alternative options for doing similar levels of testing as `Test-NetConnection` allows with versions of Windows or Powershell that don't support the cmdlet.\n\n#### Test-Connection\nThe `Test-Connection` cmdlet was introduced in Powershell 3 and allows you to do a simple ping test. For this purpose it's much more versatile than Test-NetConnection as you can define the TTL (Time To Live), Packet Size, Delay and other settings as well as use it in `-quiet` mode to suppress messages.\n\n#### System.Net.Sockets\nFor performing TCP port tests, you can use the .NET class System.Net.Sockets.TcpClient, which can be particularly advantageous if you don't want to install the Telnet Client.\n\n`(New-Object System.Net.Sockets.TcpClient).Connect('{remote server}', {port})`\n\n*Source: http://www.travisgan.com/2014/03/use-powershell-to-test-port.html*\n\n#### Poor man's Powershell Traceroute\nBelow is a function found on Stack Overflow written by Mathias R Jessen which provides a way to do a traceroute via Powershell without the Test-NetConnection function:\n\n*Source: http://stackoverflow.com/questions/32434882/powershell-tracert*\n\n```language-powershell\nfunction Invoke-Tracert {\n    param([string]$RemoteHost)\n\n    tracert $RemoteHost |ForEach-Object{\n        if($_.Trim() -match \"Tracing route to .*\") {\n            Write-Host $_ -ForegroundColor Green\n        } elseif ($_.Trim() -match \"^\\d{1,2}\\s+\") {\n            $n,$a1,$a2,$a3,$target,$null = $_.Trim()-split\"\\s{2,}\"\n            $Properties = @{\n                Hop    = $n;\n                First  = $a1;\n                Second = $a2;\n                Third  = $a3;\n                Node   = $target\n            }\n            New-Object psobject -Property $Properties\n        }\n    }\n}\n```\n*Script author: [Mathias R Jessen](http://stackoverflow.com/users/712649/mathias-r-jessen)*",
                        "html": "<p>Powershell version 4 and Windows 8.1 / 2012 R2 introduced the <code>Test-NetConnection</code> command as a tool for performing network connectivity tests with Powershell. <code>Test-NetConnection</code> allows you to perform ping, traceroute and TCP port tests and from Windows 10 and Server 2016 onward introduces the ability to do \"Diagnose Routing\" tests with the same cmdlet.</p>\n\n<blockquote>\n  <p>The Test-NetConnection cmdlet displays diagnostic information for a connection. It supports ping test, TCP test, route tracing, and route selection diagnostics. Depending on the input parameters, the output can include the DNS lookup results, a list of IP interfaces, IPsec rules, route/source address selection results, and/or confirmation of connection establishment.</p>\n  \n  <p><em>Source: <a href=\"https://technet.microsoft.com/en-us/library/dn372891.aspx\">https://technet.microsoft.com/en-us/library/dn372891.aspx</a></em></p>\n</blockquote>\n\n<p>The official documentation indicates that there is no \"default value\" for the <code>-ComputerName</code> parameter (the destination of your tests), but it does seem to actually default to  internetbeacon.msedge.net if not specified. Other parameters for this command vary depending on which type of testing you want to do.</p>\n\n<h1 id=\"testnetconnection\">Test-NetConnection</h1>\n\n<ul>\n<li>Uses <code>Write-Progress</code> for status messages and <code>Write-Warning</code> to report failure states (Ping or TCP connect failed) to the console.</li>\n<li>There is no way to define a timeout value for any of the tests.</li>\n<li>The cmdlet always does a ping test and name resolution, even if you are using it to test a TCP port or to perform a traceroute (you get both results, with ping and name resolution seemingly being performed first which can slow the performance of the cmdlet overall).</li>\n<li>You can use <code>-InformationLevel Quiet</code> to simplify the result to true/false:</li>\n</ul>\n\n<blockquote>\n  <p><code>-InformationLevel</code>: If you set this parameter to Quiet, the cmdlet returns basic information. For example, for a ping test, this cmdlet returns a Boolean value that indicates whether the attempt to ping a host or port is successful.</p>\n</blockquote>\n\n<h4 id=\"pingicmptesting\">Ping / ICMP testing</h4>\n\n<p>Perform a simple ping test (although as noted below in my section on tools for other Windows versions, test-connection might be a better choice for this as it's more flexible):  </p>\n\n<pre><code>Test-NetConnection google.com  \n</code></pre>\n\n<h4 id=\"tcpporttesting\">TCP port testing</h4>\n\n<p>Perform a test to a Common TCP port by name using the <code>-CommonTCPPort</code> parameter (options are only the following: HTTP, RDP, SMB, WINRM) or to any numbered TCP port by using the <code>-Port</code> parameter, examples:  </p>\n\n<pre><code>Test-NetConnection wragg.io -CommonTCPPort HTTP  \nTest-NetConnection twitter.com -Port 443  \nTest-NetConnection localhost -Port 445  \n</code></pre>\n\n<p><img src=\"/content/images/2016/09/test-netconnection-port443.png\" alt=\"\" /></p>\n\n<h4 id=\"traceroutetesting\">Traceroute testing</h4>\n\n<p>Perform a traceroute (from Windows 10 / Server 2016 only) you can use the <code>-Hops</code> parameter to define a maximum number of hops:  </p>\n\n<pre><code>Test-NetConnection sammart.in -TraceRoute  \n</code></pre>\n\n<p><img src=\"/content/images/2016/09/test-netconnection-traceroute.png\" alt=\"\" /></p>\n\n<p>Here's how you might use this to get the second to last hop of a traceroute:  </p>\n\n<pre><code>(Test-NetConnection google.com -TraceRoute).TraceRoute | Select -SkipLast 1 | Select -Last 1\n</code></pre>\n\n<h1 id=\"examples\">Examples</h1>\n\n<p>Here's a Gist with various other examples expanding on those described above:</p>\n\n<script src=\"https://gist.github.com/markwragg/78e923e9e7a1d5aa8a6acdcc2dc6bdce.js\"></script>\n\n<h1 id=\"toolsforotherwinserverversions\">Tools for other Win / Server versions</h1>\n\n<p>There are some alternative options for doing similar levels of testing as <code>Test-NetConnection</code> allows with versions of Windows or Powershell that don't support the cmdlet.</p>\n\n<h4 id=\"testconnection\">Test-Connection</h4>\n\n<p>The <code>Test-Connection</code> cmdlet was introduced in Powershell 3 and allows you to do a simple ping test. For this purpose it's much more versatile than Test-NetConnection as you can define the TTL (Time To Live), Packet Size, Delay and other settings as well as use it in <code>-quiet</code> mode to suppress messages.</p>\n\n<h4 id=\"systemnetsockets\">System.Net.Sockets</h4>\n\n<p>For performing TCP port tests, you can use the .NET class System.Net.Sockets.TcpClient, which can be particularly advantageous if you don't want to install the Telnet Client.</p>\n\n<p><code>(New-Object System.Net.Sockets.TcpClient).Connect('{remote server}', {port})</code></p>\n\n<p><em>Source: <a href=\"http://www.travisgan.com/2014/03/use-powershell-to-test-port.html\">http://www.travisgan.com/2014/03/use-powershell-to-test-port.html</a></em></p>\n\n<h4 id=\"poormanspowershelltraceroute\">Poor man's Powershell Traceroute</h4>\n\n<p>Below is a function found on Stack Overflow written by Mathias R Jessen which provides a way to do a traceroute via Powershell without the Test-NetConnection function:</p>\n\n<p><em>Source: <a href=\"http://stackoverflow.com/questions/32434882/powershell-tracert\">http://stackoverflow.com/questions/32434882/powershell-tracert</a></em></p>\n\n<pre><code class=\"language-powershell\">function Invoke-Tracert {  \n    param([string]$RemoteHost)\n\n    tracert $RemoteHost |ForEach-Object{\n        if($_.Trim() -match \"Tracing route to .*\") {\n            Write-Host $_ -ForegroundColor Green\n        } elseif ($_.Trim() -match \"^\\d{1,2}\\s+\") {\n            $n,$a1,$a2,$a3,$target,$null = $_.Trim()-split\"\\s{2,}\"\n            $Properties = @{\n                Hop    = $n;\n                First  = $a1;\n                Second = $a2;\n                Third  = $a3;\n                Node   = $target\n            }\n            New-Object psobject -Property $Properties\n        }\n    }\n}\n</code></pre>\n\n<p><em>Script author: <a href=\"http://stackoverflow.com/users/712649/mathias-r-jessen\">Mathias R Jessen</a></em></p>",
                        "image": "/content/images/2016/06/connectivity.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-06-07 06:17:33",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:02:20",
                        "updated_by": 1,
                        "published_at": "2016-09-08 20:53:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 31,
                        "uuid": "c5db52c1-8a9b-4873-94cf-5442de8089b4",
                        "title": "Get Transaction Sensor URL settings from PRTG with Powershell",
                        "slug": "get-transaction-sensor-url-settings-from-prtg-with-powershell",
                        "markdown": "If you're looking for a way to interrogate multiple HTTP Transaction Sensors in your PRTG monitoring installation using a script then this blog post is for you.\n\nI was interested in auditing our monitoring sensors to check they were correctly configured, in particular our Transaction Sensors which visit a series of URLs in turn to simulate a login and check for keyword values on certain pages. We had an issue where (due to sensor cloning) some of these sensors were pointing to the wrong URLs and we needed to understand how extensive this problem was.\n\n![](/content/images/2016/09/tip-of-the-iceberg-90839.jpg)\n\nPRTG has fairly detailed [API documentation](https://www.paessler.com/manuals/prtg/application_programming_interface_api_definition), but doing the above turned out to be surprisingly challenging, so i'm detailing it here in case it should help someone else in the future. \n\nThe below method allows you to return a list of objects (i'm filtering for sensors) in JSON which match a certain filter (i'm getting PRTG to filter by type for the HTTP Transaction sensors). This gets me the list of sensors I want to query:\n\n```\n/api/table.json?content=sensors&output=json&columns=objid,probe,group,device,sensor,status&count=10000&filter_type=HTTPTransaction\n```\n\nThe API also includes a method for querying an object setting, which works on an individual basis by passing the ID of the object and the **name** of the property you want to return:\n\n```\n/api/getobjectproperty.htm?id={object ID}&name={property name}\n```\n\nSo far so good.. unfortunately the Paessler documentation of the property names is pretty light. It directs you to look at the edit page for the setting you want to query and check the name for the input box. For some settings this is fairly derivable, the object name is usually \"name\" and you can get the tags with \"tags\" for example. However determining the names of the Transaction Sensor URL step properties (such as URL):\n\n![](/content/images/2016/09/TransactionURL-2.png)\n\nUltimately required guesswork. The answer turned out to be that the URL property is named:\n\n> **HTTPURL{n}** \n\ni.e HTTPURL1, HTTPURL2, HTTPURL3, HTTPURL4, HTTPURL5 .. HTTPURL10.\n\nThe API only permits you to query a single property of a single object at a time, so I used Powershell to loop through the settings that I wanted to query.\n\nHaving worked out the above property name, I googled it to see if anyone had come before me. They seemingly had not, but I did come across a [malware analysis site that had analysed the PRTG core executable](https://malwr.com/analysis/OGNhNjRlZTI4NmI4NGZlNGJjM2U2M2Y1ZWIwZjhkNzQ/) and as a result had a page of string values, on which was HTTPURL1. This led me to the following additional list, which I tested against the HTTP Transaction sensor and confirmed are valid for that sensor type:\n\n**Transaction Sensor Property Names**\n\ncookies\nhttpauthentication\nhttpmethod1\nhttpmethod2\nhttpmethod3\nhttpmethod4\nhttpmethod5\nhttpmethod6\nhttpmethod7\nhttpmethod8\nhttpmethod9\nhttpmethod10\nhttppassword\nhttpurl1\nhttpurl2\nhttpurl3\nhttpurl4\nhttpurl5\nhttpurl6\nhttpurl7\nhttpurl8\nhttpurl9\nhttpurl10\nhttpuser\nincludemaynot1\nincludemaynot2\nincludemaynot3\nincludemaynot4\nincludemaynot5\nincludemaynot6\nincludemaynot7\nincludemaynot8\nincludemaynot9\nincludemaynot10\nincludemust1\nincludemust2\nincludemust3\nincludemust4\nincludemust5\nincludemust6\nincludemust7\nincludemust8\nincludemust9\nincludemust10\nmaxdownload\nname\nparenttags\npostdata1\npostdata2\npostdata3\npostdata4\npostdata5\npostdata6\npostdata7\npostdata8\npostdata9\npostdata10\npriority\nproxypassword\nproxyport\nproxyuser\nsteptimeout\ntags\ntimeout\n\n*-- Note: this is not an exhaustive list. Password properties return stars rather than the actual password (as does the priority property but I think that's intentional :) ).*\n\nI wanted to find a property for knowing whether a transaction step is enabled or disabled (this is an option from step 2 onward) but unfortunately have not been able to find one.\n\nFor the regular HTTP sensor types, the names above without numbers are likely all valid.\n\n**The script**\n\nReturning to the matter at hand, the full script for auditing the URL settings of my Transaction sensors is below. The script works as follows:\n\n1. Uses the PRTG API to get a filtered list of all sensor objects which match a certain type (as defined in the Param block) in JSON and converts this to a Powershell object. Note that whatever you set as `$SensorType` is sent via the query string so be careful with characters (such as spaces) that may need to be URL encoded.\n2. Loops through the list of servers and gets the URL property for all 10 URLs steps (note that it checks them all whether they are all populated or not and it does not check or return whether the steps are enabled). Through the loop it adds each URL to the sensor object.\n3. The final complete object (the original sensor settings and the 10 new columns with the URL details) are piped to a CSV file (you could modify where the output is redirected to whatever suits your needs). \n\nI've also got a bit of a thing for `Write-Progress` at the moment and this was a legitimate opportunity to use nested progress bars to report progress:\n\n![](/content/images/2016/09/get-prtgsensordetails.png)\n\nHere's the full script for anyone looking for a similar solution:\n\n<script src=\"https://gist.github.com/markwragg/7640aeab480f2f4e4b278908d108c1ef.js\"></script>\n\n# Get Auto Acknowledge setting for all Ping sensors from PRTG with Powershell\n\nWhile browsing Paessler's Knowledge Base I came across [a post from someone looking to extract the auto acknowledge setting](https://kb.paessler.com/en/topic/71062-extract-auto-acknowledge-setting-from-all-sensors-via-api) (which appears to be an option on Ping sensors only) from all sensors in PRTG using the API. As this was fundamentally a similar problem as my script above, I made a quick modified version of it to achieve this result, which you can find below:\n\n<script src=\"https://gist.github.com/markwragg/02a91560f46210cf67ca5c810681f6ae.js\"></script>\n",
                        "html": "<p>If you're looking for a way to interrogate multiple HTTP Transaction Sensors in your PRTG monitoring installation using a script then this blog post is for you.</p>\n\n<p>I was interested in auditing our monitoring sensors to check they were correctly configured, in particular our Transaction Sensors which visit a series of URLs in turn to simulate a login and check for keyword values on certain pages. We had an issue where (due to sensor cloning) some of these sensors were pointing to the wrong URLs and we needed to understand how extensive this problem was.</p>\n\n<p><img src=\"/content/images/2016/09/tip-of-the-iceberg-90839.jpg\" alt=\"\" /></p>\n\n<p>PRTG has fairly detailed <a href=\"https://www.paessler.com/manuals/prtg/application_programming_interface_api_definition\">API documentation</a>, but doing the above turned out to be surprisingly challenging, so i'm detailing it here in case it should help someone else in the future. </p>\n\n<p>The below method allows you to return a list of objects (i'm filtering for sensors) in JSON which match a certain filter (i'm getting PRTG to filter by type for the HTTP Transaction sensors). This gets me the list of sensors I want to query:</p>\n\n<pre><code>/api/table.json?content=sensors&amp;output=json&amp;columns=objid,probe,group,device,sensor,status&amp;count=10000&amp;filter_type=HTTPTransaction\n</code></pre>\n\n<p>The API also includes a method for querying an object setting, which works on an individual basis by passing the ID of the object and the <strong>name</strong> of the property you want to return:</p>\n\n<pre><code>/api/getobjectproperty.htm?id={object ID}&amp;name={property name}\n</code></pre>\n\n<p>So far so good.. unfortunately the Paessler documentation of the property names is pretty light. It directs you to look at the edit page for the setting you want to query and check the name for the input box. For some settings this is fairly derivable, the object name is usually \"name\" and you can get the tags with \"tags\" for example. However determining the names of the Transaction Sensor URL step properties (such as URL):</p>\n\n<p><img src=\"/content/images/2016/09/TransactionURL-2.png\" alt=\"\" /></p>\n\n<p>Ultimately required guesswork. The answer turned out to be that the URL property is named:</p>\n\n<blockquote>\n  <p><strong>HTTPURL{n}</strong> </p>\n</blockquote>\n\n<p>i.e HTTPURL1, HTTPURL2, HTTPURL3, HTTPURL4, HTTPURL5 .. HTTPURL10.</p>\n\n<p>The API only permits you to query a single property of a single object at a time, so I used Powershell to loop through the settings that I wanted to query.</p>\n\n<p>Having worked out the above property name, I googled it to see if anyone had come before me. They seemingly had not, but I did come across a <a href=\"https://malwr.com/analysis/OGNhNjRlZTI4NmI4NGZlNGJjM2U2M2Y1ZWIwZjhkNzQ/\">malware analysis site that had analysed the PRTG core executable</a> and as a result had a page of string values, on which was HTTPURL1. This led me to the following additional list, which I tested against the HTTP Transaction sensor and confirmed are valid for that sensor type:</p>\n\n<p><strong>Transaction Sensor Property Names</strong></p>\n\n<p>cookies <br />\nhttpauthentication <br />\nhttpmethod1 <br />\nhttpmethod2 <br />\nhttpmethod3 <br />\nhttpmethod4 <br />\nhttpmethod5 <br />\nhttpmethod6 <br />\nhttpmethod7 <br />\nhttpmethod8 <br />\nhttpmethod9 <br />\nhttpmethod10 <br />\nhttppassword <br />\nhttpurl1 <br />\nhttpurl2 <br />\nhttpurl3 <br />\nhttpurl4 <br />\nhttpurl5 <br />\nhttpurl6 <br />\nhttpurl7 <br />\nhttpurl8 <br />\nhttpurl9 <br />\nhttpurl10 <br />\nhttpuser <br />\nincludemaynot1 <br />\nincludemaynot2 <br />\nincludemaynot3 <br />\nincludemaynot4 <br />\nincludemaynot5 <br />\nincludemaynot6 <br />\nincludemaynot7 <br />\nincludemaynot8 <br />\nincludemaynot9 <br />\nincludemaynot10 <br />\nincludemust1 <br />\nincludemust2 <br />\nincludemust3 <br />\nincludemust4 <br />\nincludemust5 <br />\nincludemust6 <br />\nincludemust7 <br />\nincludemust8 <br />\nincludemust9 <br />\nincludemust10 <br />\nmaxdownload <br />\nname <br />\nparenttags <br />\npostdata1 <br />\npostdata2 <br />\npostdata3 <br />\npostdata4 <br />\npostdata5 <br />\npostdata6 <br />\npostdata7 <br />\npostdata8 <br />\npostdata9 <br />\npostdata10 <br />\npriority <br />\nproxypassword <br />\nproxyport <br />\nproxyuser <br />\nsteptimeout <br />\ntags <br />\ntimeout</p>\n\n<p><em>-- Note: this is not an exhaustive list. Password properties return stars rather than the actual password (as does the priority property but I think that's intentional :) ).</em></p>\n\n<p>I wanted to find a property for knowing whether a transaction step is enabled or disabled (this is an option from step 2 onward) but unfortunately have not been able to find one.</p>\n\n<p>For the regular HTTP sensor types, the names above without numbers are likely all valid.</p>\n\n<p><strong>The script</strong></p>\n\n<p>Returning to the matter at hand, the full script for auditing the URL settings of my Transaction sensors is below. The script works as follows:</p>\n\n<ol>\n<li>Uses the PRTG API to get a filtered list of all sensor objects which match a certain type (as defined in the Param block) in JSON and converts this to a Powershell object. Note that whatever you set as <code>$SensorType</code> is sent via the query string so be careful with characters (such as spaces) that may need to be URL encoded.  </li>\n<li>Loops through the list of servers and gets the URL property for all 10 URLs steps (note that it checks them all whether they are all populated or not and it does not check or return whether the steps are enabled). Through the loop it adds each URL to the sensor object.  </li>\n<li>The final complete object (the original sensor settings and the 10 new columns with the URL details) are piped to a CSV file (you could modify where the output is redirected to whatever suits your needs). </li>\n</ol>\n\n<p>I've also got a bit of a thing for <code>Write-Progress</code> at the moment and this was a legitimate opportunity to use nested progress bars to report progress:</p>\n\n<p><img src=\"/content/images/2016/09/get-prtgsensordetails.png\" alt=\"\" /></p>\n\n<p>Here's the full script for anyone looking for a similar solution:</p>\n\n<script src=\"https://gist.github.com/markwragg/7640aeab480f2f4e4b278908d108c1ef.js\"></script>\n\n<h1 id=\"getautoacknowledgesettingforallpingsensorsfromprtgwithpowershell\">Get Auto Acknowledge setting for all Ping sensors from PRTG with Powershell</h1>\n\n<p>While browsing Paessler's Knowledge Base I came across <a href=\"https://kb.paessler.com/en/topic/71062-extract-auto-acknowledge-setting-from-all-sensors-via-api\">a post from someone looking to extract the auto acknowledge setting</a> (which appears to be an option on Ping sensors only) from all sensors in PRTG using the API. As this was fundamentally a similar problem as my script above, I made a quick modified version of it to achieve this result, which you can find below:</p>\n\n<script src=\"https://gist.github.com/markwragg/02a91560f46210cf67ca5c810681f6ae.js\"></script>",
                        "image": "/content/images/2016/09/PRTG-logo.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-09-01 14:38:11",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:03:12",
                        "updated_by": 1,
                        "published_at": "2016-09-01 21:31:39",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 32,
                        "uuid": "d319b3e1-fc7f-48f7-970f-738b80606909",
                        "title": "Using Write-Progress to provide feedback in Powershell",
                        "slug": "using-write-progress-to-provide-feedback-in-powershell",
                        "markdown": "I like my scripts to give feedback to the console to demonstrate progress where possible and Powershell provides a number of cmdlets to do this, one of which is `Write-Progress`.\n\n> The Write-Progress cmdlet displays a progress bar in a Windows PowerShell command window that depicts the status of a running command or script. You can select the indicators that the bar reflects and the text that appears above and below the progress bar.\n>\n> *Source: https://technet.microsoft.com/en-us/library/hh849902.aspx*\n\nThe progress bar and status messages displayed by `Write-Progress` float automatically at the top of the page, overlaying whatever text is present. There are several different ways to use `Write-Progress` some of which may surprise you:\n\n## 1. You don't *actually* need to know how much progress you've made to use Write-Progress\n\nSurprisingly, you can use `Write-Progress` without actually displaying a progress bar. This can be useful if you want to utilise a floating progress notification but have no way of knowing how much progress has been made. For example:\n\n```language-powershell\nWrite-Progress -Activity \"I'm going to sleep for 5 seconds\" -Status \"Zzzzz\"\nStart-Sleep 5\n```\n![](/content/images/2016/09/write-progress-no-progress.png)\n\nTo be explicit about not knowing how much progress to report, you can specify `-PercentComplete -1`.\n\n## 2. You can tell an instance of Write-Progress to disappear\n\nIf you want a Progress message to disappear while your script continues to do other processing, you can dismiss them using the `-Completed` switch. For example:\n\n```language-powershell\nWrite-Progress -Activity \"I'm going to sleep for 5 seconds\" -Status \"Zzzzz\"\nStart-Sleep 5\nWrite-Progress -Activity \"Sleep\" -Completed\nStart-Sleep 2\n```\n\n## 3. You can use Write-Progress to count down a number of seconds remaining\n\nThis is most useful when your script needs to wait for a predetermined amount of time before proceeding and you want to show the user how long they have left to wait. To do this use the `-SecondsRemaining` switch. For example:\n\n```language-powershell\nFor ($i=5; $i -gt 1; $i–-) {\n    Write-Progress -Activity \"Launching rocket\" -SecondsRemaining $i\n    Start-Sleep 1\n}\n```\n![](/content/images/2016/09/write-progress-seconds-countdown.png)\n\n![](/content/images/2016/09/rocket-launch-1.jpg)\n\n\n## 4. To show a progress bar, just calculate the percentage of work completed\n\nTo show a progress bar you use the `-PercentComplete` switch to report a value between 0 and 100. Within a `For` loop you can likely do this using the variables that count the iterations. For a `ForEach-Object` loop, its best to use a counter variable which you increment on each iteration and then the `.Count` property of your collection for the total. For example:\n\n```language-powershell\n$WinSxS = Get-ChildItem C:\\Windows\\WinSxS\n$i = 1\n\n$WinSxS | ForEach-Object {\n    Write-Progress -Activity \"Counting WinSxS file $($_.name)\" -Status \"File $i of $($WinSxS.Count)\" -PercentComplete (($i / $WinSxS.Count) * 100)  \n    $i++\n}\n```\n![](/content/images/2016/09/write-progress-bar.png)\n## 5. You can show more than one progress bar at a time\n\nReferred to as nested progress bars, you simply need to use the `-id` parameter to ensure they are not layered on top of each other. The first progress bar does not need an ID, but you then need to number all subsequent ones sequentially in the order you want them layered.\n\nThe following script demonstrates four nested progress bars by displaying a running clock of the current time.\n\n<script src=\"https://gist.github.com/markwragg/73addf16504caaf72da1633cdac57e68.js\"></script>\n\n![](/content/images/2016/09/watch-timepassing.png)\n\n\nThe script ends automatically at midnight (to have it running perpetually, change the last `Until` statement to `While ($true)`).",
                        "html": "<p>I like my scripts to give feedback to the console to demonstrate progress where possible and Powershell provides a number of cmdlets to do this, one of which is <code>Write-Progress</code>.</p>\n\n<blockquote>\n  <p>The Write-Progress cmdlet displays a progress bar in a Windows PowerShell command window that depicts the status of a running command or script. You can select the indicators that the bar reflects and the text that appears above and below the progress bar.</p>\n  \n  <p><em>Source: <a href=\"https://technet.microsoft.com/en-us/library/hh849902.aspx\">https://technet.microsoft.com/en-us/library/hh849902.aspx</a></em></p>\n</blockquote>\n\n<p>The progress bar and status messages displayed by <code>Write-Progress</code> float automatically at the top of the page, overlaying whatever text is present. There are several different ways to use <code>Write-Progress</code> some of which may surprise you:</p>\n\n<h2 id=\"1youdontactuallyneedtoknowhowmuchprogressyouvemadetousewriteprogress\">1. You don't <em>actually</em> need to know how much progress you've made to use Write-Progress</h2>\n\n<p>Surprisingly, you can use <code>Write-Progress</code> without actually displaying a progress bar. This can be useful if you want to utilise a floating progress notification but have no way of knowing how much progress has been made. For example:</p>\n\n<pre><code class=\"language-powershell\">Write-Progress -Activity \"I'm going to sleep for 5 seconds\" -Status \"Zzzzz\"  \nStart-Sleep 5  \n</code></pre>\n\n<p><img src=\"/content/images/2016/09/write-progress-no-progress.png\" alt=\"\" /></p>\n\n<p>To be explicit about not knowing how much progress to report, you can specify <code>-PercentComplete -1</code>.</p>\n\n<h2 id=\"2youcantellaninstanceofwriteprogresstodisappear\">2. You can tell an instance of Write-Progress to disappear</h2>\n\n<p>If you want a Progress message to disappear while your script continues to do other processing, you can dismiss them using the <code>-Completed</code> switch. For example:</p>\n\n<pre><code class=\"language-powershell\">Write-Progress -Activity \"I'm going to sleep for 5 seconds\" -Status \"Zzzzz\"  \nStart-Sleep 5  \nWrite-Progress -Activity \"Sleep\" -Completed  \nStart-Sleep 2  \n</code></pre>\n\n<h2 id=\"3youcanusewriteprogresstocountdownanumberofsecondsremaining\">3. You can use Write-Progress to count down a number of seconds remaining</h2>\n\n<p>This is most useful when your script needs to wait for a predetermined amount of time before proceeding and you want to show the user how long they have left to wait. To do this use the <code>-SecondsRemaining</code> switch. For example:</p>\n\n<pre><code class=\"language-powershell\">For ($i=5; $i -gt 1; $i–-) {  \n    Write-Progress -Activity \"Launching rocket\" -SecondsRemaining $i\n    Start-Sleep 1\n}\n</code></pre>\n\n<p><img src=\"/content/images/2016/09/write-progress-seconds-countdown.png\" alt=\"\" /></p>\n\n<p><img src=\"/content/images/2016/09/rocket-launch-1.jpg\" alt=\"\" /></p>\n\n<h2 id=\"4toshowaprogressbarjustcalculatethepercentageofworkcompleted\">4. To show a progress bar, just calculate the percentage of work completed</h2>\n\n<p>To show a progress bar you use the <code>-PercentComplete</code> switch to report a value between 0 and 100. Within a <code>For</code> loop you can likely do this using the variables that count the iterations. For a <code>ForEach-Object</code> loop, its best to use a counter variable which you increment on each iteration and then the <code>.Count</code> property of your collection for the total. For example:</p>\n\n<pre><code class=\"language-powershell\">$WinSxS = Get-ChildItem C:\\Windows\\WinSxS\n$i = 1\n\n$WinSxS | ForEach-Object {\n    Write-Progress -Activity \"Counting WinSxS file $($_.name)\" -Status \"File $i of $($WinSxS.Count)\" -PercentComplete (($i / $WinSxS.Count) * 100)  \n    $i++\n}\n</code></pre>\n\n<p><img src=\"/content/images/2016/09/write-progress-bar.png\" alt=\"\" /></p>\n\n<h2 id=\"5youcanshowmorethanoneprogressbaratatime\">5. You can show more than one progress bar at a time</h2>\n\n<p>Referred to as nested progress bars, you simply need to use the <code>-id</code> parameter to ensure they are not layered on top of each other. The first progress bar does not need an ID, but you then need to number all subsequent ones sequentially in the order you want them layered.</p>\n\n<p>The following script demonstrates four nested progress bars by displaying a running clock of the current time.</p>\n\n<script src=\"https://gist.github.com/markwragg/73addf16504caaf72da1633cdac57e68.js\"></script>\n\n<p><img src=\"/content/images/2016/09/watch-timepassing.png\" alt=\"\" /></p>\n\n<p>The script ends automatically at midnight (to have it running perpetually, change the last <code>Until</code> statement to <code>While ($true)</code>).</p>",
                        "image": "/content/images/2016/09/time-passing.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-09-05 10:49:55",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:02:45",
                        "updated_by": 1,
                        "published_at": "2016-09-05 19:48:07",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 33,
                        "uuid": "c3102349-f1a9-4eb2-8876-b4b40b562537",
                        "title": "Cleaning up paused devices from PRTG with Powershell",
                        "slug": "clean-up-paused-devices-from-prtg-with-powershell",
                        "markdown": "One of our AWS based products uses auto-scaling and when new instances are deployed a script automatically creates sensors for them in PRTG (our monitoring tool). When the instances are scaled down/terminated there is not a script that automatically removes them from PRTG (in part so that we can temporarily retain the monitoring history). As a result, our monitoring over time can become cluttered with redundant paused devices, so a script was needed to automate the process of clearing those down.\n\nAs covered previously, PRTG has a HTTP based API that provides a way to query the configuration and return results as JSON (my preference) or XML or to manipulate objects by passing parameters through the querystring.\n\nAfter I had the basic needed functionality written in Powershell, I underwent several revisions as I was keen to try out (and understand the implementation of) some Powershell functionality that I hadn't used much in the past. \n\n# Whatif and Confirm\n\nThe first of those was to add `-whatif` and `-confirm` functionality to the script. In case you're not aware: \n\n- `-whatif` makes a cmdlet print out the changes it would make without making them.\n- `-confirm` makes a cmdlet prompt for confirmation before making each change (with the option to say yes/no to all).\n\nAdding the following to the top of a script or function:\n```\n[CmdletBinding()]\n```\nEnables you to use common parameters such as `-verbose` and `-debug` to turn on and off any `write-verbose` and `write-debug` messages you've included.\n\nExtending this to be:\n```\n[CmdletBinding(SupportsShouldProcess=$True,ConfirmImpact=Medium)]\n```\nIs the first step to enabling `-confirm` and `-whatif`. You don't have to specify `ConfirmImpact` (Medium is the default I believe) but this parameter controls whether confirm is on or off by default and can be set to [High, Medium, Low or None](https://msdn.microsoft.com/en-us/library/system.management.automation.confirmimpact(v=vs.85).aspx).\n\nAs PRTG doesn't provide an easy way to reverse the deletion of an object (you'd need to restore the whole configuration database from a backup) I've set `ConfirmImpact=High`. As a result, my script will always prompt for confirmation even when `-confirm` hasn't been explicitly set. Doing this allowed me to strip out of my script a manual \"Are you sure?\" check via `read-host`.\n\n> It's worth noting that if a user changes their `$confirmpreference` variable to \"none\" they'll override my default.\n\nThe second step needed to fully implement `-whatif` and `-confirm` is to identify what parts of your script are destructive and surround them with the following `if` block:\n\n```\nIf ($PSCmdlet.ShouldProcess(\"$($_.device)\",\"PRTG: Delete Object\"))\n{\n   Destructive bit of code..\n}\n```\nTwo parameters you pass to `ShouldProcess` (as shown above) populate the whatif message with what is being affected and what the effect is. And that's it, `-whatif` and `-confirm` parameters implemented!\n\n![](/content/images/2016/09/RemovePRTGPausedDevices-WhatIf-Confirm.png)\n\n# Script to get paused PRTG devices and remove them\nAt this point I had a single script for getting a list of paused devices based on several ways that I wanted to filter for those (which the script accepts as parameters):\n\n- `-name` filters the list of paused devices to those that partially match the text provided in the device name field\n- `-message` filters the list of paused devices to those that partially match the text provided in the message (comment added by a user) field. This was useful for me because we had a process where certain sensors were paused referenced a particular runbook ID.\n- `-dayspaused` filters the list of paused devices to just those that have been paused at least this many days.\n\nThese filters can be combined. For example to match any devices partially named \"LON\" with \"Paused by me\" in the message field that have been paused for 7 days or longer you'd run the script with:\n\n```\n-name \"LON\" -message \"Paused by me\" -dayspaused 7 -prtgurl prtg.mycompany.com -username myuser -passhash 12345\n```\nNoting that (as above) you also need to include the URL to your server (excluding https://) and a username and passhash that has permissions to query the API. Here's the script:\n\n<script src=\"https://gist.github.com/markwragg/18ef05965e2f6b2cf0ea150e7c52d487.js\"></script>\n\n# Getters and Setters\n\nFrom here, it occurred to me that as I was doing two distinct actions a \"Get\" type action to retrieve and filter the data I wanted and a \"Set\" type action to make changes via that collection (in this case a Remove action) the script warranted breaking in to those components, with the ability to use the pipeline to communicate objects between them.\n\nMaking these seperate functions makes them more flexible, but ultimately what I wanted was the original goal to continue to be as simple as \n\n```\nGet-PRTGPausedDevices -name \"somedevices\" | Remove-PRTGPausedDevices\n```\n\nI actually ended up with 3 functions and here's why:\n\n1. I obviously needed a `Get-PRTGPausedDevices` function to do the grunt work of querying the PRTG API, filtering the results and returning that as a Powershell object\n2. I needed a very simple `Remove-PRTGDevices` function that I could pass the results to and trigger the removal from PRTG (I even descoped the \"Paused\" part at this point and figured this function could  be simplified to just remove anything you want from PRTG). But it occurred to me it still made sense to have..\n3. A `Remove-PRTGPausedDevices` function that removed Paused devices, but that could also be used in isolation to also do the get/filtering part per the first function. The reason this made sense to me is because (for example) you can use the `Remove-Item` cmdlet on it's own to delete files. You don't have to use `Get-ChildItem` and pipe the results to it. But you can do that. So what I wanted was a similar function, that could be used on it's own via parameters (per my original script) but that could also accept pipeline input from my Get-PRTGPausedDevices function or from any other source that might be suitable in the future.\n\n# PRTG paused devices module\nHere's the 3 functions described above packaged as a module. To use them, simply download the file, load it with:\n```\nImport-Module prtgPausedDevices.psml\n```\nand then execute the above listed cmdlet names.\n<script src=\"https://gist.github.com/markwragg/cd9fd02ad539fb5dd4adfa6add0909bc.js\"></script>\n\n# Things I learned\n\nI had not developed functions in this style before and I learnt a few things along the way. I don't think the above module represents \"best practice\", particularly on the issue of credentials that I will cover later. Finding some indication of best practice for this sort of thing turned out to be very difficult, so the above is a working solution with some choices made that I will describe below.\n\nImplementing pipeline input for my `Remove-` functions was a simple case of declaring one or more parameters as\n```\n[int][Parameter(ValueFromPipelineByPropertyName=$True)]$objid\n```\nWhich I did as a minimum with $objid so that I had a unique identifier with which to perform the delete actions against. `ValueFromPipelineByPropertyName` makes the script automatically match any incoming properties to the same named parameters, but what's cool is that if you're also passing in objects that have other data that data isn't lost and is available for use by the script. This meant that when I was piping in the Device name, I could use it without it being explicitly defined as a parameter.\n\nYou can optionally choose to structure your function with the following blocks (Begin and End are optional):\n\n```\nBegin{\n    Things that happen once before anything else \n    (including before there being any pipeline input).\n}\nProcess{\n    Thing that happens for each pipeline object.\n}\nEnd{\n    Thing that happens once at the end.\n}\n```\n\nThere's some efficiency to working this way, but there's some caveats too:\n\n- You can't count the total of how many objects have been piped in to the function or (as far as I could tell) keep a track of how far through the list of objects you were (e.g to use with `write-progress`). \n- You also can't work with the pipeline input at all in the Begin phase as at this point it doesn't exist. That being said I liked having these 3 structural blocks so used this structure for my more indepth Remove-PRTGPausedDevices function. \n- It meant I could use the Begin section to perform the \"Get-\" behaviour if there was not pipeline input, based on any defined parameters I also had to include a ForEach-Object loop in to the Process section (despite that section looping by design) to cover where there was no Pipeline input and the function was generating its own input.\n\nFor the smaller/simpler `Remove-PRTGDevices` function, I went with the other option which was to not have Begin..Process..End blocks at all. Instead you use a ForEach-Object block to loop through the pipeline and you can use the default $Input object to query the collection, so can get a count of how many objects there are and report progress via `Write-Progress`.\n![](/content/images/2016/09/Remove-PRTGDevices.png)\n*-- I still use `Write-Progress` to give feedback in the other function, I just can't show progress through the total. However as the web calls can be slow it still seemed worthwhile to use it to give the user feedback.*\n\nThe final functionality I wanted to include is likely the most controversial. I wanted it so that I could accept the PRTGURL, Username and Passhash parameters via the Pipeline from the other cmdlets. So that I could have:\n```\nGet-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 | Remove-PRTGPausedDevices -WhatIf\n```\nRather than:\n```\nGet-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 | Remove-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 -WhatIf\n```\n\nPutting aside the issue of passing credentials in plaintext (I played around with ConvertTo/From-SecureString for a bit but quickly came unstuck) deciding the right way to communicate the credentials between the Functions was difficult.\n\nOne option was to just define them as Globally scoped variables in the `Get-` function with e.g \n```\nSet-Variable -Name PRTGURL -Value $PRTGURL\n```\nSo that they were then accessible to the other functions. I then cleaned them up with `Remove-Variable` in the End section, but if someone hits CTRL+C during processing they are never cleaned up.\n\nThe solution I went with in the end was to simply include them as additional properties of each object during the `Get-` function as so:\n```\n$_ | Add-Member –MemberType NoteProperty –Name \"PRTGURL\" –Value $PRTGURL -Force\n$_ | Add-Member –MemberType NoteProperty –Name \"username\" –Value $Username -Force\n$_ | Add-Member –MemberType NoteProperty –Name \"passhash\" –Value $Passhash -Force\n            \n```\nThe downside is that the credentials are duplicated horribly, once for each object in the pipeline.\n\nThe upside is that I can accept them automatically in the `Remove-` functions simply by having them as Parameters that accept Pipeline input. Further I can make these parameters Mandatory and then if they are coming in via the Pipeline the user isn't prompted for them, but if they aren't provided via the pipeline the user has to provide them. This wasn't possible (or as easy anyway) with them as Global variables.\n\nI did briefly consider having a nested object, so that I could have the sort of parent properties (PRTGURL and credentials) and then a child object that had all of the devices. This broke some functionality i'd added earlier, which was the below (from the End block), which sets a nice default view output for the `Get-PRTGPausedDevices` function where it only displays the device name, message and paused date and using `Format-Table` by default:\n```\n$Devices.PSObject.TypeNames.Insert(0,'PRTG.PausedDevice')\n$defaultDisplaySet = 'device','message','paused'\n$defaultDisplayPropertySet = New-Object System.Management.Automation.PSPropertySet(‘DefaultDisplayPropertySet’,[string[]]$defaultDisplaySet)\n$PSStandardMembers = [System.Management.Automation.PSMemberInfo[]]@($defaultDisplayPropertySet)\n$Devices | Add-Member MemberSet PSStandardMembers $PSStandardMembers\n```\n![](/content/images/2016/09/Get-PRTGPausedDevices-Format-Table.png)\nI'm sure it is possible to get this functionality working even where there's a nested Object, but trying to do so started to make the whole thing horribly complex.\n\nI'm also sure there's a better answer to handling credentials (or \"parent\" type properties) between functions via the pipeline, so if you know it please comment! Overall i'm pretty happy with the result and (until I know better) I would use this framework in the future for developing similar functionality.",
                        "html": "<p>One of our AWS based products uses auto-scaling and when new instances are deployed a script automatically creates sensors for them in PRTG (our monitoring tool). When the instances are scaled down/terminated there is not a script that automatically removes them from PRTG (in part so that we can temporarily retain the monitoring history). As a result, our monitoring over time can become cluttered with redundant paused devices, so a script was needed to automate the process of clearing those down.</p>\n\n<p>As covered previously, PRTG has a HTTP based API that provides a way to query the configuration and return results as JSON (my preference) or XML or to manipulate objects by passing parameters through the querystring.</p>\n\n<p>After I had the basic needed functionality written in Powershell, I underwent several revisions as I was keen to try out (and understand the implementation of) some Powershell functionality that I hadn't used much in the past. </p>\n\n<h1 id=\"whatifandconfirm\">Whatif and Confirm</h1>\n\n<p>The first of those was to add <code>-whatif</code> and <code>-confirm</code> functionality to the script. In case you're not aware: </p>\n\n<ul>\n<li><code>-whatif</code> makes a cmdlet print out the changes it would make without making them.</li>\n<li><code>-confirm</code> makes a cmdlet prompt for confirmation before making each change (with the option to say yes/no to all).</li>\n</ul>\n\n<p>Adding the following to the top of a script or function:  </p>\n\n<pre><code>[CmdletBinding()]\n</code></pre>\n\n<p>Enables you to use common parameters such as <code>-verbose</code> and <code>-debug</code> to turn on and off any <code>write-verbose</code> and <code>write-debug</code> messages you've included.</p>\n\n<p>Extending this to be:  </p>\n\n<pre><code>[CmdletBinding(SupportsShouldProcess=$True,ConfirmImpact=Medium)]\n</code></pre>\n\n<p>Is the first step to enabling <code>-confirm</code> and <code>-whatif</code>. You don't have to specify <code>ConfirmImpact</code> (Medium is the default I believe) but this parameter controls whether confirm is on or off by default and can be set to <a href=\"https://msdn.microsoft.com/en-us/library/system.management.automation.confirmimpact(v=vs.85).aspx\">High, Medium, Low or None</a>.</p>\n\n<p>As PRTG doesn't provide an easy way to reverse the deletion of an object (you'd need to restore the whole configuration database from a backup) I've set <code>ConfirmImpact=High</code>. As a result, my script will always prompt for confirmation even when <code>-confirm</code> hasn't been explicitly set. Doing this allowed me to strip out of my script a manual \"Are you sure?\" check via <code>read-host</code>.</p>\n\n<blockquote>\n  <p>It's worth noting that if a user changes their <code>$confirmpreference</code> variable to \"none\" they'll override my default.</p>\n</blockquote>\n\n<p>The second step needed to fully implement <code>-whatif</code> and <code>-confirm</code> is to identify what parts of your script are destructive and surround them with the following <code>if</code> block:</p>\n\n<pre><code>If ($PSCmdlet.ShouldProcess(\"$($_.device)\",\"PRTG: Delete Object\"))  \n{\n   Destructive bit of code..\n}\n</code></pre>\n\n<p>Two parameters you pass to <code>ShouldProcess</code> (as shown above) populate the whatif message with what is being affected and what the effect is. And that's it, <code>-whatif</code> and <code>-confirm</code> parameters implemented!</p>\n\n<p><img src=\"/content/images/2016/09/RemovePRTGPausedDevices-WhatIf-Confirm.png\" alt=\"\" /></p>\n\n<h1 id=\"scripttogetpausedprtgdevicesandremovethem\">Script to get paused PRTG devices and remove them</h1>\n\n<p>At this point I had a single script for getting a list of paused devices based on several ways that I wanted to filter for those (which the script accepts as parameters):</p>\n\n<ul>\n<li><code>-name</code> filters the list of paused devices to those that partially match the text provided in the device name field</li>\n<li><code>-message</code> filters the list of paused devices to those that partially match the text provided in the message (comment added by a user) field. This was useful for me because we had a process where certain sensors were paused referenced a particular runbook ID.</li>\n<li><code>-dayspaused</code> filters the list of paused devices to just those that have been paused at least this many days.</li>\n</ul>\n\n<p>These filters can be combined. For example to match any devices partially named \"LON\" with \"Paused by me\" in the message field that have been paused for 7 days or longer you'd run the script with:</p>\n\n<pre><code>-name \"LON\" -message \"Paused by me\" -dayspaused 7 -prtgurl prtg.mycompany.com -username myuser -passhash 12345\n</code></pre>\n\n<p>Noting that (as above) you also need to include the URL to your server (excluding https://) and a username and passhash that has permissions to query the API. Here's the script:</p>\n\n<script src=\"https://gist.github.com/markwragg/18ef05965e2f6b2cf0ea150e7c52d487.js\"></script>\n\n<h1 id=\"gettersandsetters\">Getters and Setters</h1>\n\n<p>From here, it occurred to me that as I was doing two distinct actions a \"Get\" type action to retrieve and filter the data I wanted and a \"Set\" type action to make changes via that collection (in this case a Remove action) the script warranted breaking in to those components, with the ability to use the pipeline to communicate objects between them.</p>\n\n<p>Making these seperate functions makes them more flexible, but ultimately what I wanted was the original goal to continue to be as simple as </p>\n\n<pre><code>Get-PRTGPausedDevices -name \"somedevices\" | Remove-PRTGPausedDevices  \n</code></pre>\n\n<p>I actually ended up with 3 functions and here's why:</p>\n\n<ol>\n<li>I obviously needed a <code>Get-PRTGPausedDevices</code> function to do the grunt work of querying the PRTG API, filtering the results and returning that as a Powershell object  </li>\n<li>I needed a very simple <code>Remove-PRTGDevices</code> function that I could pass the results to and trigger the removal from PRTG (I even descoped the \"Paused\" part at this point and figured this function could  be simplified to just remove anything you want from PRTG). But it occurred to me it still made sense to have..  </li>\n<li>A <code>Remove-PRTGPausedDevices</code> function that removed Paused devices, but that could also be used in isolation to also do the get/filtering part per the first function. The reason this made sense to me is because (for example) you can use the <code>Remove-Item</code> cmdlet on it's own to delete files. You don't have to use <code>Get-ChildItem</code> and pipe the results to it. But you can do that. So what I wanted was a similar function, that could be used on it's own via parameters (per my original script) but that could also accept pipeline input from my Get-PRTGPausedDevices function or from any other source that might be suitable in the future.</li>\n</ol>\n\n<h1 id=\"prtgpauseddevicesmodule\">PRTG paused devices module</h1>\n\n<p>Here's the 3 functions described above packaged as a module. To use them, simply download the file, load it with:  </p>\n\n<pre><code>Import-Module prtgPausedDevices.psml  \n</code></pre>\n\n<p>and then execute the above listed cmdlet names.  </p>\n\n<script src=\"https://gist.github.com/markwragg/cd9fd02ad539fb5dd4adfa6add0909bc.js\"></script>\n\n<h1 id=\"thingsilearned\">Things I learned</h1>\n\n<p>I had not developed functions in this style before and I learnt a few things along the way. I don't think the above module represents \"best practice\", particularly on the issue of credentials that I will cover later. Finding some indication of best practice for this sort of thing turned out to be very difficult, so the above is a working solution with some choices made that I will describe below.</p>\n\n<p>Implementing pipeline input for my <code>Remove-</code> functions was a simple case of declaring one or more parameters as  </p>\n\n<pre><code>[int][Parameter(ValueFromPipelineByPropertyName=$True)]$objid\n</code></pre>\n\n<p>Which I did as a minimum with $objid so that I had a unique identifier with which to perform the delete actions against. <code>ValueFromPipelineByPropertyName</code> makes the script automatically match any incoming properties to the same named parameters, but what's cool is that if you're also passing in objects that have other data that data isn't lost and is available for use by the script. This meant that when I was piping in the Device name, I could use it without it being explicitly defined as a parameter.</p>\n\n<p>You can optionally choose to structure your function with the following blocks (Begin and End are optional):</p>\n\n<pre><code>Begin{  \n    Things that happen once before anything else \n    (including before there being any pipeline input).\n}\nProcess{  \n    Thing that happens for each pipeline object.\n}\nEnd{  \n    Thing that happens once at the end.\n}\n</code></pre>\n\n<p>There's some efficiency to working this way, but there's some caveats too:</p>\n\n<ul>\n<li>You can't count the total of how many objects have been piped in to the function or (as far as I could tell) keep a track of how far through the list of objects you were (e.g to use with <code>write-progress</code>). </li>\n<li>You also can't work with the pipeline input at all in the Begin phase as at this point it doesn't exist. That being said I liked having these 3 structural blocks so used this structure for my more indepth Remove-PRTGPausedDevices function. </li>\n<li>It meant I could use the Begin section to perform the \"Get-\" behaviour if there was not pipeline input, based on any defined parameters I also had to include a ForEach-Object loop in to the Process section (despite that section looping by design) to cover where there was no Pipeline input and the function was generating its own input.</li>\n</ul>\n\n<p>For the smaller/simpler <code>Remove-PRTGDevices</code> function, I went with the other option which was to not have Begin..Process..End blocks at all. Instead you use a ForEach-Object block to loop through the pipeline and you can use the default $Input object to query the collection, so can get a count of how many objects there are and report progress via <code>Write-Progress</code>. <br />\n<img src=\"/content/images/2016/09/Remove-PRTGDevices.png\" alt=\"\" />\n<em>-- I still use <code>Write-Progress</code> to give feedback in the other function, I just can't show progress through the total. However as the web calls can be slow it still seemed worthwhile to use it to give the user feedback.</em></p>\n\n<p>The final functionality I wanted to include is likely the most controversial. I wanted it so that I could accept the PRTGURL, Username and Passhash parameters via the Pipeline from the other cmdlets. So that I could have:  </p>\n\n<pre><code>Get-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 | Remove-PRTGPausedDevices -WhatIf  \n</code></pre>\n\n<p>Rather than:  </p>\n\n<pre><code>Get-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 | Remove-PRTGPausedDevices -PRTGURL prtg.myserver.com -Username Mark -Passhash 12345 -WhatIf  \n</code></pre>\n\n<p>Putting aside the issue of passing credentials in plaintext (I played around with ConvertTo/From-SecureString for a bit but quickly came unstuck) deciding the right way to communicate the credentials between the Functions was difficult.</p>\n\n<p>One option was to just define them as Globally scoped variables in the <code>Get-</code> function with e.g  </p>\n\n<pre><code>Set-Variable -Name PRTGURL -Value $PRTGURL  \n</code></pre>\n\n<p>So that they were then accessible to the other functions. I then cleaned them up with <code>Remove-Variable</code> in the End section, but if someone hits CTRL+C during processing they are never cleaned up.</p>\n\n<p>The solution I went with in the end was to simply include them as additional properties of each object during the <code>Get-</code> function as so:  </p>\n\n<pre><code>$_ | Add-Member –MemberType NoteProperty –Name \"PRTGURL\" –Value $PRTGURL -Force\n$_ | Add-Member –MemberType NoteProperty –Name \"username\" –Value $Username -Force\n$_ | Add-Member –MemberType NoteProperty –Name \"passhash\" –Value $Passhash -Force\n</code></pre>\n\n<p>The downside is that the credentials are duplicated horribly, once for each object in the pipeline.</p>\n\n<p>The upside is that I can accept them automatically in the <code>Remove-</code> functions simply by having them as Parameters that accept Pipeline input. Further I can make these parameters Mandatory and then if they are coming in via the Pipeline the user isn't prompted for them, but if they aren't provided via the pipeline the user has to provide them. This wasn't possible (or as easy anyway) with them as Global variables.</p>\n\n<p>I did briefly consider having a nested object, so that I could have the sort of parent properties (PRTGURL and credentials) and then a child object that had all of the devices. This broke some functionality i'd added earlier, which was the below (from the End block), which sets a nice default view output for the <code>Get-PRTGPausedDevices</code> function where it only displays the device name, message and paused date and using <code>Format-Table</code> by default:  </p>\n\n<pre><code>$Devices.PSObject.TypeNames.Insert(0,'PRTG.PausedDevice')\n$defaultDisplaySet = 'device','message','paused'\n$defaultDisplayPropertySet = New-Object System.Management.Automation.PSPropertySet(‘DefaultDisplayPropertySet’,[string[]]$defaultDisplaySet)\n$PSStandardMembers = [System.Management.Automation.PSMemberInfo[]]@($defaultDisplayPropertySet)\n$Devices | Add-Member MemberSet PSStandardMembers $PSStandardMembers\n</code></pre>\n\n<p><img src=\"/content/images/2016/09/Get-PRTGPausedDevices-Format-Table.png\" alt=\"\" />\nI'm sure it is possible to get this functionality working even where there's a nested Object, but trying to do so started to make the whole thing horribly complex.</p>\n\n<p>I'm also sure there's a better answer to handling credentials (or \"parent\" type properties) between functions via the pipeline, so if you know it please comment! Overall i'm pretty happy with the result and (until I know better) I would use this framework in the future for developing similar functionality.</p>",
                        "image": "/content/images/2016/09/isometricBG_5.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Powershell pipeline functions to clean up paused devices from PRTG",
                        "meta_description": "I developed Get- and Remove- Powershell functions to clean up paused devices in PRTG while investigating passing objects between functions via the pipeline.",
                        "author_id": 1,
                        "created_at": "2016-09-08 16:59:33",
                        "created_by": 1,
                        "updated_at": "2016-09-11 19:08:44",
                        "updated_by": 1,
                        "published_at": "2016-09-11 18:50:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 35,
                        "uuid": "fb6023ed-2ca0-4ffa-910b-da592651538e",
                        "title": "Getting started with Powershell",
                        "slug": "getting-started-with-powershell",
                        "markdown": "This post is a list of resources and tips to help anyone new to Windows Powershell in getting started with the language. If Powershell is completely new to you, I recommend you review all of the listed resources. If you find other great resources along the way, please feel free to comment below and i'll contribute them to this list.\n\n# Getting Started\n\n[Getting Started with PowerShell 3.0 Jump Start](https://mva.microsoft.com/en-us/training-courses/getting-started-with-powershell-3-0-jump-start-8276?l=r54IrOWy_2304984382)\n(video series)\n\n> This Jump Start Microsoft PowerShell course is designed to teach busy IT professionals, admins, and help desk persons about how to use PowerShell to improve management capabilities, automate redundant tasks, and manage the environment in scale. Through this PowerShell tutorial, learn how PowerShell works – and how to make PowerShell work for you – from experts Jeffrey Snover, the inventor of PowerShell, and Jason Helmick, Senior Technologist at Concentrated Technology.\n\nAlthough this was released around Powershell 3 (and we're currently on version 5) this is still a very relevant introduction to Powershell, as each new version of Powershell builds on the last.\n\n[My 12 PowerShell Best Practices](http://windowsitpro.com/blog/my-12-powershell-best-practices) (Blog Series)\n\n> A 12-part series of posts entitled \"What To Do / Not To Do in PowerShell.\" These summarize my 12 major best practices for the shell, many of which are also enthusiastically endorsed by the broader PowerShell community.\n\nThis promotes some essential best practices that will make your scripts more robust and easier for others to read/understand, so it's a good idea to develop these habits early.\n\n[Powershell Basic Cheat Sheet](http://ramblingcookiemonster.github.io/images/Cheat-Sheets/powershell-basic-cheat-sheet2.pdf) (PDF)\n\n> PowerShell is a task based command line shell and scripting language. To run it, click Start, type PowerShell, run PowerShell ISE or PowerShell as Administrator. Commands are written in verb-noun form, and named parameters start with a dash.\n\nThis is worth printing and keeping nearby when coding as a handy reference guide to the basic constructs and conventions.\n\n[How to Install Windows 2016 TP5](How to Install Windows 2016 TP5) (Blog Post)\n\n> This blog post (by me :) ) takes you through how to road test Windows Server 2016 Technical Preview 5 using VMWare Player as a desktop hypervisor. Note that you'll need to do this on a machine with a fair amount of spare RAM.\nWhile this isn't strictly a Powershell resource, having a server VM to play around with is useful when learning Powershell.\n\nThis earlier blog post details how to install the trial version of the latest Windows Operating system where you can play around with the latest version of Powershell on the latest OS. You might want to spin up multiple machines if you want to experiment with using Powershell to communicate between servers.\n\n[Powerscripting Podcast](https://powershell.org/podcast/) (Podcast)\n\n> Shortly after PowerShell's introduction to the world, Jon Walz and Hal Rottenberg launched the PowerScripting Podcast, a weekly show featuring expert guest speakers, fantastic behind-the-scenes information, members of the PowerShell community, and more.\n\nThe more recent shows tend to be interviews on developments in Powershell and the community, but go back to episode 1 and the first 20 or so episodes do a good job of explaining and exploring the basics and this is a good way to use dead time such as a commute to immerse yourself in Powershell.\n\n[Learn Powershell in a Month of Lunches](https://www.amazon.co.uk/Learn-Windows-PowerShell-Month-Lunches/dp/1617290211) (Book / eBook)\n\n> *Learn Windows PowerShell in a Month of Lunches* is an innovative tutorial designed for busy administrators. Author Don Jones has taught thousands of administrators to use PowerShell, and now he brings his years of training techniques to a concise, easy-to-follow book. Just set aside one hour a day—lunchtime would be perfect—for an entire month, and readers will be automating administrative tasks faster than they ever thought possible.\n\nI have not personally read it, but this book is recommended almost everywhere as a great way to learn Powershell.\n\n# General Tips\n- Google is your friend, but beware that as parameters start with a hyphen you might need to enclose some of your search with speechmarks as using a hyphen in a Google search means to exclude that term by default. E.g: instead of get-aduser -properties do get-aduser \"-properties\"\n- StackExchange is your other friend and someone has probably already asked your question: http://stackoverflow.com/questions/tagged/powershell\n- The console and ISE both support tab completion, so you can type part of a cmdlet name then tab through the options. Also the ISE has intellisense, it will show tooltips to help you complete commands and it will red underline coding errors.\n- Get-Help is sometimes helpful (although Technet or ss64 are likely easier to read), e.g get-help get-service. Also try using it with any of these parameters: -examples -detailed or -full.\n- Powershell is all about the pipeline. Cmdlets return collections of results known as \"objects\" that can be passed forward down the pipeline in to other cmdlets to manipulate the result set. In particular, get to grips with where-object, select-object and foreach-object (and bear in mind these are sometimes used via their \"alias\" which are ?, select and % respectively) \n- Get-Member is incredibly helpful, as it shows you all the properties and methods for an object (and can help you to understand what type of object you are handling). To use get-member just pipe an object to it. For example:\n```\nPS C:\\Users\\mwragg> get-service | get-member\n\n\n   TypeName: System.ServiceProcess.ServiceController\n\nName                      MemberType    Definition\n----                      ----------    ----------\nName                      AliasProperty Name = ServiceName\nRequiredServices          AliasProperty RequiredServices = ServicesDependedOn\nDisposed                  Event         System.EventHandler Disposed(System.Object, System.EventArgs)\nClose                     Method        void Close()\nContinue                  Method        void Continue()\nCreateObjRef              Method        System.Runtime.Remoting.ObjRef CreateObjRef(type requestedType)\nDispose                   Method        void Dispose(), void IDisposable.Dispose()\nEquals                    Method        bool Equals(System.Object obj)\nExecuteCommand            Method        void ExecuteCommand(int command)\nGetHashCode               Method        int GetHashCode()\nGetLifetimeService        Method        System.Object GetLifetimeService()\nGetType                   Method        type GetType()\nInitializeLifetimeService Method        System.Object InitializeLifetimeService()\nPause                     Method        void Pause()\nRefresh                   Method        void Refresh()\nStart                     Method        void Start(), void Start(string[] args)\nStop                      Method        void Stop()\nWaitForStatus             Method        void WaitForStatus(System.ServiceProcess.ServiceControllerStatus desiredStat...\nCanPauseAndContinue       Property      bool CanPauseAndContinue {get;}\nCanShutdown               Property      bool CanShutdown {get;}\nCanStop                   Property      bool CanStop {get;}\nContainer                 Property      System.ComponentModel.IContainer Container {get;}\nDependentServices         Property      System.ServiceProcess.ServiceController[] DependentServices {get;}\nDisplayName               Property      string DisplayName {get;set;}\nMachineName               Property      string MachineName {get;set;}\nServiceHandle             Property      System.Runtime.InteropServices.SafeHandle ServiceHandle {get;}\nServiceName               Property      string ServiceName {get;set;}\nServicesDependedOn        Property      System.ServiceProcess.ServiceController[] ServicesDependedOn {get;}\nServiceType               Property      System.ServiceProcess.ServiceType ServiceType {get;}\nSite                      Property      System.ComponentModel.ISite Site {get;set;}\nStatus                    Property      System.ServiceProcess.ServiceControllerStatus Status {get;}\nToString                  ScriptMethod  System.Object ToString();\n```\n-  If you have a complex command that is being piped between multiple cmdlets, step backwards (removing parts of the pipeline) so you can see that the output is as you expect at each stage. For example:\n\n```\nPS C:\\Users\\mwragg> get-service | where-object {$_.displayname -like \"Windows*\"} | select-object status | where-object {\n$_.status -eq \"Running\"} | measure-object | select count\n\nCount\n-----\n   11\n\n\nPS C:\\Users\\mwragg> get-service | where-object {$_.displayname -like \"Windows*\"} | select-object status | where-object {\n$_.status -eq \"Running\"}\n\n Status\n ------\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\nRunning\n\n\nPS C:\\Users\\mwragg> get-service | where-object {$_.displayname -like \"Windows*\"}\n\nStatus   Name               DisplayName\n------   ----               -----------\nRunning  AudioEndpointBu... Windows Audio Endpoint Builder\nRunning  Audiosrv           Windows Audio\nRunning  EventLog           Windows Event Log\nRunning  FontCache          Windows Font Cache Service\nRunning  FontCache3.0.0.0   Windows Presentation Foundation Fon...\nStopped  icssvc             Windows Mobile Hotspot Service\nStopped  LicenseManager     Windows License Manager Service\nRunning  MpsSvc             Windows Firewall\nStopped  msiserver          Windows Installer\nStopped  SDRSVC             Windows Backup\nStopped  stisvc             Windows Image Acquisition (WIA)\nStopped  TrustedInstaller   Windows Modules Installer\nRunning  W32Time            Windows Time\nStopped  WbioSrvc           Windows Biometric Service\nRunning  Wcmsvc             Windows Connection Manager\nStopped  wcncsvc            Windows Connect Now - Config Registrar\nStopped  WcsPlugInService   Windows Color System\nStopped  WdNisSvc           Windows Defender Network Inspection...\nStopped  Wecsvc             Windows Event Collector\nStopped  WEPHOSTSVC         Windows Encryption Provider Host Se...\nStopped  WerSvc             Windows Error Reporting Service\nStopped  WinDefend          Windows Defender Service\nRunning  Winmgmt            Windows Management Instrumentation\nStopped  WinRM              Windows Remote Management (WS-Manag...\nStopped  WMPNetworkSvc      Windows Media Player Network Sharin...\nStopped  WpnService         Windows Push Notifications Service\nRunning  WSearch            Windows Search\nStopped  WSService          Windows Store Service (WSService)\nStopped  wuauserv           Windows Update\nRunning  wudfsvc            Windows Driver Foundation - User-mo...\n```\n\n- In the ISE, press CTRL+J to load templates for various scripting constructs such as cmdlets, loops, if else, try catch blocks and more. Note that it will drop the template in to whatever script/window you have open and wherever the cursor currently is.\n\n# Further Topics\n\n[Powershell Cheat Sheet](http://ramblingcookiemonster.github.io/images/Cheat-Sheets/powershell-cheat-sheet.pdf) (PDF)\n\n> This is a more detailed Cheat Sheet similar to the Basic Cheat Sheet above that provides more detailed/advanced conventions.\n\nWorth printing and keeping nearby when you're starting to use more advanced concepts.\n\n[An introduction to Powershell Modules](https://www.simple-talk.com/sysadmin/powershell/an-introduction-to-powershell-modules/) (Blog Post)\n\n> For PowerShell to provide specialised scripting, especially for administering server technologies, it can have the range of Cmdlets available to it extended by means of Snapins. From version 2 there is an easier and better method of extending PowerShell: the Module. These can be distributed with the application to be administered, and a wide range of Cmdlets are now available to the PowerShell user. PowerShell has grown up.\n\nModules extend Powershell with additional cmdlets and you can create your own to package together a set of functions or cmdlets you've written.\nTip: In the Powershell ISE, press CTRL+J and select cmdlet (advanced function) to load a template for creating a cmdlet that you could save as a module.\n\n[Advanced Tools & Scripting with PowerShell 3.0 Jump Start](https://mva.microsoft.com/en-US/training-courses/advanced-tools-scripting-with-powershell-3-0-jump-start-8277#fbid=wdBipBJLtCB) (Video Series)\n\n> Take this advanced PowerShell scripting course to find out how to turn your real time management and automation scripts into useful reusable tools and cmdlets. You’ll learn the best patterns and practices for building PowerShell scripts and maintaining tools, and you’ll pick up some special tips and tricks along the way from the architect and inventor of PowerShell, Distinguished Engineer Jeffrey Snover, and IT Pro, Jason Helmick.\n\nNot one to start with, but once you've mastered the basics this will help guide you to developing more advanced tools and scripts with Powershell.\n\n[Active Directory Powershell Quick Reference](http://www.jonathanmedd.net/wp-content/uploads/2009/10/ADPowerShell_QuickReference.pdf) (PDF)\n\n> A Cheat Sheet for working with the Active Directory module for Powershell that covers various constructs.\n\nVery useful guide for when you need to work with the Active Directory cmdlets (import-module activedirectory on a Domain Controller, or having installed AD tools).\n\n[Powershell Slack Community](https://powershell.slack.com/) (Live Chat)\n\n> Slack is a chat tool similar to Hipchat. Powershell.slack.com has an active community of Powershell experts that you can discuss things with and also bridges various other IRC-based Powershell channels.\t\n\nAn opportunity to ask questions to a community and get a (likely) instantaneous response.\nNote you need [sign up](http://slack.poshcode.org/) first, but it's automated.",
                        "html": "<p>This post is a list of resources and tips to help anyone new to Windows Powershell in getting started with the language. If Powershell is completely new to you, I recommend you review all of the listed resources. If you find other great resources along the way, please feel free to comment below and i'll contribute them to this list.</p>\n\n<h1 id=\"gettingstarted\">Getting Started</h1>\n\n<p><a href=\"https://mva.microsoft.com/en-us/training-courses/getting-started-with-powershell-3-0-jump-start-8276?l=r54IrOWy_2304984382\">Getting Started with PowerShell 3.0 Jump Start</a>\n(video series)</p>\n\n<blockquote>\n  <p>This Jump Start Microsoft PowerShell course is designed to teach busy IT professionals, admins, and help desk persons about how to use PowerShell to improve management capabilities, automate redundant tasks, and manage the environment in scale. Through this PowerShell tutorial, learn how PowerShell works – and how to make PowerShell work for you – from experts Jeffrey Snover, the inventor of PowerShell, and Jason Helmick, Senior Technologist at Concentrated Technology.</p>\n</blockquote>\n\n<p>Although this was released around Powershell 3 (and we're currently on version 5) this is still a very relevant introduction to Powershell, as each new version of Powershell builds on the last.</p>\n\n<p><a href=\"http://windowsitpro.com/blog/my-12-powershell-best-practices\">My 12 PowerShell Best Practices</a> (Blog Series)</p>\n\n<blockquote>\n  <p>A 12-part series of posts entitled \"What To Do / Not To Do in PowerShell.\" These summarize my 12 major best practices for the shell, many of which are also enthusiastically endorsed by the broader PowerShell community.</p>\n</blockquote>\n\n<p>This promotes some essential best practices that will make your scripts more robust and easier for others to read/understand, so it's a good idea to develop these habits early.</p>\n\n<p><a href=\"http://ramblingcookiemonster.github.io/images/Cheat-Sheets/powershell-basic-cheat-sheet2.pdf\">Powershell Basic Cheat Sheet</a> (PDF)</p>\n\n<blockquote>\n  <p>PowerShell is a task based command line shell and scripting language. To run it, click Start, type PowerShell, run PowerShell ISE or PowerShell as Administrator. Commands are written in verb-noun form, and named parameters start with a dash.</p>\n</blockquote>\n\n<p>This is worth printing and keeping nearby when coding as a handy reference guide to the basic constructs and conventions.</p>\n\n<p><a href=\"How to Install Windows 2016 TP5\">How to Install Windows 2016 TP5</a> (Blog Post)</p>\n\n<blockquote>\n  <p>This blog post (by me :) ) takes you through how to road test Windows Server 2016 Technical Preview 5 using VMWare Player as a desktop hypervisor. Note that you'll need to do this on a machine with a fair amount of spare RAM.\n  While this isn't strictly a Powershell resource, having a server VM to play around with is useful when learning Powershell.</p>\n</blockquote>\n\n<p>This earlier blog post details how to install the trial version of the latest Windows Operating system where you can play around with the latest version of Powershell on the latest OS. You might want to spin up multiple machines if you want to experiment with using Powershell to communicate between servers.</p>\n\n<p><a href=\"https://powershell.org/podcast/\">Powerscripting Podcast</a> (Podcast)</p>\n\n<blockquote>\n  <p>Shortly after PowerShell's introduction to the world, Jon Walz and Hal Rottenberg launched the PowerScripting Podcast, a weekly show featuring expert guest speakers, fantastic behind-the-scenes information, members of the PowerShell community, and more.</p>\n</blockquote>\n\n<p>The more recent shows tend to be interviews on developments in Powershell and the community, but go back to episode 1 and the first 20 or so episodes do a good job of explaining and exploring the basics and this is a good way to use dead time such as a commute to immerse yourself in Powershell.</p>\n\n<p><a href=\"https://www.amazon.co.uk/Learn-Windows-PowerShell-Month-Lunches/dp/1617290211\">Learn Powershell in a Month of Lunches</a> (Book / eBook)</p>\n\n<blockquote>\n  <p><em>Learn Windows PowerShell in a Month of Lunches</em> is an innovative tutorial designed for busy administrators. Author Don Jones has taught thousands of administrators to use PowerShell, and now he brings his years of training techniques to a concise, easy-to-follow book. Just set aside one hour a day—lunchtime would be perfect—for an entire month, and readers will be automating administrative tasks faster than they ever thought possible.</p>\n</blockquote>\n\n<p>I have not personally read it, but this book is recommended almost everywhere as a great way to learn Powershell.</p>\n\n<h1 id=\"generaltips\">General Tips</h1>\n\n<ul>\n<li>Google is your friend, but beware that as parameters start with a hyphen you might need to enclose some of your search with speechmarks as using a hyphen in a Google search means to exclude that term by default. E.g: instead of get-aduser -properties do get-aduser \"-properties\"</li>\n<li>StackExchange is your other friend and someone has probably already asked your question: <a href=\"http://stackoverflow.com/questions/tagged/powershell\">http://stackoverflow.com/questions/tagged/powershell</a></li>\n<li>The console and ISE both support tab completion, so you can type part of a cmdlet name then tab through the options. Also the ISE has intellisense, it will show tooltips to help you complete commands and it will red underline coding errors.</li>\n<li>Get-Help is sometimes helpful (although Technet or ss64 are likely easier to read), e.g get-help get-service. Also try using it with any of these parameters: -examples -detailed or -full.</li>\n<li>Powershell is all about the pipeline. Cmdlets return collections of results known as \"objects\" that can be passed forward down the pipeline in to other cmdlets to manipulate the result set. In particular, get to grips with where-object, select-object and foreach-object (and bear in mind these are sometimes used via their \"alias\" which are ?, select and % respectively) </li>\n<li>Get-Member is incredibly helpful, as it shows you all the properties and methods for an object (and can help you to understand what type of object you are handling). To use get-member just pipe an object to it. For example:</li>\n</ul>\n\n<pre><code>PS C:\\Users\\mwragg&gt; get-service | get-member\n\n\n   TypeName: System.ServiceProcess.ServiceController\n\nName                      MemberType    Definition  \n----                      ----------    ----------\nName                      AliasProperty Name = ServiceName  \nRequiredServices          AliasProperty RequiredServices = ServicesDependedOn  \nDisposed                  Event         System.EventHandler Disposed(System.Object, System.EventArgs)  \nClose                     Method        void Close()  \nContinue                  Method        void Continue()  \nCreateObjRef              Method        System.Runtime.Remoting.ObjRef CreateObjRef(type requestedType)  \nDispose                   Method        void Dispose(), void IDisposable.Dispose()  \nEquals                    Method        bool Equals(System.Object obj)  \nExecuteCommand            Method        void ExecuteCommand(int command)  \nGetHashCode               Method        int GetHashCode()  \nGetLifetimeService        Method        System.Object GetLifetimeService()  \nGetType                   Method        type GetType()  \nInitializeLifetimeService Method        System.Object InitializeLifetimeService()  \nPause                     Method        void Pause()  \nRefresh                   Method        void Refresh()  \nStart                     Method        void Start(), void Start(string[] args)  \nStop                      Method        void Stop()  \nWaitForStatus             Method        void WaitForStatus(System.ServiceProcess.ServiceControllerStatus desiredStat...  \nCanPauseAndContinue       Property      bool CanPauseAndContinue {get;}  \nCanShutdown               Property      bool CanShutdown {get;}  \nCanStop                   Property      bool CanStop {get;}  \nContainer                 Property      System.ComponentModel.IContainer Container {get;}  \nDependentServices         Property      System.ServiceProcess.ServiceController[] DependentServices {get;}  \nDisplayName               Property      string DisplayName {get;set;}  \nMachineName               Property      string MachineName {get;set;}  \nServiceHandle             Property      System.Runtime.InteropServices.SafeHandle ServiceHandle {get;}  \nServiceName               Property      string ServiceName {get;set;}  \nServicesDependedOn        Property      System.ServiceProcess.ServiceController[] ServicesDependedOn {get;}  \nServiceType               Property      System.ServiceProcess.ServiceType ServiceType {get;}  \nSite                      Property      System.ComponentModel.ISite Site {get;set;}  \nStatus                    Property      System.ServiceProcess.ServiceControllerStatus Status {get;}  \nToString                  ScriptMethod  System.Object ToString();  \n</code></pre>\n\n<ul>\n<li>If you have a complex command that is being piped between multiple cmdlets, step backwards (removing parts of the pipeline) so you can see that the output is as you expect at each stage. For example:</li>\n</ul>\n\n<pre><code>PS C:\\Users\\mwragg&gt; get-service | where-object {$_.displayname -like \"Windows*\"} | select-object status | where-object {  \n$_.status -eq \"Running\"} | measure-object | select count\n\nCount  \n-----\n   11\n\n\nPS C:\\Users\\mwragg&gt; get-service | where-object {$_.displayname -like \"Windows*\"} | select-object status | where-object {  \n$_.status -eq \"Running\"}\n\n Status\n ------\nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning  \nRunning\n\n\nPS C:\\Users\\mwragg&gt; get-service | where-object {$_.displayname -like \"Windows*\"}\n\nStatus   Name               DisplayName  \n------   ----               -----------\nRunning  AudioEndpointBu... Windows Audio Endpoint Builder  \nRunning  Audiosrv           Windows Audio  \nRunning  EventLog           Windows Event Log  \nRunning  FontCache          Windows Font Cache Service  \nRunning  FontCache3.0.0.0   Windows Presentation Foundation Fon...  \nStopped  icssvc             Windows Mobile Hotspot Service  \nStopped  LicenseManager     Windows License Manager Service  \nRunning  MpsSvc             Windows Firewall  \nStopped  msiserver          Windows Installer  \nStopped  SDRSVC             Windows Backup  \nStopped  stisvc             Windows Image Acquisition (WIA)  \nStopped  TrustedInstaller   Windows Modules Installer  \nRunning  W32Time            Windows Time  \nStopped  WbioSrvc           Windows Biometric Service  \nRunning  Wcmsvc             Windows Connection Manager  \nStopped  wcncsvc            Windows Connect Now - Config Registrar  \nStopped  WcsPlugInService   Windows Color System  \nStopped  WdNisSvc           Windows Defender Network Inspection...  \nStopped  Wecsvc             Windows Event Collector  \nStopped  WEPHOSTSVC         Windows Encryption Provider Host Se...  \nStopped  WerSvc             Windows Error Reporting Service  \nStopped  WinDefend          Windows Defender Service  \nRunning  Winmgmt            Windows Management Instrumentation  \nStopped  WinRM              Windows Remote Management (WS-Manag...  \nStopped  WMPNetworkSvc      Windows Media Player Network Sharin...  \nStopped  WpnService         Windows Push Notifications Service  \nRunning  WSearch            Windows Search  \nStopped  WSService          Windows Store Service (WSService)  \nStopped  wuauserv           Windows Update  \nRunning  wudfsvc            Windows Driver Foundation - User-mo...  \n</code></pre>\n\n<ul>\n<li>In the ISE, press CTRL+J to load templates for various scripting constructs such as cmdlets, loops, if else, try catch blocks and more. Note that it will drop the template in to whatever script/window you have open and wherever the cursor currently is.</li>\n</ul>\n\n<h1 id=\"furthertopics\">Further Topics</h1>\n\n<p><a href=\"http://ramblingcookiemonster.github.io/images/Cheat-Sheets/powershell-cheat-sheet.pdf\">Powershell Cheat Sheet</a> (PDF)</p>\n\n<blockquote>\n  <p>This is a more detailed Cheat Sheet similar to the Basic Cheat Sheet above that provides more detailed/advanced conventions.</p>\n</blockquote>\n\n<p>Worth printing and keeping nearby when you're starting to use more advanced concepts.</p>\n\n<p><a href=\"https://www.simple-talk.com/sysadmin/powershell/an-introduction-to-powershell-modules/\">An introduction to Powershell Modules</a> (Blog Post)</p>\n\n<blockquote>\n  <p>For PowerShell to provide specialised scripting, especially for administering server technologies, it can have the range of Cmdlets available to it extended by means of Snapins. From version 2 there is an easier and better method of extending PowerShell: the Module. These can be distributed with the application to be administered, and a wide range of Cmdlets are now available to the PowerShell user. PowerShell has grown up.</p>\n</blockquote>\n\n<p>Modules extend Powershell with additional cmdlets and you can create your own to package together a set of functions or cmdlets you've written. <br />\nTip: In the Powershell ISE, press CTRL+J and select cmdlet (advanced function) to load a template for creating a cmdlet that you could save as a module.</p>\n\n<p><a href=\"https://mva.microsoft.com/en-US/training-courses/advanced-tools-scripting-with-powershell-3-0-jump-start-8277#fbid=wdBipBJLtCB\">Advanced Tools &amp; Scripting with PowerShell 3.0 Jump Start</a> (Video Series)</p>\n\n<blockquote>\n  <p>Take this advanced PowerShell scripting course to find out how to turn your real time management and automation scripts into useful reusable tools and cmdlets. You’ll learn the best patterns and practices for building PowerShell scripts and maintaining tools, and you’ll pick up some special tips and tricks along the way from the architect and inventor of PowerShell, Distinguished Engineer Jeffrey Snover, and IT Pro, Jason Helmick.</p>\n</blockquote>\n\n<p>Not one to start with, but once you've mastered the basics this will help guide you to developing more advanced tools and scripts with Powershell.</p>\n\n<p><a href=\"http://www.jonathanmedd.net/wp-content/uploads/2009/10/ADPowerShell_QuickReference.pdf\">Active Directory Powershell Quick Reference</a> (PDF)</p>\n\n<blockquote>\n  <p>A Cheat Sheet for working with the Active Directory module for Powershell that covers various constructs.</p>\n</blockquote>\n\n<p>Very useful guide for when you need to work with the Active Directory cmdlets (import-module activedirectory on a Domain Controller, or having installed AD tools).</p>\n\n<p><a href=\"https://powershell.slack.com/\">Powershell Slack Community</a> (Live Chat)</p>\n\n<blockquote>\n  <p>Slack is a chat tool similar to Hipchat. Powershell.slack.com has an active community of Powershell experts that you can discuss things with and also bridges various other IRC-based Powershell channels.    </p>\n</blockquote>\n\n<p>An opportunity to ask questions to a community and get a (likely) instantaneous response. <br />\nNote you need <a href=\"http://slack.poshcode.org/\">sign up</a> first, but it's automated.</p>",
                        "image": "/content/images/2016/09/getting-started-crop.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-09-27 08:48:58",
                        "created_by": 1,
                        "updated_at": "2016-09-28 10:29:45",
                        "updated_by": 1,
                        "published_at": "2016-09-27 09:21:39",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 36,
                        "uuid": "df372c86-10fc-4359-bfc0-d0d1952ef7c1",
                        "title": "Getting started with Pester (for operational testing)",
                        "slug": "getting-started-with-pester-for-operational-testing",
                        "markdown": "This is a guide for anyone new to the Pester testing framework module for Powershell who would like to learn how Pester works in order to maintain or create Pester test scripts that are used for operational testing.\n\n# What is Pester?\n\nPester is an open source testing framework for Powershell.\n> Pester provides a framework for running unit tests to execute and validate PowerShell commands from within PowerShell. Pester consists of a simple set of functions that expose a testing domain-specific language (DSL) for isolating, running, evaluating and reporting the results of PowerShell commands.\n>\n> *-- https://github.com/pester/Pester*\n\n# Why use Pester for operational testing?\n\nWhile the purpose of Pester is primarily to perform unit testing, I've leveraged Pester largely for the alternative purpose of operational and infrastructure testing. Pester is ideal for this purpose because:\n\n- It's Powershell at its core, so you can leverage the full functionality of Powershell to perform your testing\n- It creates a consistent framework with which to write and perform tests\n- It gives a clean output of the test result\n- It can (optionally) be included in a CI pipeline, where as part of a larger automation chain Pester is kicked off (e.g after you check in a change to source control) and the result of the tests kick off subsequent activity (such as clean up of test infrastructure, notifications of the result or pushing the changes in to Production if the tests passed)\n\n# Installing Pester\n\nIf you are running Windows 10 or Windows Server 2016 you already have it installed (congratulations!). You can doublecheck you have it by executing\n\n```\nget-command invoke-pester\n\nCommandType     Name                                               Version    Source\n-----------     ----                                               -------    ------\nFunction        Invoke-Pester                                      3.3.5      Pester\n```\n\nIf not, you simply need to download and load the module as follows:\n\n- Go here https://github.com/pester/Pester\n- Click \"Clone or download\" > Download Zip\n- Open the zip and extract it somewhere (your module directory might be ideal: C:\\Users\\\\{you}\\Documents\\WindowsPowerShell\\Modules)\n- Execute `import-module Pester`\n\n# Writing a Pester test\n\nTo create a Pester test, do the following:\n\n- Open the Powershell ISE (or another Powershell editor of your choice).\n- Save a new file as myfirst.tests.ps1 (anything named .tests.ps1 will be picked up and executed by Pester by default, so using this naming convention makes your tests easier to execute)\n\nTests are composed of the following constructs:\n\n- First you need a [\"Describe\" block](https://github.com/pester/Pester/wiki/Describe) – This just gives the test or set of tests a name that is output when the tests run to help the user understand what kind of test/s are being performed.\n- You can optionally include a [\"Context\" block](https://github.com/pester/Pester/wiki/Context) – This is another way to group up a set of tests to help give the user context.\n- Write your test with [\"It\"](https://github.com/pester/Pester/wiki/It) and [\"Should\"](https://github.com/pester/Pester/wiki/Should) – This is where the magic happens, combining these two commands tells Pester what you expected (the It), what Powershell should perform to validate that assertion and then what you expect the result to be (the Should). You do this by piping between these three components.\n\nHere's a simple example to get you started:\n\n```\nDescribe  'Process checks' {\n     \n    Context 'Checking essential Windows processes are running' {\n \n        It 'winlogon.exe is running' {\n            get-process -Name 'winlogon' | Should be $true\n        }\n         \n    }\n}\n```\n\nHere's the result when that is run:\n\n![](/content/images/2016/09/pester-test-pass.png)\n\nIn the output above you can see:\n\n- The describe block we defined\n- The context block we defined\n- The winlogon.exe test (with the text output we defined), which is green and has a + symbol because it passed. The 649ms part is how long the test took to run.\n\nHere's our example but now including a failing test (true could never be false unless it was opposite day):\n\n```\nDescribe  'My example checks' {\n     \n    Context 'Checking essential Windows processes are running' {\n \n        It 'winlogon.exe is running' {\n            get-process -Name 'winlogon' | Should be $true\n        }\n        It 'It should be opposite day' {\n            $true | Should be $false\n        }\n         \n    }\n}\n```\nHere's the result:\n\n![](/content/images/2016/09/pester-test-fail-opposite-day.png)\n\n- The opposite day test is red because it failed and the test has a - symbol. Again it shows the time it took for test to run, then after the test it includes error output to indicate exactly why it failed (it expected a result of \"false\" but we sent it \"true\").\n\nOther assertions using \"Should\"\nSo far in the examples above we've performed a simple $true or $false check using \"Should Be\". We could also have reversed our logic and stated \"Should Not Be\" and sometimes it is a good idea to test for both positive and negative scenarios. You can also do other evaluations with Should including:\n\n- Should Be\n- Should BeExactly\n- Should BeGreaterThan\n- Should BeLessThan\n- Should BeLike\n- Should BeOfType\n- Should Exist\n- Should Contain\n- Should ContainExactly\n- Should Match\n- Should MatchExactly\n- Should Throw\n- Should BeNullOrEmpty\n\nFor a full description of how each of these works, see the official documentation: https://github.com/pester/Pester/wiki/Should\n\nHere's an example of using \"Exist\" and \"Not Exist\" to check two paths. The \"Exist\" statement performs a test-path, so while we're piping a string here, the \"Exist\" test takes that string and uses it with test-path to validate if it returns an object.\n\n```\nDescribe 'Operating System check' {\n \n    It 'C:\\Windows folder exists' {\n       'C:\\Windows' | Should Exist\n    }\n \n    It 'C:\\Linux folder does not exist' {\n       'C:\\Linux' | Should Not Exist\n    }\n \n}\n```\n\nHere's the result:\n\n![](/content/images/2016/09/pester-foldercheck.png)\n\n# Creating dynamic tests by incorporating other Powershell syntax\n\nPester scripts are still just Powershell scripts at the end of the day. That means that you can incorporate Powershell anywhere during the script. You might for example have a block of Powershell at the beginning of the script which gathers local settings and configuration that is then used as part of the test evaluations.\n\nBut you can also use Powershell syntax such as loops and if blocks (etc.) to make your tests more dynamic.\n\nHere's an example to test whether today is each day of the week. We could do this by writing seven individual \"It\" blocks for each weekday, or instead we could use an array and a ForEach-Object loop like this:\n\n```\nDescribe  'Day of the week check' {\n    \n    $WeekDays = \"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"\n \n    $WeekDays | ForEach-Object {\n \n        It \"Today should be $_\" {\n            (get-date).DayOfWeek | Should be $_\n        }\n    }    \n}\n```\n\nHere's the result:\n\n![](/content/images/2016/09/pester-dayofweek.png)\n\nHere's a similar example which gets every service that is named \"Windows\" and validates if it is in a Running state:\n\n```\nDescribe  'Windows service check' {\n     \n    Get-Service -DisplayName \"Windows*\" | ForEach-Object {\n \n        It \"The $($_.DisplayName) service should be running\" {\n            $_.Status | Should be 'Running'\n        }\n    }    \n}\n```\n\nHere's the result:\n\n![](/content/images/2016/09/pester-windowsservice.png)\n\n# Running Pester tests\n\nPer my examples above, Pester scripts can just be run by executing the Powershell scripts in the same way as you would any other.\n\nAlternatively (and preferably) you can use `invoke-pester`.\n\nRunning `Invoke-Pester` will run any tests named *.tests.ps1 in the current directory (and possibly sub-directories). The tests will run and print their results to the console as above, but you will also get this summary at the end with counts of the test results:\n\n![](/content/images/2016/09/pester-results.png)\n\nInvoke-Pester has several parameters that are best described in the official documentation here: https://github.com/pester/Pester/wiki/Invoke-Pester\n\nOne way in which you might use Invoke-Pester as part of a pipeline would be to use the -PassThru parameter to capture the results as an object.\n\nHere's an example that runs Pester and returns the results in to $Results:\n\n```\n$Results = Invoke-Pester -PassThru\n```\nThat object has the following properties:\n```\n$Results | Get-Member\n\n  TypeName: Selected.System.Management.Automation.PSCustomObject\n\nName             MemberType   Definition                               \n----             ----------   ----------                               \nEquals           Method       bool Equals(System.Object obj)           \nGetHashCode      Method       int GetHashCode()                        \nGetType          Method       type GetType()                           \nToString         Method       string ToString()                        \nExcludeTagFilter NoteProperty string[] ExcludeTagFilter=System.String[]\nFailedCount      NoteProperty int FailedCount=26                       \nPassedCount      NoteProperty int PassedCount=16                       \nPendingCount     NoteProperty int PendingCount=0                       \nSkippedCount     NoteProperty int SkippedCount=0                       \nTagFilter        NoteProperty string[] TagFilter=System.String[]       \nTestNameFilter   NoteProperty object TestNameFilter=null               \nTestResult       NoteProperty Object[] TestResult=System.Object[]      \nTime             NoteProperty timespan Time=00:00:00.8949340           \nTotalCount       NoteProperty int TotalCount=42                     \n```\nWhich I could use to drive further actions, for example `If ($Results.FailedCount -gt 0) { Do some thing }`.\n\n# Further reading\n\nThe official documentation is likely your best reference for further reading: https://github.com/pester/Pester/wiki/Pester",
                        "html": "<p>This is a guide for anyone new to the Pester testing framework module for Powershell who would like to learn how Pester works in order to maintain or create Pester test scripts that are used for operational testing.</p>\n\n<h1 id=\"whatispester\">What is Pester?</h1>\n\n<p>Pester is an open source testing framework for Powershell.  </p>\n\n<blockquote>\n  <p>Pester provides a framework for running unit tests to execute and validate PowerShell commands from within PowerShell. Pester consists of a simple set of functions that expose a testing domain-specific language (DSL) for isolating, running, evaluating and reporting the results of PowerShell commands.</p>\n  \n  <p><em>-- <a href=\"https://github.com/pester/Pester\">https://github.com/pester/Pester</a></em></p>\n</blockquote>\n\n<h1 id=\"whyusepesterforoperationaltesting\">Why use Pester for operational testing?</h1>\n\n<p>While the purpose of Pester is primarily to perform unit testing, I've leveraged Pester largely for the alternative purpose of operational and infrastructure testing. Pester is ideal for this purpose because:</p>\n\n<ul>\n<li>It's Powershell at its core, so you can leverage the full functionality of Powershell to perform your testing</li>\n<li>It creates a consistent framework with which to write and perform tests</li>\n<li>It gives a clean output of the test result</li>\n<li>It can (optionally) be included in a CI pipeline, where as part of a larger automation chain Pester is kicked off (e.g after you check in a change to source control) and the result of the tests kick off subsequent activity (such as clean up of test infrastructure, notifications of the result or pushing the changes in to Production if the tests passed)</li>\n</ul>\n\n<h1 id=\"installingpester\">Installing Pester</h1>\n\n<p>If you are running Windows 10 or Windows Server 2016 you already have it installed (congratulations!). You can doublecheck you have it by executing</p>\n\n<pre><code>get-command invoke-pester\n\nCommandType     Name                                               Version    Source  \n-----------     ----                                               -------    ------\nFunction        Invoke-Pester                                      3.3.5      Pester  \n</code></pre>\n\n<p>If not, you simply need to download and load the module as follows:</p>\n\n<ul>\n<li>Go here <a href=\"https://github.com/pester/Pester\">https://github.com/pester/Pester</a></li>\n<li>Click \"Clone or download\" > Download Zip</li>\n<li>Open the zip and extract it somewhere (your module directory might be ideal: C:\\Users\\{you}\\Documents\\WindowsPowerShell\\Modules)</li>\n<li>Execute <code>import-module Pester</code></li>\n</ul>\n\n<h1 id=\"writingapestertest\">Writing a Pester test</h1>\n\n<p>To create a Pester test, do the following:</p>\n\n<ul>\n<li>Open the Powershell ISE (or another Powershell editor of your choice).</li>\n<li>Save a new file as myfirst.tests.ps1 (anything named .tests.ps1 will be picked up and executed by Pester by default, so using this naming convention makes your tests easier to execute)</li>\n</ul>\n\n<p>Tests are composed of the following constructs:</p>\n\n<ul>\n<li>First you need a <a href=\"https://github.com/pester/Pester/wiki/Describe\">\"Describe\" block</a> – This just gives the test or set of tests a name that is output when the tests run to help the user understand what kind of test/s are being performed.</li>\n<li>You can optionally include a <a href=\"https://github.com/pester/Pester/wiki/Context\">\"Context\" block</a> – This is another way to group up a set of tests to help give the user context.</li>\n<li>Write your test with <a href=\"https://github.com/pester/Pester/wiki/It\">\"It\"</a> and <a href=\"https://github.com/pester/Pester/wiki/Should\">\"Should\"</a> – This is where the magic happens, combining these two commands tells Pester what you expected (the It), what Powershell should perform to validate that assertion and then what you expect the result to be (the Should). You do this by piping between these three components.</li>\n</ul>\n\n<p>Here's a simple example to get you started:</p>\n\n<pre><code>Describe  'Process checks' {\n\n    Context 'Checking essential Windows processes are running' {\n\n        It 'winlogon.exe is running' {\n            get-process -Name 'winlogon' | Should be $true\n        }\n\n    }\n}\n</code></pre>\n\n<p>Here's the result when that is run:</p>\n\n<p><img src=\"/content/images/2016/09/pester-test-pass.png\" alt=\"\" /></p>\n\n<p>In the output above you can see:</p>\n\n<ul>\n<li>The describe block we defined</li>\n<li>The context block we defined</li>\n<li>The winlogon.exe test (with the text output we defined), which is green and has a + symbol because it passed. The 649ms part is how long the test took to run.</li>\n</ul>\n\n<p>Here's our example but now including a failing test (true could never be false unless it was opposite day):</p>\n\n<pre><code>Describe  'My example checks' {\n\n    Context 'Checking essential Windows processes are running' {\n\n        It 'winlogon.exe is running' {\n            get-process -Name 'winlogon' | Should be $true\n        }\n        It 'It should be opposite day' {\n            $true | Should be $false\n        }\n\n    }\n}\n</code></pre>\n\n<p>Here's the result:</p>\n\n<p><img src=\"/content/images/2016/09/pester-test-fail-opposite-day.png\" alt=\"\" /></p>\n\n<ul>\n<li>The opposite day test is red because it failed and the test has a - symbol. Again it shows the time it took for test to run, then after the test it includes error output to indicate exactly why it failed (it expected a result of \"false\" but we sent it \"true\").</li>\n</ul>\n\n<p>Other assertions using \"Should\" <br />\nSo far in the examples above we've performed a simple $true or $false check using \"Should Be\". We could also have reversed our logic and stated \"Should Not Be\" and sometimes it is a good idea to test for both positive and negative scenarios. You can also do other evaluations with Should including:</p>\n\n<ul>\n<li>Should Be</li>\n<li>Should BeExactly</li>\n<li>Should BeGreaterThan</li>\n<li>Should BeLessThan</li>\n<li>Should BeLike</li>\n<li>Should BeOfType</li>\n<li>Should Exist</li>\n<li>Should Contain</li>\n<li>Should ContainExactly</li>\n<li>Should Match</li>\n<li>Should MatchExactly</li>\n<li>Should Throw</li>\n<li>Should BeNullOrEmpty</li>\n</ul>\n\n<p>For a full description of how each of these works, see the official documentation: <a href=\"https://github.com/pester/Pester/wiki/Should\">https://github.com/pester/Pester/wiki/Should</a></p>\n\n<p>Here's an example of using \"Exist\" and \"Not Exist\" to check two paths. The \"Exist\" statement performs a test-path, so while we're piping a string here, the \"Exist\" test takes that string and uses it with test-path to validate if it returns an object.</p>\n\n<pre><code>Describe 'Operating System check' {\n\n    It 'C:\\Windows folder exists' {\n       'C:\\Windows' | Should Exist\n    }\n\n    It 'C:\\Linux folder does not exist' {\n       'C:\\Linux' | Should Not Exist\n    }\n\n}\n</code></pre>\n\n<p>Here's the result:</p>\n\n<p><img src=\"/content/images/2016/09/pester-foldercheck.png\" alt=\"\" /></p>\n\n<h1 id=\"creatingdynamictestsbyincorporatingotherpowershellsyntax\">Creating dynamic tests by incorporating other Powershell syntax</h1>\n\n<p>Pester scripts are still just Powershell scripts at the end of the day. That means that you can incorporate Powershell anywhere during the script. You might for example have a block of Powershell at the beginning of the script which gathers local settings and configuration that is then used as part of the test evaluations.</p>\n\n<p>But you can also use Powershell syntax such as loops and if blocks (etc.) to make your tests more dynamic.</p>\n\n<p>Here's an example to test whether today is each day of the week. We could do this by writing seven individual \"It\" blocks for each weekday, or instead we could use an array and a ForEach-Object loop like this:</p>\n\n<pre><code>Describe  'Day of the week check' {\n\n    $WeekDays = \"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"\n\n    $WeekDays | ForEach-Object {\n\n        It \"Today should be $_\" {\n            (get-date).DayOfWeek | Should be $_\n        }\n    }    \n}\n</code></pre>\n\n<p>Here's the result:</p>\n\n<p><img src=\"/content/images/2016/09/pester-dayofweek.png\" alt=\"\" /></p>\n\n<p>Here's a similar example which gets every service that is named \"Windows\" and validates if it is in a Running state:</p>\n\n<pre><code>Describe  'Windows service check' {\n\n    Get-Service -DisplayName \"Windows*\" | ForEach-Object {\n\n        It \"The $($_.DisplayName) service should be running\" {\n            $_.Status | Should be 'Running'\n        }\n    }    \n}\n</code></pre>\n\n<p>Here's the result:</p>\n\n<p><img src=\"/content/images/2016/09/pester-windowsservice.png\" alt=\"\" /></p>\n\n<h1 id=\"runningpestertests\">Running Pester tests</h1>\n\n<p>Per my examples above, Pester scripts can just be run by executing the Powershell scripts in the same way as you would any other.</p>\n\n<p>Alternatively (and preferably) you can use <code>invoke-pester</code>.</p>\n\n<p>Running <code>Invoke-Pester</code> will run any tests named *.tests.ps1 in the current directory (and possibly sub-directories). The tests will run and print their results to the console as above, but you will also get this summary at the end with counts of the test results:</p>\n\n<p><img src=\"/content/images/2016/09/pester-results.png\" alt=\"\" /></p>\n\n<p>Invoke-Pester has several parameters that are best described in the official documentation here: <a href=\"https://github.com/pester/Pester/wiki/Invoke-Pester\">https://github.com/pester/Pester/wiki/Invoke-Pester</a></p>\n\n<p>One way in which you might use Invoke-Pester as part of a pipeline would be to use the -PassThru parameter to capture the results as an object.</p>\n\n<p>Here's an example that runs Pester and returns the results in to $Results:</p>\n\n<pre><code>$Results = Invoke-Pester -PassThru\n</code></pre>\n\n<p>That object has the following properties:  </p>\n\n<pre><code>$Results | Get-Member\n\n  TypeName: Selected.System.Management.Automation.PSCustomObject\n\nName             MemberType   Definition  \n----             ----------   ----------                               \nEquals           Method       bool Equals(System.Object obj)  \nGetHashCode      Method       int GetHashCode()  \nGetType          Method       type GetType()  \nToString         Method       string ToString()  \nExcludeTagFilter NoteProperty string[] ExcludeTagFilter=System.String[]  \nFailedCount      NoteProperty int FailedCount=26  \nPassedCount      NoteProperty int PassedCount=16  \nPendingCount     NoteProperty int PendingCount=0  \nSkippedCount     NoteProperty int SkippedCount=0  \nTagFilter        NoteProperty string[] TagFilter=System.String[]  \nTestNameFilter   NoteProperty object TestNameFilter=null  \nTestResult       NoteProperty Object[] TestResult=System.Object[]  \nTime             NoteProperty timespan Time=00:00:00.8949340  \nTotalCount       NoteProperty int TotalCount=42  \n</code></pre>\n\n<p>Which I could use to drive further actions, for example <code>If ($Results.FailedCount -gt 0) { Do some thing }</code>.</p>\n\n<h1 id=\"furtherreading\">Further reading</h1>\n\n<p>The official documentation is likely your best reference for further reading: <a href=\"https://github.com/pester/Pester/wiki/Pester\">https://github.com/pester/Pester/wiki/Pester</a></p>",
                        "image": "/content/images/2016/09/maze.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-09-27 14:48:40",
                        "created_by": 1,
                        "updated_at": "2016-10-16 07:06:13",
                        "updated_by": 1,
                        "published_at": "2016-09-28 10:39:20",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 37,
                        "uuid": "3b06eb0e-5545-4dc1-9d3b-7482a849d3f6",
                        "title": "MCSA Server 2012",
                        "slug": "mcsa-server-2012",
                        "markdown": "The MCSA course catalogue has been broken down into smaller training videos and the whole thing is available on YouTube.\n\n# Installing and Configuring Windows Server 2012\n(Prepares you for exam [70-410](https://www.microsoft.com/en-us/learning/exam-70-410.aspx))\n\nhttps://youtu.be/t9ng8mOysTc?list=PLKWqPrqG4yyRl8JGRCzbwr5riYScPYkjL\n\n# Administering Windows Server 2012\n(Prepares you for exam [70-411](https://www.microsoft.com/en-us/learning/exam-70-411.aspx))\n\nhttps://youtu.be/9ChgKPDqCVw?list=PLKWqPrqG4yyTb9yrfzDBBiq9hZCeO5ZyV\n\n# Configuring Advanced Windows Server 2012 Services\n(Prepares you for exam [70-412](https://www.microsoft.com/en-us/learning/exam-70-412.aspx))\n\nhttps://youtu.be/8IdnlueWCbU?list=PLKWqPrqG4yySe-VucNQ_MHWmQKkVJIeb-\n\n\n[MCSA Complete Study Guide on Amazon](https://www.amazon.co.uk/Windows-Server-Complete-Study-Guide/dp/111885991X/ref=sr_1_1?ie=UTF8&qid=1475582364&sr=8-1&keywords=mcsa+complete)\n\n\n# R2 training videos here\n\nhttps://www.youtube.com/channel/UCiLEaVJlVFB_DoeBQGWV63g/playlists",
                        "html": "<p>The MCSA course catalogue has been broken down into smaller training videos and the whole thing is available on YouTube.</p>\n\n<h1 id=\"installingandconfiguringwindowsserver2012\">Installing and Configuring Windows Server 2012</h1>\n\n<p>(Prepares you for exam <a href=\"https://www.microsoft.com/en-us/learning/exam-70-410.aspx\">70-410</a>)</p>\n\n<p><a href=\"https://youtu.be/t9ng8mOysTc?list=PLKWqPrqG4yyRl8JGRCzbwr5riYScPYkjL\">https://youtu.be/t9ng8mOysTc?list=PLKWqPrqG4yyRl8JGRCzbwr5riYScPYkjL</a></p>\n\n<h1 id=\"administeringwindowsserver2012\">Administering Windows Server 2012</h1>\n\n<p>(Prepares you for exam <a href=\"https://www.microsoft.com/en-us/learning/exam-70-411.aspx\">70-411</a>)</p>\n\n<p><a href=\"https://youtu.be/9ChgKPDqCVw?list=PLKWqPrqG4yyTb9yrfzDBBiq9hZCeO5ZyV\">https://youtu.be/9ChgKPDqCVw?list=PLKWqPrqG4yyTb9yrfzDBBiq9hZCeO5ZyV</a></p>\n\n<h1 id=\"configuringadvancedwindowsserver2012services\">Configuring Advanced Windows Server 2012 Services</h1>\n\n<p>(Prepares you for exam <a href=\"https://www.microsoft.com/en-us/learning/exam-70-412.aspx\">70-412</a>)</p>\n\n<p><a href=\"https://youtu.be/8IdnlueWCbU?list=PLKWqPrqG4yySe-VucNQ_MHWmQKkVJIeb-\">https://youtu.be/8IdnlueWCbU?list=PLKWqPrqG4yySe-VucNQ_MHWmQKkVJIeb-</a></p>\n\n<p><a href=\"https://www.amazon.co.uk/Windows-Server-Complete-Study-Guide/dp/111885991X/ref=sr_1_1?ie=UTF8&amp;qid=1475582364&amp;sr=8-1&amp;keywords=mcsa+complete\">MCSA Complete Study Guide on Amazon</a></p>\n\n<h1 id=\"r2trainingvideoshere\">R2 training videos here</h1>\n\n<p><a href=\"https://www.youtube.com/channel/UCiLEaVJlVFB_DoeBQGWV63g/playlists\">https://www.youtube.com/channel/UCiLEaVJlVFB_DoeBQGWV63g/playlists</a></p>",
                        "image": "/content/images/2016/10/mcsa-course-training.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-10-04 12:17:44",
                        "created_by": 1,
                        "updated_at": "2016-10-12 10:08:31",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 38,
                        "uuid": "66d8f48c-f09c-4ba4-9ff3-0ac983b9a1a4",
                        "title": "Using Powershell to find and clean up old files",
                        "slug": "using-powershell-to-clean-up-old-files",
                        "markdown": "I was previously responsible for supporting a number of file servers that have a large number of very small files archived to them. These files are occasionally referenced from their source systems but I had long suspected that there were a number of orphaned/redundant files on these servers (e.g where we had decommissioned the source system and not cleaned up the archive) and wanted Powershell to help verify this. \n\nI had three wishes:\n\n1. To identify which folders on the archive servers were orphans. Our source systems reference these via a DFS path, so its a safe bet that if there are no DFS paths with folder targets pointing to the files on the archive location, they are probably orphans.\n\n2. To know when a file was last written/modified in these orphan locations (as a second check that the parent folders were not still being written to directly, e.g outside of a DFS path).\n\n3. To know how large the orphaned folders were so that I could brag about how much storage saving I had made.\n\n![](/content/images/2016/10/Aladdin-Lamp.jpg)\n\n",
                        "html": "<p>I was previously responsible for supporting a number of file servers that have a large number of very small files archived to them. These files are occasionally referenced from their source systems but I had long suspected that there were a number of orphaned/redundant files on these servers (e.g where we had decommissioned the source system and not cleaned up the archive) and wanted Powershell to help verify this. </p>\n\n<p>I had three wishes:</p>\n\n<ol>\n<li><p>To identify which folders on the archive servers were orphans. Our source systems reference these via a DFS path, so its a safe bet that if there are no DFS paths with folder targets pointing to the files on the archive location, they are probably orphans.</p></li>\n<li><p>To know when a file was last written/modified in these orphan locations (as a second check that the parent folders were not still being written to directly, e.g outside of a DFS path).</p></li>\n<li><p>To know how large the orphaned folders were so that I could brag about how much storage saving I had made.</p></li>\n</ol>\n\n<p><img src=\"/content/images/2016/10/Aladdin-Lamp.jpg\" alt=\"\" /></p>",
                        "image": "/content/images/2016/10/file-system.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-10-06 09:53:41",
                        "created_by": 1,
                        "updated_at": "2016-11-10 09:22:34",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 39,
                        "uuid": "cf070a54-6053-4dbe-aff4-e0be59e42b74",
                        "title": "Operational Testing for Antivirus: Validating Symantec Endpoint Protection with Pester",
                        "slug": "operational-testing-for-antivirus-validating-symantec-endpoint-protection-with-pester",
                        "markdown": "This post contains an operational validation test for Symantec Endpoint Protection (SEP) using the [Pester testing framework module](https://github.com/pester/Pester) with Powershell. It performs a few basic checks to ensure SEP is running and healthy. It is intended as a starting point and could be developed further. Your individual requirements will likely vary depending on how you have SEP configured and deployed in your environment.\n\nThis test script came as a result of a training session I ran last week to encourage my team to continue to use Pester and Powershell to build on our set of automated operational tests. We don't make changes to SEP very often, but having a way to validate the product after changes as well as to check consistency across our environment seemed useful.\n\n> If you're not familiar with Pester for Operational Testing, have a look at my [getting started with Pester](http://wragg.io/getting-started-with-pester-for-operational-testing/) post.\n\nAt the conclusion of the training we each worked on a couple of items from a list of ideas of tests we could perform against our applications and infrastructure. I chose to tackle SEP.\n\nThe script has a fairly basic set of tests at the moment and largely validates the configuration by reviewing settings in the registry. At time of writing, the tests are as follows:\n\n- Checks the Symantec Endpoint Protection and Symantec Management Client Windows services are running.\n- Checks the smc.exe and ccsvchst.exe processes are running.\n- Checks that the SEP version is greater than the master version as defined in the param block at the top of the script (current latest version is 12.1.7).\n\n*-- I was really pleased with how easy it was to do this comparison in Powershell. It might need further testing, but I think as a string 12.1.7.x.x.x is considered \"greater than\" 12.1.7 so regardless of the installed minor version it was easy to validate it was a certain major version or newer without things needing to get too [complex](http://www.regular-expressions.info/).*\n\n- Checks that a scheduled scan occurred in the last 1 days.\n- Checks that the current latest virus definitions are no more than 3 days old.\n- Checks that the system is not currently considered infected.\n- Checks that the Spyware Protection feature is enabled.\n- Checks that the Virus Protection feature is enabled.\n- Checks that Firewall Protection feature is enabled.\n- Checks that the System Network Access feature is enabled.\n\n*-- Again, your feature set might vary from this, so you could modify the expected results of these tests to either be enabled or disabled per your own expectation.*\n\nHere's the script. To run ensure you have the [Pester](https://github.com/pester/Pester) module installed and then execute with `Invoke-Pester`:\n\n<script src=\"https://gist.github.com/markwragg/5904856087b73857756e5b5ac0250f5b.js\"></script>\n\nHere's how it looks:\n\n![](/content/images/2016/10/SEP-Pester-Tests.png)\n\nIf you'd like to develop this further, feel free to [Fork the above Gist](https://gist.github.com/markwragg/5904856087b73857756e5b5ac0250f5b#file-sep-tests-ps1).",
                        "html": "<p>This post contains an operational validation test for Symantec Endpoint Protection (SEP) using the <a href=\"https://github.com/pester/Pester\">Pester testing framework module</a> with Powershell. It performs a few basic checks to ensure SEP is running and healthy. It is intended as a starting point and could be developed further. Your individual requirements will likely vary depending on how you have SEP configured and deployed in your environment.</p>\n\n<p>This test script came as a result of a training session I ran last week to encourage my team to continue to use Pester and Powershell to build on our set of automated operational tests. We don't make changes to SEP very often, but having a way to validate the product after changes as well as to check consistency across our environment seemed useful.</p>\n\n<blockquote>\n  <p>If you're not familiar with Pester for Operational Testing, have a look at my <a href=\"http://wragg.io/getting-started-with-pester-for-operational-testing/\">getting started with Pester</a> post.</p>\n</blockquote>\n\n<p>At the conclusion of the training we each worked on a couple of items from a list of ideas of tests we could perform against our applications and infrastructure. I chose to tackle SEP.</p>\n\n<p>The script has a fairly basic set of tests at the moment and largely validates the configuration by reviewing settings in the registry. At time of writing, the tests are as follows:</p>\n\n<ul>\n<li>Checks the Symantec Endpoint Protection and Symantec Management Client Windows services are running.</li>\n<li>Checks the smc.exe and ccsvchst.exe processes are running.</li>\n<li>Checks that the SEP version is greater than the master version as defined in the param block at the top of the script (current latest version is 12.1.7).</li>\n</ul>\n\n<p><em>-- I was really pleased with how easy it was to do this comparison in Powershell. It might need further testing, but I think as a string 12.1.7.x.x.x is considered \"greater than\" 12.1.7 so regardless of the installed minor version it was easy to validate it was a certain major version or newer without things needing to get too <a href=\"http://www.regular-expressions.info/\">complex</a>.</em></p>\n\n<ul>\n<li>Checks that a scheduled scan occurred in the last 1 days.</li>\n<li>Checks that the current latest virus definitions are no more than 3 days old.</li>\n<li>Checks that the system is not currently considered infected.</li>\n<li>Checks that the Spyware Protection feature is enabled.</li>\n<li>Checks that the Virus Protection feature is enabled.</li>\n<li>Checks that Firewall Protection feature is enabled.</li>\n<li>Checks that the System Network Access feature is enabled.</li>\n</ul>\n\n<p><em>-- Again, your feature set might vary from this, so you could modify the expected results of these tests to either be enabled or disabled per your own expectation.</em></p>\n\n<p>Here's the script. To run ensure you have the <a href=\"https://github.com/pester/Pester\">Pester</a> module installed and then execute with <code>Invoke-Pester</code>:</p>\n\n<script src=\"https://gist.github.com/markwragg/5904856087b73857756e5b5ac0250f5b.js\"></script>\n\n<p>Here's how it looks:</p>\n\n<p><img src=\"/content/images/2016/10/SEP-Pester-Tests.png\" alt=\"\" /></p>\n\n<p>If you'd like to develop this further, feel free to <a href=\"https://gist.github.com/markwragg/5904856087b73857756e5b5ac0250f5b#file-sep-tests-ps1\">Fork the above Gist</a>.</p>",
                        "image": "/content/images/2016/10/ThinkstockPhotos-158695294.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2016-10-12 10:05:29",
                        "created_by": 1,
                        "updated_at": "2016-10-19 14:34:17",
                        "updated_by": 1,
                        "published_at": "2016-10-19 14:01:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 40,
                        "uuid": "d766e765-37dd-4acb-a04d-c01a92956574",
                        "title": "A Slack Slash Command using Powershell Azure Functions",
                        "slug": "a-slack-slash-command-using-powershell-azure-functions",
                        "markdown": "In my previous post I talked about [creating a Powershell function to lookup terms in a large internal glossary](http://wragg.io/a-powershell-cmdlet-for-looking-up-internal-terms-in-a-glossary/) I had collated. This post takes this a step further and creates a custom slash command integration in a Slack chatroom that calls out to a Powershell-based Azure Function to query my (now) Azure hosted glossary.\n\n> [Azure Functions](https://azure.microsoft.com/en-gb/services/functions/) is currently in Preview. It's also [open source](https://github.com/azure/azure-webjobs-sdk-script) which is awesome. If you're familiar with AWS, Azure Functions are Microsoft's answer to Lambda, i.e [serverless computing](https://en.wikipedia.org/wiki/Serverless_computing).\n>\n> [Slack](https://slack.com/) is a real-time group messaging tool that extends the traditional chat room with an endless array of integrations, empowering a movement known as [ChatOps](http://blogs.atlassian.com/2016/01/what-is-chatops-adoption-guide/).\n\nWhile my glossary lookup Powershell cmdlet was useful to me, it wasn't easy to share and I couldn't really see others wanting to search the glossary via the console. However my company does make use of Slack (albeit somewhat sporadically) and this is something I want to encourage my team to adopt. I also wanted to see how easy it might be to build these kinds of integrations.\n\n# Registering an Azure account\n\nIf you don't yet have an Azure account, you simply need to register one (for free). In fact it's probably worth noting from the outset that all of what i'm about to describe is cost free (within the Trial of Azure certainly) and you're only likely to start [incurring cost](https://azure.microsoft.com/en-gb/pricing/details/functions/) with Functions once you're doing something complex that is being called hundreds of thousands of times in a month or (seemingly) if you store files within the Functions. See the [pricing calculator](https://azure.microsoft.com/en-gb/pricing/calculator/?service=app-service) for more details).\n\n1. Go to https://azure.microsoft.com/\n- Click \"Start for free\" (and again on the next page)\n- Sign in with a Microsoft user account (or create one).\n- Complete the registration (if your experience is like mine this will require validating your human-credentials via a mobile number and credit card - which they ensure you they will not charge while you are on the free tier).\n\nYou should now have an Azure account with £125 of credit (valid for 30 days, and after this time if you want to keep it alive you will just need to switch the account to pay-as-you-go or another payment plan, which *should* still incur little or no cost).\n\n![Azure](/content/images/2016/11/Azure.png)\n\n# Creating your Function App\n\n![Azure Function App](/content/images/2016/11/Azure-FunctionApp.png)\n\n1. In the portal, click **+ New** and search for \"Function App\".\n- Click on Function App then the \"Create\" button.\n- Enter a name for your app, which will be used to create a *yourname*.azurewebsites.com address.\n- You need to enter a name for Resource Group, unless you have an existing one you'd rather use.\n- Most of the other defaults here can probably be left alone, so when happy click \"Create\".\n\nYour function app will now start deploying and you can monitor the progress of this via the notifications area top right. When it's finished, you should find it listed under \"All Resources\".\n\n# Creating your Function\n\n![Azure Running Function App](/content/images/2016/11/Azure-FunctionApp-Running.png)\n\nIf you have clicked in to your Function App resource, you should see the interface above. Ignore the \"Get started quickly\" page, as it only offers you C# or Javascript. Instead:\n\n1. Click **+ New Function**\n- From the drop-down choose the Language \"Powershell\".\n- Click \"HttpTrigger-Powershell\".\n- Enter a name for your Function.\n- Leave the Authorisation level as \"Function\" which will mean that there will a function specific key generated that will be needed with API calls for security.\n- Click \"Create\".\n\n![Azure Function App Powershell Code](/content/images/2016/11/Azure-FunctionApp-Powershell-Code.png)\n\n# Configuring the Slash Command in Slack\n\nNow is a good time to set up the slash command configuration in Slack, because we can then use it for testing. We also now have the API URL that we want Slack to post to (this is the Function URL as you can see in the screenshot above).\n\n1. Log in to your Slack account via a web browser (if you don't have a Slack account, [you can create one for free](https://slack.com/)).\n- Go to **Build** and then click \"Make a Custom Integration\".\n- Click Slash Commands.\n- Enter the name for your command (e.g /lookup) and click \"Add Slash Command Integration\".\n- Scroll down to the Integration Settings section. Go back to Azure and copy the \"Function URL\" in to the URL box of the Integration.\n- All the other defaults are OK, but you might want to make changes to \"Customize Name\", \"Customize Icon\" and \"Autocomplete help text\" to make your integration a little friendlier.\n- Make sure you click **Save Integration** when you're done.\n\n![Slack Slash Command Integration](/content/images/2016/11/Slack-Slash-Command.png)\n\n# Configuring your Function\n\nSwitch back to your Function in Azure. There's some default/sample code which is helpful to review and try to understand. All you really need to know at this point is that whatever Slack submits to our Function URL will be deposited in to a default variable named `$req`.\n\nThe default code here takes the contents of this variable and pipes it to `ConvertFrom-Json` (assuming that the input its receiving is in JSON format). This isn't the case (for reasons I don't know) in Slack, so this is the first thing that needs to go.\n\nSlack does a POST request, which results in the Function receiving the variables as an ampersand separated list. I found an easy way to turn this back in to a Powershell object is to do the following:\n```\n$requestBody = Get-Content $req -raw\n$requestBody = $requestBody.Replace('&',\"`n\")\n$requestBody = ConvertFrom-StringData -StringData $requestBody\n```\n*As an aside, if you want to see for yourself what Slack submits to the Function, you can simply turn it around and send it back by doing the following:*\n\n```\n$requestBody = Get-Content $req -Raw`\nOut-File -Encoding Ascii -FilePath $res -inputObject $requestBody\n```\n*Here's an example of the response of doing that as seen in Slack:*\n![Example of returning the POST of a Slack Slash Command back to Slack to see what it looks like](/content/images/2016/11/Slack-Function-Post-Example-1.png)\n\n*Further, it's worthwhile copying this message, returning to your Azure Function where if you click \"Test\" top-right, you can paste this in to the \"Request body\" box and use it for testing (by hitting the \"Run\" button on the function and seeing the result in the Output box.*\n\n/Aside\n\nHaving got the Slack POST in to a Powershell object, next we need to search our Glossary for a match. But currently the Glossary isn't online anywhere. There's plenty of options for this (Azure Blob Storage, S3, probably OneDrive all likely good options). An equally simple solution is just to upload your file to the Function as follows:\n\n1. Click \"View Files\" top right\n- Click Upload.\n- Select your local Glossary.csv file (note that if you're playing along this needs to be a CSV file with at least \"Term\" and \"Description\" as headers and ideally some contents).\n\n> This is the only place where I seemed to have incurred some cost from my Azure trial free credit (currently I've apparently spent a financially crippling £0.02 on storage writes. I imagine if I were to store my CSV in OneDrive or somewhere else I would get round this (and keep the solution completely cost free), but I have not yet tested.\n\nHaving uploaded the file in to the function, it is available here: D:\\home\\site\\wwwroot\\{yourfunction}\\{yourfile}.csv so we can now do this to query it:\n```\n$Response = (Import-CSV \"D:\\home\\site\\wwwroot\\Glossary\\Glossary.csv\") | Where-Object {$_.Term -like \"$($requestBody.text)*\"}\n\nif(!$Response){Out-File -Encoding Ascii -FilePath $res -inputObject \"Sorry, there is no match in the Glossary for *$($requestBody.text)*.\"}\n```\nAnd we can send back to Slack one or more matches as follows:\n```\n$Response | ForEach-Object{\n    Out-File -Encoding Ascii -FilePath $res -inputObject \"*$($_.Term):* $($_.Description)\" -Append\n}\n```\n# The Result\n\nHere's what the result looks like from Slack (I tested by searching my Glossary for \"FC\" by typing `/lookup FC` for which there are multiple matches:\n\n![](/content/images/2016/11/Slack-Lookup-Fibre-Channel.png)\n\nNote that it says \"Only visible to you\". If you wanted to write a slash command that had results visible to all users, we could change the Azure Function code so that it posts back to the room via an Incoming Webhook.\n\nFor ease of anyone looking to copy the Function code, here it is in full:\n<script src=\"https://gist.github.com/markwragg/f7e45994b45fa3b295e4e5ae8b1b4243.js\"></script>\n\n# Caveats\n\nThe above is a relatively simple solution in the hopes that it demonstrates how you can use Azure Powershell Functions to drive Slack integrations and is helpful to anyone for whom Powershell is a language of choice.\n\nThere are a couple of Caveats that I want to note (all of which are resolvable with more development or as noted):\n\n- The \"command\" passed from Slack is URL encoded, so you need to unencode it if you want multi-word searches (or searches with special characters) to work as expected (otherwise a space characters is a + etc.).\n- On the current plan, the Function App goes to sleep after a while and it takes a call then 10+ seconds to wake it up (which means you can end up calling the command multiple times from Slack before it responds). The quickest/easiest way to work around this is to use a web based monitoring tool such as StatusCake or Pingdom to just hit the URL of your function every 15 minutes or so which effectively keeps it alive. Alternatively if you're not using the Free or Shared plans [you can enable \"Always On\" for your Function as described here](http://stackoverflow.com/questions/39430932/how-do-i-turn-on-always-on-for-an-azure-function).\n\nYou also might want to look in to presenting the information returned from your functions in a nicer way using the more advanced [attachment based message format](https://api.slack.com/docs/message-attachments) that Slack provides.",
                        "html": "<p>In my previous post I talked about <a href=\"http://wragg.io/a-powershell-cmdlet-for-looking-up-internal-terms-in-a-glossary/\">creating a Powershell function to lookup terms in a large internal glossary</a> I had collated. This post takes this a step further and creates a custom slash command integration in a Slack chatroom that calls out to a Powershell-based Azure Function to query my (now) Azure hosted glossary.</p>\n\n<blockquote>\n  <p><a href=\"https://azure.microsoft.com/en-gb/services/functions/\">Azure Functions</a> is currently in Preview. It's also <a href=\"https://github.com/azure/azure-webjobs-sdk-script\">open source</a> which is awesome. If you're familiar with AWS, Azure Functions are Microsoft's answer to Lambda, i.e <a href=\"https://en.wikipedia.org/wiki/Serverless_computing\">serverless computing</a>.</p>\n  \n  <p><a href=\"https://slack.com/\">Slack</a> is a real-time group messaging tool that extends the traditional chat room with an endless array of integrations, empowering a movement known as <a href=\"http://blogs.atlassian.com/2016/01/what-is-chatops-adoption-guide/\">ChatOps</a>.</p>\n</blockquote>\n\n<p>While my glossary lookup Powershell cmdlet was useful to me, it wasn't easy to share and I couldn't really see others wanting to search the glossary via the console. However my company does make use of Slack (albeit somewhat sporadically) and this is something I want to encourage my team to adopt. I also wanted to see how easy it might be to build these kinds of integrations.</p>\n\n<h1 id=\"registeringanazureaccount\">Registering an Azure account</h1>\n\n<p>If you don't yet have an Azure account, you simply need to register one (for free). In fact it's probably worth noting from the outset that all of what i'm about to describe is cost free (within the Trial of Azure certainly) and you're only likely to start <a href=\"https://azure.microsoft.com/en-gb/pricing/details/functions/\">incurring cost</a> with Functions once you're doing something complex that is being called hundreds of thousands of times in a month or (seemingly) if you store files within the Functions. See the <a href=\"https://azure.microsoft.com/en-gb/pricing/calculator/?service=app-service\">pricing calculator</a> for more details).</p>\n\n<ol>\n<li>Go to <a href=\"https://azure.microsoft.com/\">https://azure.microsoft.com/</a>  </li>\n<li>Click \"Start for free\" (and again on the next page)</li>\n<li>Sign in with a Microsoft user account (or create one).</li>\n<li>Complete the registration (if your experience is like mine this will require validating your human-credentials via a mobile number and credit card - which they ensure you they will not charge while you are on the free tier).</li>\n</ol>\n\n<p>You should now have an Azure account with £125 of credit (valid for 30 days, and after this time if you want to keep it alive you will just need to switch the account to pay-as-you-go or another payment plan, which <em>should</em> still incur little or no cost).</p>\n\n<p><img src=\"/content/images/2016/11/Azure.png\" alt=\"Azure\" /></p>\n\n<h1 id=\"creatingyourfunctionapp\">Creating your Function App</h1>\n\n<p><img src=\"/content/images/2016/11/Azure-FunctionApp.png\" alt=\"Azure Function App\" /></p>\n\n<ol>\n<li>In the portal, click <strong>+ New</strong> and search for \"Function App\".  </li>\n<li>Click on Function App then the \"Create\" button.</li>\n<li>Enter a name for your app, which will be used to create a <em>yourname</em>.azurewebsites.com address.</li>\n<li>You need to enter a name for Resource Group, unless you have an existing one you'd rather use.</li>\n<li>Most of the other defaults here can probably be left alone, so when happy click \"Create\".</li>\n</ol>\n\n<p>Your function app will now start deploying and you can monitor the progress of this via the notifications area top right. When it's finished, you should find it listed under \"All Resources\".</p>\n\n<h1 id=\"creatingyourfunction\">Creating your Function</h1>\n\n<p><img src=\"/content/images/2016/11/Azure-FunctionApp-Running.png\" alt=\"Azure Running Function App\" /></p>\n\n<p>If you have clicked in to your Function App resource, you should see the interface above. Ignore the \"Get started quickly\" page, as it only offers you C# or Javascript. Instead:</p>\n\n<ol>\n<li>Click <strong>+ New Function</strong>  </li>\n<li>From the drop-down choose the Language \"Powershell\".</li>\n<li>Click \"HttpTrigger-Powershell\".</li>\n<li>Enter a name for your Function.</li>\n<li>Leave the Authorisation level as \"Function\" which will mean that there will a function specific key generated that will be needed with API calls for security.</li>\n<li>Click \"Create\".</li>\n</ol>\n\n<p><img src=\"/content/images/2016/11/Azure-FunctionApp-Powershell-Code.png\" alt=\"Azure Function App Powershell Code\" /></p>\n\n<h1 id=\"configuringtheslashcommandinslack\">Configuring the Slash Command in Slack</h1>\n\n<p>Now is a good time to set up the slash command configuration in Slack, because we can then use it for testing. We also now have the API URL that we want Slack to post to (this is the Function URL as you can see in the screenshot above).</p>\n\n<ol>\n<li>Log in to your Slack account via a web browser (if you don't have a Slack account, <a href=\"https://slack.com/\">you can create one for free</a>).  </li>\n<li>Go to <strong>Build</strong> and then click \"Make a Custom Integration\".</li>\n<li>Click Slash Commands.</li>\n<li>Enter the name for your command (e.g /lookup) and click \"Add Slash Command Integration\".</li>\n<li>Scroll down to the Integration Settings section. Go back to Azure and copy the \"Function URL\" in to the URL box of the Integration.</li>\n<li>All the other defaults are OK, but you might want to make changes to \"Customize Name\", \"Customize Icon\" and \"Autocomplete help text\" to make your integration a little friendlier.</li>\n<li>Make sure you click <strong>Save Integration</strong> when you're done.</li>\n</ol>\n\n<p><img src=\"/content/images/2016/11/Slack-Slash-Command.png\" alt=\"Slack Slash Command Integration\" /></p>\n\n<h1 id=\"configuringyourfunction\">Configuring your Function</h1>\n\n<p>Switch back to your Function in Azure. There's some default/sample code which is helpful to review and try to understand. All you really need to know at this point is that whatever Slack submits to our Function URL will be deposited in to a default variable named <code>$req</code>.</p>\n\n<p>The default code here takes the contents of this variable and pipes it to <code>ConvertFrom-Json</code> (assuming that the input its receiving is in JSON format). This isn't the case (for reasons I don't know) in Slack, so this is the first thing that needs to go.</p>\n\n<p>Slack does a POST request, which results in the Function receiving the variables as an ampersand separated list. I found an easy way to turn this back in to a Powershell object is to do the following:  </p>\n\n<pre><code>$requestBody = Get-Content $req -raw\n$requestBody = $requestBody.Replace('&amp;',\"`n\")\n$requestBody = ConvertFrom-StringData -StringData $requestBody\n</code></pre>\n\n<p><em>As an aside, if you want to see for yourself what Slack submits to the Function, you can simply turn it around and send it back by doing the following:</em></p>\n\n<pre><code>$requestBody = Get-Content $req -Raw`\nOut-File -Encoding Ascii -FilePath $res -inputObject $requestBody  \n</code></pre>\n\n<p><em>Here's an example of the response of doing that as seen in Slack:</em>\n<img src=\"/content/images/2016/11/Slack-Function-Post-Example-1.png\" alt=\"Example of returning the POST of a Slack Slash Command back to Slack to see what it looks like\" /></p>\n\n<p><em>Further, it's worthwhile copying this message, returning to your Azure Function where if you click \"Test\" top-right, you can paste this in to the \"Request body\" box and use it for testing (by hitting the \"Run\" button on the function and seeing the result in the Output box.</em></p>\n\n<p>/Aside</p>\n\n<p>Having got the Slack POST in to a Powershell object, next we need to search our Glossary for a match. But currently the Glossary isn't online anywhere. There's plenty of options for this (Azure Blob Storage, S3, probably OneDrive all likely good options). An equally simple solution is just to upload your file to the Function as follows:</p>\n\n<ol>\n<li>Click \"View Files\" top right  </li>\n<li>Click Upload.</li>\n<li>Select your local Glossary.csv file (note that if you're playing along this needs to be a CSV file with at least \"Term\" and \"Description\" as headers and ideally some contents).</li>\n</ol>\n\n<blockquote>\n  <p>This is the only place where I seemed to have incurred some cost from my Azure trial free credit (currently I've apparently spent a financially crippling £0.02 on storage writes. I imagine if I were to store my CSV in OneDrive or somewhere else I would get round this (and keep the solution completely cost free), but I have not yet tested.</p>\n</blockquote>\n\n<p>Having uploaded the file in to the function, it is available here: D:\\home\\site\\wwwroot{yourfunction}{yourfile}.csv so we can now do this to query it:  </p>\n\n<pre><code>$Response = (Import-CSV \"D:\\home\\site\\wwwroot\\Glossary\\Glossary.csv\") | Where-Object {$_.Term -like \"$($requestBody.text)*\"}\n\nif(!$Response){Out-File -Encoding Ascii -FilePath $res -inputObject \"Sorry, there is no match in the Glossary for *$($requestBody.text)*.\"}  \n</code></pre>\n\n<p>And we can send back to Slack one or more matches as follows:  </p>\n\n<pre><code>$Response | ForEach-Object{\n    Out-File -Encoding Ascii -FilePath $res -inputObject \"*$($_.Term):* $($_.Description)\" -Append\n}\n</code></pre>\n\n<h1 id=\"theresult\">The Result</h1>\n\n<p>Here's what the result looks like from Slack (I tested by searching my Glossary for \"FC\" by typing <code>/lookup FC</code> for which there are multiple matches:</p>\n\n<p><img src=\"/content/images/2016/11/Slack-Lookup-Fibre-Channel.png\" alt=\"\" /></p>\n\n<p>Note that it says \"Only visible to you\". If you wanted to write a slash command that had results visible to all users, we could change the Azure Function code so that it posts back to the room via an Incoming Webhook.</p>\n\n<p>For ease of anyone looking to copy the Function code, here it is in full:  </p>\n\n<script src=\"https://gist.github.com/markwragg/f7e45994b45fa3b295e4e5ae8b1b4243.js\"></script>\n\n<h1 id=\"caveats\">Caveats</h1>\n\n<p>The above is a relatively simple solution in the hopes that it demonstrates how you can use Azure Powershell Functions to drive Slack integrations and is helpful to anyone for whom Powershell is a language of choice.</p>\n\n<p>There are a couple of Caveats that I want to note (all of which are resolvable with more development or as noted):</p>\n\n<ul>\n<li>The \"command\" passed from Slack is URL encoded, so you need to unencode it if you want multi-word searches (or searches with special characters) to work as expected (otherwise a space characters is a + etc.).</li>\n<li>On the current plan, the Function App goes to sleep after a while and it takes a call then 10+ seconds to wake it up (which means you can end up calling the command multiple times from Slack before it responds). The quickest/easiest way to work around this is to use a web based monitoring tool such as StatusCake or Pingdom to just hit the URL of your function every 15 minutes or so which effectively keeps it alive. Alternatively if you're not using the Free or Shared plans <a href=\"http://stackoverflow.com/questions/39430932/how-do-i-turn-on-always-on-for-an-azure-function\">you can enable \"Always On\" for your Function as described here</a>.</li>\n</ul>\n\n<p>You also might want to look in to presenting the information returned from your functions in a nicer way using the more advanced <a href=\"https://api.slack.com/docs/message-attachments\">attachment based message format</a> that Slack provides.</p>",
                        "image": "/content/images/2016/11/buildazure_slack-1.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": "How to use an Azure Powershell Function to respond to a slash command in Slack. In this example I host a glossary of terms in Azure and query it from Slack.",
                        "author_id": 1,
                        "created_at": "2016-11-11 16:35:32",
                        "created_by": 1,
                        "updated_at": "2016-11-19 16:27:23",
                        "updated_by": 1,
                        "published_at": "2016-11-19 16:27:23",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 41,
                        "uuid": "c759eb97-eba7-4aac-9c6e-573b087cad40",
                        "title": "A Powershell cmdlet for looking up internal terms in a glossary",
                        "slug": "a-powershell-cmdlet-for-looking-up-internal-terms-in-a-glossary",
                        "markdown": "I've recently started a new role and as a result I have a lot of new business terms and acronyms to get to grips with. A list of about 50 of these were handed to me on my first day which was really helpful, but when I searched around the intranet and various other sources of documentation I found there were many more.\n\nIn an effort to reduce the amount of searching I might have to do in the future, I decided to see if I could compile these sources together. This was generally pretty easy to do, because the various lists were already in a tabular format, or laid out in a consistent way where I could use things like Excel's Text to Columns function to get them consistent. At first I had a simple 2 column spreadsheet of \"Term\" and \"Description\" but then it also started to make sense to broadly categorise them (based on their various sources) which was simple to do with an additional \"Category\" column. Finally some of the definitions had web links to more informaton so I added a \"Link\" column too to optionally include this.\n\n**When I was done, I found I had managed to compile a list of over 900 terms.** Which was ~~horrifying~~ great! But then searching Excel felt clunky and so I turned to Powershell.\n\nI created a function called `Search-Glossary`. Initially I decided I wanted the output to look akin to the Powershell help function, with capitilised titles for each of my spreadsheets columns, but only presenting each bit of information where it exists. Pretty simple with the much maligned `write-host`:\n\n![](/content/images/2016/11/Glossary-Powershell1-2.png)\n\nCreating the output this way initially meant that strings wider than the console wrapped half way through words and only the first line was tabbed which didn't look great. I found a [wrapText function on Stack Exchange](http://stackoverflow.com/questions/1059663/is-there-a-way-to-wordwrap-results-of-a-powershell-cmdlet) (and modified it a little to my needs) that solved that issue, so it now moves to the next line whenever a word will go over the current width of the console.\n\nI liked this, but it didn't look so great when the search term returned multiple matches. The standard output from `format-list` was better for this and by switching to this when there were multiple matches it was then more obvious that multiple matches had been returned:\n\n![](/content/images/2016/11/Glossary-Powershell2-1.png)\n\nHowever both of these previous options broke the pipeline. While I didn't currently have a need to push the results down the pipeline, it seemed a shame not to be able to do so in the future for the sake of some formatting. To solve this I added a `-PassThru` switch (a common parameter name on some other cmdlets) that changes the result to just the default `Return` and resultant formatting (which is `format-table`) and fixes the pipeline:\n\n![](/content/images/2016/11/Glossary-Powershell3-1.png)\n\nHere's the full code, free to a good home:\n\n<script src=\"https://gist.github.com/markwragg/0e59148c8a6e7ac1fa0c669a63584474.js\"></script>\n\nThere was a certain pointless novelty to this, but it is useful to me (although still a little clunky to use as I have to open Powershell, bypass the execution policy and load my module before I can use it) however I can't see others in my team leaping to adopt it as a way to query a glossary. But the contents of that glossary are likely useful (particularly to other new starters). Which got me thinking..\n\nThis is the perfect excuse to play with something I've been meaning to try for a while: Powershell Azure Functions -- and more specifically -- leveraging them to create a slash command in Slack. See my next post for details on that (coming soon)!\n",
                        "html": "<p>I've recently started a new role and as a result I have a lot of new business terms and acronyms to get to grips with. A list of about 50 of these were handed to me on my first day which was really helpful, but when I searched around the intranet and various other sources of documentation I found there were many more.</p>\n\n<p>In an effort to reduce the amount of searching I might have to do in the future, I decided to see if I could compile these sources together. This was generally pretty easy to do, because the various lists were already in a tabular format, or laid out in a consistent way where I could use things like Excel's Text to Columns function to get them consistent. At first I had a simple 2 column spreadsheet of \"Term\" and \"Description\" but then it also started to make sense to broadly categorise them (based on their various sources) which was simple to do with an additional \"Category\" column. Finally some of the definitions had web links to more informaton so I added a \"Link\" column too to optionally include this.</p>\n\n<p><strong>When I was done, I found I had managed to compile a list of over 900 terms.</strong> Which was <del>horrifying</del> great! But then searching Excel felt clunky and so I turned to Powershell.</p>\n\n<p>I created a function called <code>Search-Glossary</code>. Initially I decided I wanted the output to look akin to the Powershell help function, with capitilised titles for each of my spreadsheets columns, but only presenting each bit of information where it exists. Pretty simple with the much maligned <code>write-host</code>:</p>\n\n<p><img src=\"/content/images/2016/11/Glossary-Powershell1-2.png\" alt=\"\" /></p>\n\n<p>Creating the output this way initially meant that strings wider than the console wrapped half way through words and only the first line was tabbed which didn't look great. I found a <a href=\"http://stackoverflow.com/questions/1059663/is-there-a-way-to-wordwrap-results-of-a-powershell-cmdlet\">wrapText function on Stack Exchange</a> (and modified it a little to my needs) that solved that issue, so it now moves to the next line whenever a word will go over the current width of the console.</p>\n\n<p>I liked this, but it didn't look so great when the search term returned multiple matches. The standard output from <code>format-list</code> was better for this and by switching to this when there were multiple matches it was then more obvious that multiple matches had been returned:</p>\n\n<p><img src=\"/content/images/2016/11/Glossary-Powershell2-1.png\" alt=\"\" /></p>\n\n<p>However both of these previous options broke the pipeline. While I didn't currently have a need to push the results down the pipeline, it seemed a shame not to be able to do so in the future for the sake of some formatting. To solve this I added a <code>-PassThru</code> switch (a common parameter name on some other cmdlets) that changes the result to just the default <code>Return</code> and resultant formatting (which is <code>format-table</code>) and fixes the pipeline:</p>\n\n<p><img src=\"/content/images/2016/11/Glossary-Powershell3-1.png\" alt=\"\" /></p>\n\n<p>Here's the full code, free to a good home:</p>\n\n<script src=\"https://gist.github.com/markwragg/0e59148c8a6e7ac1fa0c669a63584474.js\"></script>\n\n<p>There was a certain pointless novelty to this, but it is useful to me (although still a little clunky to use as I have to open Powershell, bypass the execution policy and load my module before I can use it) however I can't see others in my team leaping to adopt it as a way to query a glossary. But the contents of that glossary are likely useful (particularly to other new starters). Which got me thinking..</p>\n\n<p>This is the perfect excuse to play with something I've been meaning to try for a while: Powershell Azure Functions -- and more specifically -- leveraging them to create a slash command in Slack. See my next post for details on that (coming soon)!</p>",
                        "image": "/content/images/2016/11/Glossary.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": "A Powershell cmdlet for querying a large CSV glossary of internal terms. The cmdlet returns formatted results or can pass the results down the pipeline.",
                        "author_id": 1,
                        "created_at": "2016-11-11 18:14:04",
                        "created_by": 1,
                        "updated_at": "2016-11-11 20:15:06",
                        "updated_by": 1,
                        "published_at": "2016-11-11 20:05:19",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 42,
                        "uuid": "c46b6fec-ebd7-472d-919c-7dd18687a330",
                        "title": "Powershell Slack Bot using the Real Time Messaging API",
                        "slug": "powershell-slack-bot-using-the-real-time-messaging-api",
                        "markdown": "This post details how PowerShell can be used to run a custom Slack Bot that utilises the Slack RTM (Real Time Messaging) API. \n\nFollowing on from my previous post where I set up a [Slack Slash Command using Azure Functions](http://wragg.io/a-slack-slash-command-using-powershell-azure-functions/), I wanted to provide more functionality to my team via a single multi function Slack Bot. Slack has three APIs: The Web API, The Events API and the RTM API. There are lots of [community frameworks](https://api.slack.com/community) for working with the Slack APIs in a variety of languages, but for PowerShell existing options seemed to be limited to the Web API.\n\n> The full code for this project can be found [here in GitHub](https://github.com/markwragg/Powershell-SlackBot) where I warmly welcome contributions. To learn more about the how and the why, read on below.\n\n### Why use the RTM API?\n\nI actually got a workable solution implemented with the Web API, but it felt a little like bastardisation when using it to author a bot. I used the [PSSlack module by Warren F](http://ramblingcookiemonster.github.io/PSSlack/) to repeatedly read the message history of a slack Channel, monitoring for messages directed to the name of my bot and responding when appropriate. This was fine to a point, but apart from feeling like a misuse of the API it also had a few limitations:\n\n- **I had to specify which Channel/s to monitor for messages.** This is fine if you want the Bot to only be present in specific channels. Also you could potentially monitor all channels by using the Web API to get a list of them, but as the message history of channels can be large, I was skeptical about how well this would scale for a busy Slack account.\n- **The Web API can't read or respond to direct messages.** One of my users asked for this as a feature, so that they could interact with the Bot without always spamming other users.\n\nThe RTM API allows a bot to establish a websocket connection to the Slack API through which it receives an ongoing stream of messages that it is privy to (just like any other user does via whatever Slack client they use). The Bot connects and sees chat messages for channels it is present in, messages sent to it directly, user presence changes etc.\n\n![](/content/images/2017/01/Slack-Bot-Slack-1.png)\n\n### Caveats\n\n- There's no native way to interact with Websockets via PowerShell. Instead, you need to utilise .NET and the [ClientWebSocket Class](https://msdn.microsoft.com/en-us/library/system.net.websockets.clientwebsocket(v=vs.110).aspx) of the [System.Net.WebSockets Namespace](https://msdn.microsoft.com/en-us/library/system.net.websockets(v=vs.110).aspx).\n- System.Net.WebSockets is only available with Windows 8 / Server 2012 and newer Operating Systems.\n- The RTM API can only send \"simple\" messages. However there's nothing stopping you from utilising the Web API if/when you want to send more advanced messages (e.g with [attachments](https://api.slack.com/docs/attachments)) and using [PSSlack](http://ramblingcookiemonster.github.io/PSSlack/) can make this easier. \n\n> I would not have figured out how to do this had I not come across [this code](https://github.com/brianddk/ripple-ps-websocket) by [brianddk](https://github.com/brianddk) which demonstrated the exact .NET concepts I needed within a PowerShell script, albeit for a different purpose.\n\n### Getting Started\n\nIf you haven't already, you first need to log in to your Slack account and create a Bot under Custom Integrations > Bots. You then need the API token of your Bot, which I've elected to store in an XML file via `Export-Clixml` so that it's not hard coded in my script and instead read from a file:\n\n```\nParam(\n    $Token = (Import-Clixml Token.xml)\n)\n```\n\n### The code\n\nAs detailed in the [RTM API Documentation](https://api.slack.com/rtm), the script first makes a Web API call to the `rtm.start` method with the Bot's token:\n\n```\n$RTMSession = Invoke-RestMethod -Uri https://slack.com/api/rtm.start -Body @{token=\"$Token\"}\nWrite-Verbose \"I am $($RTMSession.self.name)\"\n```\n\n> If this call is successful, `$RTMSession` will contain various properties that you might also find useful, including details of the Bot users name, ID, preferences, a list of channels, groups etc.\n\n$RTMSession then has a `.URL` property which is the WebSocket URL needed to connect to the RTM API using the ClientWebSocket .NET class.\n\nI have encapsulated the rest of the code in a Try block, because I wanted to make sure that the WebSocket was properly closed whenever I quit the script. This does happen automatically when the PowerShell process ends, but if you're repeatedly stopping and starting the the script within a single PowerShell window (or the ISE) it doesn't, so by using `Try` I could put clean up code in a `Finally` block which is executed when you quit the script with CTRL+C.\n\n```\nTry{\n    Do{\n        $WS = New-Object System.Net.WebSockets.ClientWebSocket                                                \n        $CT = New-Object System.Threading.CancellationToken                                                   \n\n        $Conn = $WS.ConnectAsync($RTMSession.URL, $CT)                                                  \n        While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n        Write-Verbose \"Connected to $($RTMSession.URL)\"\n\n        $Size = 1024\n        $Array = [byte[]] @(,0) * $Size\n        $Recv = New-Object System.ArraySegment[byte] -ArgumentList @(,$Array)\n```\nThe above establishes the connection to the websocket. This is encapsulated in a Do loop, because one of the messages sent through the RTM API is a \"reconnect_url\" event, which gives us a new websocket URL to connect to every 30 seconds. This isn't necessarily used (the websocket connection should stay alive as long as we need it) but it gives a fallback way to reconnect should the connection drop, without having to recall the `rtm.start` method.\n\n```\n        While ($WS.State -eq 'Open') {\n\n            $RTM = \"\"\n        \n            Do {\n                $Conn = $WS.ReceiveAsync($Recv, $CT)\n                While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n                $Recv.Array[0..($Conn.Result.Count - 1)] | ForEach { $RTM += [char]$_ }\n       \n            } Until ($Conn.Result.Count -lt $Size)\n\n            Write-Verbose \"`n$RTM\"\n```\nIf we have a successful connection to the websocket, we next read the current message through it using the `ReceiveAsync` method of the .NET class in to `$RTM` as JSON.\n\nIf we have received a message, we convert it from JSON and handle it:\n\n```\n            If ($RTM){\n                $RTM = ($RTM | convertfrom-json)\n                    \n                Switch ($RTM){\n                    {($_.type -eq 'message') -and (!$_.reply_to)} { \n                        \n                        If ( ($_.text -Match \"<@$($RTMSession.self.id)>\") -or $_.channel.StartsWith(\"D\") ){\n                            #A message was sent to the bot\n                            \n                            # *** Responses go here, for example..***\n                            $words = ($_.text.ToLower() -split \" \")\n                            \n                            Switch ($words){\n                                {@(\"hey\",\"hello\",\"hi\") -contains $_} { Send-SlackMsg -Text 'Hello!' -Channel $RTM.Channel }\n                                {@(\"bye\",\"cya\") -contains $_} { Send-SlackMsg -Text 'Goodbye!' -Channel $RTM.Channel }\n\n                                default { Write-Verbose \"I have no response for $_\" }\n                            }\n\n                        }Else{\n                            Write-Verbose \"Message ignored as it wasn't sent to @$($RTMSession.self.name) or in a DM channel\"\n                        }\n                    }\n                    {$_.type -eq 'reconnect_url'} { $RTMSession.URL = $RTM.url }\n                        \n                    default { Write-Verbose \"No action specified for $($RTM.type) event\" }            \n                }\n```\nIn the above we look for messages that were either in a Direct Message channel (e.g from any user, privately to the Bot user) or were in any public Channel that the Bot has been invited to and where the message includes @*botname* (where the name of the Bot was determined when we invoked the `rtm.start` method).\n\nIf either of these are true, then a Switch statement looks for opportunities to respond (see below for the `Send-SlackMsg` function). This is obviously where you extend the Bot with the responses and functionality that you want it to perform.\n\nHere we could also add Switch logic to handle other types of messages, e.g if we wanted the Bot to take some action for any of the other [RTM Events](https://api.slack.com/rtm).\n\n![](/content/images/2017/01/Slack-Bot.png)\n\nThe code loops continuously, reading messages from the websocket instantly whenever they are available, until the scripts execution is forcibly stopped:\n```\n            }\n        }   \n    } Until (!$Conn)\n\n}Finally{\n\n    If ($WS) { \n        Write-Verbose \"Closing websocket\"\n        $WS.Dispose()\n    }\n\n}\n```\nIn lieu of any other activity, you'll see the Bot receive a \"reconnect_url\" event every 30 seconds, which I suspect also serves to keep the connection alive.\n\n### Functions\n\nThis function uses the established websocket to send simple slack messages back to any Channel specified (including Direct Message channels) so that we can respond to messages:\n```\nFunction Send-SlackMsg\n{\n    [cmdletbinding()]\n    Param(\n        $Text,\n        $Channel,\n        $ID = (get-date).ticks\n    )\n    \n    $Prop = @{'id'      = $ID;\n              'type'    = 'message';\n              'text'    = $Text;\n              'channel' = $Channel}\n            \n    $Reply = (New-Object –TypeName PSObject –Prop $Prop) | ConvertTo-Json\n            \n    $Array = @()\n    $Reply.ToCharArray() | ForEach { $Array += [byte]$_ }          \n    $Reply = New-Object System.ArraySegment[byte]  -ArgumentList @(,$Array)\n\n    $Conn = $WS.SendAsync($Reply, [System.Net.WebSockets.WebSocketMessageType]::Text, [System.Boolean]::TrueString, $CT)\n    While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n    Return $ID\n}\n```\nAs noted earlier, we can only reply with the simple message format in this way. To use any other message formatting type (e.g attachments) you'll need to fall back to the Web API.\n\nMy basic Bot doesn't make use of the below function (which I did not write), but I wanted to share it as it is useful if you want to convert the timestamp (TS) properties you get with some of the Slack events in to a PowerShell DateTime object. \n\n```\nFunction ConvertFrom-UnixTime {\n    param(\n        [Parameter(Mandatory=$true,ValueFromPipeline=$true)]\n        [Int32]$UnixTime\n    )\n    begin {\n        $startdate = Get-Date –Date '01/01/1970' \n    }\n    process {\n        $timespan = New-Timespan -Seconds $UnixTime\n        $startdate + $timespan\n    }\n}\n```\nThis function is also in the PSSlack module where it's credited as being from [Powershell.com](http://community.idera.com/powershell/powertips/b/tips/posts/converting-unix-time).\n\n### Contributions\n\nThis project is in Github as [Powershell-SlackBot](https://github.com/markwragg/Powershell-SlackBot) and I welcome pull requests or suggestions to help improve it. It is now also listed on the [Slack Community page](https://api.slack.com/community#powershell) under Powershell.",
                        "html": "<p>This post details how PowerShell can be used to run a custom Slack Bot that utilises the Slack RTM (Real Time Messaging) API. </p>\n\n<p>Following on from my previous post where I set up a <a href=\"http://wragg.io/a-slack-slash-command-using-powershell-azure-functions/\">Slack Slash Command using Azure Functions</a>, I wanted to provide more functionality to my team via a single multi function Slack Bot. Slack has three APIs: The Web API, The Events API and the RTM API. There are lots of <a href=\"https://api.slack.com/community\">community frameworks</a> for working with the Slack APIs in a variety of languages, but for PowerShell existing options seemed to be limited to the Web API.</p>\n\n<blockquote>\n  <p>The full code for this project can be found <a href=\"https://github.com/markwragg/Powershell-SlackBot\">here in GitHub</a> where I warmly welcome contributions. To learn more about the how and the why, read on below.</p>\n</blockquote>\n\n<h3 id=\"whyusethertmapi\">Why use the RTM API?</h3>\n\n<p>I actually got a workable solution implemented with the Web API, but it felt a little like bastardisation when using it to author a bot. I used the <a href=\"http://ramblingcookiemonster.github.io/PSSlack/\">PSSlack module by Warren F</a> to repeatedly read the message history of a slack Channel, monitoring for messages directed to the name of my bot and responding when appropriate. This was fine to a point, but apart from feeling like a misuse of the API it also had a few limitations:</p>\n\n<ul>\n<li><strong>I had to specify which Channel/s to monitor for messages.</strong> This is fine if you want the Bot to only be present in specific channels. Also you could potentially monitor all channels by using the Web API to get a list of them, but as the message history of channels can be large, I was skeptical about how well this would scale for a busy Slack account.</li>\n<li><strong>The Web API can't read or respond to direct messages.</strong> One of my users asked for this as a feature, so that they could interact with the Bot without always spamming other users.</li>\n</ul>\n\n<p>The RTM API allows a bot to establish a websocket connection to the Slack API through which it receives an ongoing stream of messages that it is privy to (just like any other user does via whatever Slack client they use). The Bot connects and sees chat messages for channels it is present in, messages sent to it directly, user presence changes etc.</p>\n\n<p><img src=\"/content/images/2017/01/Slack-Bot-Slack-1.png\" alt=\"\" /></p>\n\n<h3 id=\"caveats\">Caveats</h3>\n\n<ul>\n<li>There's no native way to interact with Websockets via PowerShell. Instead, you need to utilise .NET and the <a href=\"https://msdn.microsoft.com/en-us/library/system.net.websockets.clientwebsocket(v=vs.110).aspx\">ClientWebSocket Class</a> of the <a href=\"https://msdn.microsoft.com/en-us/library/system.net.websockets(v=vs.110).aspx\">System.Net.WebSockets Namespace</a>.</li>\n<li>System.Net.WebSockets is only available with Windows 8 / Server 2012 and newer Operating Systems.</li>\n<li>The RTM API can only send \"simple\" messages. However there's nothing stopping you from utilising the Web API if/when you want to send more advanced messages (e.g with <a href=\"https://api.slack.com/docs/attachments\">attachments</a>) and using <a href=\"http://ramblingcookiemonster.github.io/PSSlack/\">PSSlack</a> can make this easier. </li>\n</ul>\n\n<blockquote>\n  <p>I would not have figured out how to do this had I not come across <a href=\"https://github.com/brianddk/ripple-ps-websocket\">this code</a> by <a href=\"https://github.com/brianddk\">brianddk</a> which demonstrated the exact .NET concepts I needed within a PowerShell script, albeit for a different purpose.</p>\n</blockquote>\n\n<h3 id=\"gettingstarted\">Getting Started</h3>\n\n<p>If you haven't already, you first need to log in to your Slack account and create a Bot under Custom Integrations > Bots. You then need the API token of your Bot, which I've elected to store in an XML file via <code>Export-Clixml</code> so that it's not hard coded in my script and instead read from a file:</p>\n\n<pre><code>Param(  \n    $Token = (Import-Clixml Token.xml)\n)\n</code></pre>\n\n<h3 id=\"thecode\">The code</h3>\n\n<p>As detailed in the <a href=\"https://api.slack.com/rtm\">RTM API Documentation</a>, the script first makes a Web API call to the <code>rtm.start</code> method with the Bot's token:</p>\n\n<pre><code>$RTMSession = Invoke-RestMethod -Uri https://slack.com/api/rtm.start -Body @{token=\"$Token\"}\nWrite-Verbose \"I am $($RTMSession.self.name)\"  \n</code></pre>\n\n<blockquote>\n  <p>If this call is successful, <code>$RTMSession</code> will contain various properties that you might also find useful, including details of the Bot users name, ID, preferences, a list of channels, groups etc.</p>\n</blockquote>\n\n<p>$RTMSession then has a <code>.URL</code> property which is the WebSocket URL needed to connect to the RTM API using the ClientWebSocket .NET class.</p>\n\n<p>I have encapsulated the rest of the code in a Try block, because I wanted to make sure that the WebSocket was properly closed whenever I quit the script. This does happen automatically when the PowerShell process ends, but if you're repeatedly stopping and starting the the script within a single PowerShell window (or the ISE) it doesn't, so by using <code>Try</code> I could put clean up code in a <code>Finally</code> block which is executed when you quit the script with CTRL+C.</p>\n\n<pre><code>Try{  \n    Do{\n        $WS = New-Object System.Net.WebSockets.ClientWebSocket                                                \n        $CT = New-Object System.Threading.CancellationToken                                                   \n\n        $Conn = $WS.ConnectAsync($RTMSession.URL, $CT)                                                  \n        While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n        Write-Verbose \"Connected to $($RTMSession.URL)\"\n\n        $Size = 1024\n        $Array = [byte[]] @(,0) * $Size\n        $Recv = New-Object System.ArraySegment[byte] -ArgumentList @(,$Array)\n</code></pre>\n\n<p>The above establishes the connection to the websocket. This is encapsulated in a Do loop, because one of the messages sent through the RTM API is a \"reconnect_url\" event, which gives us a new websocket URL to connect to every 30 seconds. This isn't necessarily used (the websocket connection should stay alive as long as we need it) but it gives a fallback way to reconnect should the connection drop, without having to recall the <code>rtm.start</code> method.</p>\n\n<pre><code>        While ($WS.State -eq 'Open') {\n\n            $RTM = \"\"\n\n            Do {\n                $Conn = $WS.ReceiveAsync($Recv, $CT)\n                While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n                $Recv.Array[0..($Conn.Result.Count - 1)] | ForEach { $RTM += [char]$_ }\n\n            } Until ($Conn.Result.Count -lt $Size)\n\n            Write-Verbose \"`n$RTM\"\n</code></pre>\n\n<p>If we have a successful connection to the websocket, we next read the current message through it using the <code>ReceiveAsync</code> method of the .NET class in to <code>$RTM</code> as JSON.</p>\n\n<p>If we have received a message, we convert it from JSON and handle it:</p>\n\n<pre><code>            If ($RTM){\n                $RTM = ($RTM | convertfrom-json)\n\n                Switch ($RTM){\n                    {($_.type -eq 'message') -and (!$_.reply_to)} { \n\n                        If ( ($_.text -Match \"&lt;@$($RTMSession.self.id)&gt;\") -or $_.channel.StartsWith(\"D\") ){\n                            #A message was sent to the bot\n\n                            # *** Responses go here, for example..***\n                            $words = ($_.text.ToLower() -split \" \")\n\n                            Switch ($words){\n                                {@(\"hey\",\"hello\",\"hi\") -contains $_} { Send-SlackMsg -Text 'Hello!' -Channel $RTM.Channel }\n                                {@(\"bye\",\"cya\") -contains $_} { Send-SlackMsg -Text 'Goodbye!' -Channel $RTM.Channel }\n\n                                default { Write-Verbose \"I have no response for $_\" }\n                            }\n\n                        }Else{\n                            Write-Verbose \"Message ignored as it wasn't sent to @$($RTMSession.self.name) or in a DM channel\"\n                        }\n                    }\n                    {$_.type -eq 'reconnect_url'} { $RTMSession.URL = $RTM.url }\n\n                    default { Write-Verbose \"No action specified for $($RTM.type) event\" }            \n                }\n</code></pre>\n\n<p>In the above we look for messages that were either in a Direct Message channel (e.g from any user, privately to the Bot user) or were in any public Channel that the Bot has been invited to and where the message includes @<em>botname</em> (where the name of the Bot was determined when we invoked the <code>rtm.start</code> method).</p>\n\n<p>If either of these are true, then a Switch statement looks for opportunities to respond (see below for the <code>Send-SlackMsg</code> function). This is obviously where you extend the Bot with the responses and functionality that you want it to perform.</p>\n\n<p>Here we could also add Switch logic to handle other types of messages, e.g if we wanted the Bot to take some action for any of the other <a href=\"https://api.slack.com/rtm\">RTM Events</a>.</p>\n\n<p><img src=\"/content/images/2017/01/Slack-Bot.png\" alt=\"\" /></p>\n\n<p>The code loops continuously, reading messages from the websocket instantly whenever they are available, until the scripts execution is forcibly stopped:  </p>\n\n<pre><code>            }\n        }   \n    } Until (!$Conn)\n\n}Finally{\n\n    If ($WS) { \n        Write-Verbose \"Closing websocket\"\n        $WS.Dispose()\n    }\n\n}\n</code></pre>\n\n<p>In lieu of any other activity, you'll see the Bot receive a \"reconnect_url\" event every 30 seconds, which I suspect also serves to keep the connection alive.</p>\n\n<h3 id=\"functions\">Functions</h3>\n\n<p>This function uses the established websocket to send simple slack messages back to any Channel specified (including Direct Message channels) so that we can respond to messages:  </p>\n\n<pre><code>Function Send-SlackMsg  \n{\n    [cmdletbinding()]\n    Param(\n        $Text,\n        $Channel,\n        $ID = (get-date).ticks\n    )\n\n    $Prop = @{'id'      = $ID;\n              'type'    = 'message';\n              'text'    = $Text;\n              'channel' = $Channel}\n\n    $Reply = (New-Object –TypeName PSObject –Prop $Prop) | ConvertTo-Json\n\n    $Array = @()\n    $Reply.ToCharArray() | ForEach { $Array += [byte]$_ }          \n    $Reply = New-Object System.ArraySegment[byte]  -ArgumentList @(,$Array)\n\n    $Conn = $WS.SendAsync($Reply, [System.Net.WebSockets.WebSocketMessageType]::Text, [System.Boolean]::TrueString, $CT)\n    While (!$Conn.IsCompleted) { Start-Sleep -Milliseconds 100 }\n\n    Return $ID\n}\n</code></pre>\n\n<p>As noted earlier, we can only reply with the simple message format in this way. To use any other message formatting type (e.g attachments) you'll need to fall back to the Web API.</p>\n\n<p>My basic Bot doesn't make use of the below function (which I did not write), but I wanted to share it as it is useful if you want to convert the timestamp (TS) properties you get with some of the Slack events in to a PowerShell DateTime object. </p>\n\n<pre><code>Function ConvertFrom-UnixTime {  \n    param(\n        [Parameter(Mandatory=$true,ValueFromPipeline=$true)]\n        [Int32]$UnixTime\n    )\n    begin {\n        $startdate = Get-Date –Date '01/01/1970' \n    }\n    process {\n        $timespan = New-Timespan -Seconds $UnixTime\n        $startdate + $timespan\n    }\n}\n</code></pre>\n\n<p>This function is also in the PSSlack module where it's credited as being from <a href=\"http://community.idera.com/powershell/powertips/b/tips/posts/converting-unix-time\">Powershell.com</a>.</p>\n\n<h3 id=\"contributions\">Contributions</h3>\n\n<p>This project is in Github as <a href=\"https://github.com/markwragg/Powershell-SlackBot\">Powershell-SlackBot</a> and I welcome pull requests or suggestions to help improve it. It is now also listed on the <a href=\"https://api.slack.com/community#powershell\">Slack Community page</a> under Powershell.</p>",
                        "image": "/content/images/2017/01/chat-bot-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Powershell Slack Bot using the Real Time Messaging API and Web Sockets",
                        "meta_description": "This post details how PowerShell and .NET can be used to run a custom Slack Bot that utilizes the Slack RTM (Real Time Messaging) API and Web Sockets.",
                        "author_id": 1,
                        "created_at": "2016-12-22 16:39:13",
                        "created_by": 1,
                        "updated_at": "2017-01-05 08:48:05",
                        "updated_by": 1,
                        "published_at": "2017-01-03 16:13:34",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 43,
                        "uuid": "07e21e68-0115-44b2-a21b-2a07ab1cd3c4",
                        "title": "Powershell 101",
                        "slug": "powershell-101",
                        "markdown": "Built-in commands to discover cmdlets and how they work:\n\n- `Get-Help` \n\nWhy doesn't this work? `get-service | get-help`\nWhy does this work? `gcm get-service | get-help`\n\n - update-help\n- `Get-Command`\n- `Get-Member`\n\nCmdlets\n\n- Parameters\n- Default params\n- accept pipeline input\n- Tab complete cmdlet names and parameter names Get-EventL [tab] -Lo [tab] App [tab]\n- -whatif and -confirm\n\nMeasure-object (measure)\nSort-Object (sort)\n\nUse any cmd commands. Note also that some cmd command names are aliased to powershell equivalents: cd, dir (as well as unix equivalents: ls, cat). As a result in these instances you can't use the legacy switches (e.g dir /s you'd need to use dir -recurse or get-childiten -recurse).\n\nverb-noun structure. - This makes commands more discoverable. E.g after you know get-process, you can look up other get-* as well as other *-process with get-help.\n\nGrouped in to modules.\n\nObjects\n\n- Properties\n\nUsing the pipeline\n\n- Chaining commands\n\n\n",
                        "html": "<p>Built-in commands to discover cmdlets and how they work:</p>\n\n<ul>\n<li><code>Get-Help</code> </li>\n</ul>\n\n<p>Why doesn't this work? <code>get-service | get-help</code> <br />\nWhy does this work? <code>gcm get-service | get-help</code></p>\n\n<ul>\n<li>update-help\n<ul><li><code>Get-Command</code></li>\n<li><code>Get-Member</code></li></ul></li>\n</ul>\n\n<p>Cmdlets</p>\n\n<ul>\n<li>Parameters</li>\n<li>Default params</li>\n<li>accept pipeline input</li>\n<li>Tab complete cmdlet names and parameter names Get-EventL [tab] -Lo [tab] App [tab]</li>\n<li>-whatif and -confirm</li>\n</ul>\n\n<p>Measure-object (measure) <br />\nSort-Object (sort)</p>\n\n<p>Use any cmd commands. Note also that some cmd command names are aliased to powershell equivalents: cd, dir (as well as unix equivalents: ls, cat). As a result in these instances you can't use the legacy switches (e.g dir /s you'd need to use dir -recurse or get-childiten -recurse).</p>\n\n<p>verb-noun structure. - This makes commands more discoverable. E.g after you know get-process, you can look up other get-* as well as other *-process with get-help.</p>\n\n<p>Grouped in to modules.</p>\n\n<p>Objects</p>\n\n<ul>\n<li>Properties</li>\n</ul>\n\n<p>Using the pipeline</p>\n\n<ul>\n<li>Chaining commands</li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-01-13 10:24:34",
                        "created_by": 1,
                        "updated_at": "2017-01-13 16:29:15",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 44,
                        "uuid": "7690fe7b-bb70-48b3-b003-ec7cf5b0d377",
                        "title": "Create dynamic PowerShell functions with Parameter Sets",
                        "slug": "create-dynamic-powershell-functions-with-parameter-sets",
                        "markdown": "While developing a PowerShell function to query the API of the webcomic [XKCD](http://xkcd.com) I decided to explore and implement Parameter Sets. These allow you to provide your users with different sets of parameters based on different use cases (assuming you have multiple use cases), which as a result provides a more dynamic set of functionality from a single cmdlet. \n\nYou don't have to use parameter sets to support multiple functionality (your function could just ignore the use of parameters that do not apply) but doing so makes your function more explicit to the user as it ensures that it outright rejects the use of invalid parameter combinations:\n\n![](/content/images/2017/01/Get-XKCD-Invalid-Parameters.png)\n\nIn addition, the `get-help` output for your cmdlet is automatically more explicit/helpful, as I will show later. Input validation is sometimes important and (probably) never a bad thing:\n\n![](http://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n\n> XKCD provides an API that is free to use, returns JSON and is [described here](https://xkcd.com/json.html).\n\n*--PowerShell is by no means the best way to read the XKCD webcomic and on it's own this function is of limited use. However my purpose for this function was to add some fun functionality to my [PowerShell based Slack Bot](http://wragg.io/powershell-slack-bot-using-the-real-time-messaging-api/). I wanted the bot to be able to pull comics directly in to a Slack Channel when requested, perhaps along with the title and image alt text that you get when viewing through [XKCD.com](http://xkcd.com), which is data that the API provides.*\n\n## Use cases\n\nAs I developed my function, I found that I wanted to support various use cases, which (presently) are as follows:\n\n1. Return the latest comic (default behaviour if no parameters passed) or one or more specific comics, by number: `-num`.\n- Return a random comic, from the full range or within a specific range. `-random` `-min` and `-max`.\n- Return the latest x number of comics `-newest`.\n\nAdditionally, I wanted the user to be able to optionally download the image of the comic: `-download` and specify a destination for the download: `-path`.\n\nHere's how those parameters look without organising them in to sets:\n\n```\nParam (\n     [switch]$Random,\n     [int]$Min = 1,\n     [int]$Max = (Invoke-RestMethod http://xkcd.com/info.0.json\").num,        \n     [int]$Newest,\n     [switch]$Download, \n     [string]$Path = $PWD,\n     [int[]]$Num = $Max     \n)\n```\nAs you can see, I've defined types for the parameters (which provides some simple validation that the user input is the right type) and I've specified some defaults where appropriate (including an initial web call that grabs the number of the latest comic).\n\n## Defining my Parameter Sets\n\nI have three use cases, which i'm naming \"specific\", \"random\" and \"newest\". To allocate a parameter to a set, you use `[Parameter(ParameterSetName=’YourName’)]` in front of each parameter, as follows:\n\n```\n   Param (\n        [Parameter(ParameterSetName=’Random’)][switch]$Random,\n        [Parameter(ParameterSetName=’Random’)][int]$Min = 1,\n        [Parameter(ParameterSetName=’Random’)][int]$Max = (Invoke-RestMethod \"http://xkcd.com/info.0.json\").num,        \n        [Parameter(ParameterSetName=’Newest’)][int]$Newest,\n        [switch]$Download,\n        [ValidateScript({Test-Path $_ -PathType ‘Container’})] \n        [string]$Path = $PWD,\n        [Parameter(ParameterSetName=’Specific’,ValueFromPipeline=$True,Position=0)][int[]]$Num = $Max     \n    )\n```\nYou can see that I have not specified a `ParameterSetName` for `$Download` or `$Path` as I want these to be available to all sets (and as a result they are). \n\n*--There is an alternative way to approach this scenario, which is to define those parameters multiple times explicitly in each set. This is obviously more explicit and has the added benefit of allowing you to specify a parameter position order for every parameter in each set, but that felt overkill for my function.*\n\nI have specified a parameter position for `$number` (`Position=0`) as it seemed logical that a user might want to say (for example) `Get-XKCD 123` without having to explicitly use the `-num` parameter name, but for all the other parameters explicit use felt more appropriate/likely.\n\nMy function has a `Process` block and respects the pipeline, so I've defined `$num` as `[int[]]` (note the extra set of square brackets) which means it accepts array input. This means I can return multiple comics at once by doing (for example) `1,2,3 | Get-XKCD` or `Get-XKCD 1..5`. \n\nFinally I decided to validate that when `-Path` is used, a valid folder path is provided. To do that I do this: `[ValidateScript({Test-Path $_ -PathType ‘Container’})]`.\n\nThere's plenty of further options to do more explicit parameter validation, but I felt I'd taken it far enough for my function (to balance sensible functionality against bloated code).\n\n## Get-Help\n\nAs mentioned earlier, aside from ensuring your function rejects odd combinations of parameters, another benefit to doing this is a more explicit help output by default. Without having defined any explicit Help text, `Get-Help Get-XKCD` returns this:\n\n![](/content/images/2017/01/Get-Help-Get-XKCD.png)\n\nWhich as you can see, defines the three different use cases under \"Syntax\". Pretty cool :).\n\n## The full code\n\nI've created a project in GitHub for this: [Powershell-XKCD](https://github.com/markwragg/Powershell-XKCD/), as i'm considering adding further functionality and other XKCD related cmdlets. Check it out on [GitHub](https://github.com/markwragg/Powershell-XKCD/blob/master/XKCD.psm1), or via the embedded the code below:\n\n<script src=\"http://gist-it.appspot.com/github/markwragg/Powershell-XKCD/blob/master/XKCD.psm1\"></script>\n\n## Usage examples\n\nHere are some usage examples as detailed in [readme.md](https://github.com/markwragg/Powershell-XKCD/blob/master/README.md).\n\n> XKCD is a webcomic by Randall Munroe. Please respect the license of his work as described here: http://xkcd.com/license.html.\n\n1) `Get-XKCD`\nBy default (and with no specified parameters) the function will return a PowerShell object with the details of the latest webcomic. For example:\n\n```\nmonth      : 1\nnum        : 1786\nlink       : \nyear       : 2017\nnews       : \nsafe_title : Trash\ntranscript : \nalt        : Plus, time's all weird in there, so most of it probably broke down and decomposed hundreds of years ago. Which reminds me, I've been meaning to get in touch \n             with Yucca Mountain to see if they're interested in a partnership.\nimg        : http://imgs.xkcd.com/comics/trash.png\ntitle      : Trash\nday        : 16\n```\n\n2) `Get-XKCD 1` or `Get-XKCD -num 1`\nSpecify the number of specifc comic/s you want to access via the -num parameter (this is a positional parameter so it doesn't need to be explicitly used).\n\n3) `Get-XKCD -Random` or `Get-XKCD -Random -Min 1 -Max 10`\nUse the -Random switch to get a Random comic. Optionally specify Min and Max if you want to restrict the randomisation to a specific range of comic numbers.\n\n4) `Get-XKCD -Newest 5`\nUse the -Newest switch to get a specified number of the newest comics. Note this cannot be used with -Random (and vice versa).\n\n5) `Get-XKCD 1,5,10` or `10..20 | Get-XKCD`\nThe number paramater accepts array input and pipeline input, so you can use either to return a specific selection in one hit.\n\n6) `Get-XKCD -Download` or `Get-XKCD 1337 -Download -Path C:\\XKCD`\nUse the -Download switch to download the image/s of the returned comics. Optionally specify a path to download to, by default it uses the current directory. Note you can use -Download and -Path with any of the other parameters.\n\n7) `1..10 | % { Get-XKCD -Random -min 1 -max 100 | select num,img } | FT -AutoSize`\nThis calls Get-XKCD 10 times in a foreach loop, returning the number and image URL of 10 random comics from the first 100 comics and presenting them as an autosized table.\n\n## Further reading\n\nThere's lots of other great articles online about Parameter Sets as well as other ways to create dynamic parameters or add further validation:\n\n- [Powershell functions and Parameter Sets](http://blog.simonw.se/powershell-functions-and-parameter-sets/)\n- [Add a Parameter to Multiple Parameter Sets in PowerShell](http://www.jonathanmedd.net/2013/01/add-a-parameter-to-multiple-parameter-sets-in-powershell.html)\n- [How To Implement Dynamic Parameters in Your PowerShell Functions](https://mcpmag.com/articles/2016/10/06/implement-dynamic-parameters.aspx?m=1)",
                        "html": "<p>While developing a PowerShell function to query the API of the webcomic <a href=\"http://xkcd.com\">XKCD</a> I decided to explore and implement Parameter Sets. These allow you to provide your users with different sets of parameters based on different use cases (assuming you have multiple use cases), which as a result provides a more dynamic set of functionality from a single cmdlet. </p>\n\n<p>You don't have to use parameter sets to support multiple functionality (your function could just ignore the use of parameters that do not apply) but doing so makes your function more explicit to the user as it ensures that it outright rejects the use of invalid parameter combinations:</p>\n\n<p><img src=\"/content/images/2017/01/Get-XKCD-Invalid-Parameters.png\" alt=\"\" /></p>\n\n<p>In addition, the <code>get-help</code> output for your cmdlet is automatically more explicit/helpful, as I will show later. Input validation is sometimes important and (probably) never a bad thing:</p>\n\n<p><img src=\"http://imgs.xkcd.com/comics/exploits_of_a_mom.png\" alt=\"\" /></p>\n\n<blockquote>\n  <p>XKCD provides an API that is free to use, returns JSON and is <a href=\"https://xkcd.com/json.html\">described here</a>.</p>\n</blockquote>\n\n<p><em>--PowerShell is by no means the best way to read the XKCD webcomic and on it's own this function is of limited use. However my purpose for this function was to add some fun functionality to my <a href=\"http://wragg.io/powershell-slack-bot-using-the-real-time-messaging-api/\">PowerShell based Slack Bot</a>. I wanted the bot to be able to pull comics directly in to a Slack Channel when requested, perhaps along with the title and image alt text that you get when viewing through <a href=\"http://xkcd.com\">XKCD.com</a>, which is data that the API provides.</em></p>\n\n<h2 id=\"usecases\">Use cases</h2>\n\n<p>As I developed my function, I found that I wanted to support various use cases, which (presently) are as follows:</p>\n\n<ol>\n<li>Return the latest comic (default behaviour if no parameters passed) or one or more specific comics, by number: <code>-num</code>.  </li>\n<li>Return a random comic, from the full range or within a specific range. <code>-random</code> <code>-min</code> and <code>-max</code>.</li>\n<li>Return the latest x number of comics <code>-newest</code>.</li>\n</ol>\n\n<p>Additionally, I wanted the user to be able to optionally download the image of the comic: <code>-download</code> and specify a destination for the download: <code>-path</code>.</p>\n\n<p>Here's how those parameters look without organising them in to sets:</p>\n\n<pre><code>Param (  \n     [switch]$Random,\n     [int]$Min = 1,\n     [int]$Max = (Invoke-RestMethod http://xkcd.com/info.0.json\").num,        \n     [int]$Newest,\n     [switch]$Download, \n     [string]$Path = $PWD,\n     [int[]]$Num = $Max     \n)\n</code></pre>\n\n<p>As you can see, I've defined types for the parameters (which provides some simple validation that the user input is the right type) and I've specified some defaults where appropriate (including an initial web call that grabs the number of the latest comic).</p>\n\n<h2 id=\"definingmyparametersets\">Defining my Parameter Sets</h2>\n\n<p>I have three use cases, which i'm naming \"specific\", \"random\" and \"newest\". To allocate a parameter to a set, you use <code>[Parameter(ParameterSetName=’YourName’)]</code> in front of each parameter, as follows:</p>\n\n<pre><code>   Param (\n        [Parameter(ParameterSetName=’Random’)][switch]$Random,\n        [Parameter(ParameterSetName=’Random’)][int]$Min = 1,\n        [Parameter(ParameterSetName=’Random’)][int]$Max = (Invoke-RestMethod \"http://xkcd.com/info.0.json\").num,        \n        [Parameter(ParameterSetName=’Newest’)][int]$Newest,\n        [switch]$Download,\n        [ValidateScript({Test-Path $_ -PathType ‘Container’})] \n        [string]$Path = $PWD,\n        [Parameter(ParameterSetName=’Specific’,ValueFromPipeline=$True,Position=0)][int[]]$Num = $Max     \n    )\n</code></pre>\n\n<p>You can see that I have not specified a <code>ParameterSetName</code> for <code>$Download</code> or <code>$Path</code> as I want these to be available to all sets (and as a result they are). </p>\n\n<p><em>--There is an alternative way to approach this scenario, which is to define those parameters multiple times explicitly in each set. This is obviously more explicit and has the added benefit of allowing you to specify a parameter position order for every parameter in each set, but that felt overkill for my function.</em></p>\n\n<p>I have specified a parameter position for <code>$number</code> (<code>Position=0</code>) as it seemed logical that a user might want to say (for example) <code>Get-XKCD 123</code> without having to explicitly use the <code>-num</code> parameter name, but for all the other parameters explicit use felt more appropriate/likely.</p>\n\n<p>My function has a <code>Process</code> block and respects the pipeline, so I've defined <code>$num</code> as <code>[int[]]</code> (note the extra set of square brackets) which means it accepts array input. This means I can return multiple comics at once by doing (for example) <code>1,2,3 | Get-XKCD</code> or <code>Get-XKCD 1..5</code>. </p>\n\n<p>Finally I decided to validate that when <code>-Path</code> is used, a valid folder path is provided. To do that I do this: <code>[ValidateScript({Test-Path $_ -PathType ‘Container’})]</code>.</p>\n\n<p>There's plenty of further options to do more explicit parameter validation, but I felt I'd taken it far enough for my function (to balance sensible functionality against bloated code).</p>\n\n<h2 id=\"gethelp\">Get-Help</h2>\n\n<p>As mentioned earlier, aside from ensuring your function rejects odd combinations of parameters, another benefit to doing this is a more explicit help output by default. Without having defined any explicit Help text, <code>Get-Help Get-XKCD</code> returns this:</p>\n\n<p><img src=\"/content/images/2017/01/Get-Help-Get-XKCD.png\" alt=\"\" /></p>\n\n<p>Which as you can see, defines the three different use cases under \"Syntax\". Pretty cool :).</p>\n\n<h2 id=\"thefullcode\">The full code</h2>\n\n<p>I've created a project in GitHub for this: <a href=\"https://github.com/markwragg/Powershell-XKCD/\">Powershell-XKCD</a>, as i'm considering adding further functionality and other XKCD related cmdlets. Check it out on <a href=\"https://github.com/markwragg/Powershell-XKCD/blob/master/XKCD.psm1\">GitHub</a>, or via the embedded the code below:</p>\n\n<script src=\"http://gist-it.appspot.com/github/markwragg/Powershell-XKCD/blob/master/XKCD.psm1\"></script>\n\n<h2 id=\"usageexamples\">Usage examples</h2>\n\n<p>Here are some usage examples as detailed in <a href=\"https://github.com/markwragg/Powershell-XKCD/blob/master/README.md\">readme.md</a>.</p>\n\n<blockquote>\n  <p>XKCD is a webcomic by Randall Munroe. Please respect the license of his work as described here: <a href=\"http://xkcd.com/license.html\">http://xkcd.com/license.html</a>.</p>\n</blockquote>\n\n<p>1) <code>Get-XKCD</code> <br />\nBy default (and with no specified parameters) the function will return a PowerShell object with the details of the latest webcomic. For example:</p>\n\n<pre><code>month      : 1  \nnum        : 1786  \nlink       :  \nyear       : 2017  \nnews       :  \nsafe_title : Trash  \ntranscript :  \nalt        : Plus, time's all weird in there, so most of it probably broke down and decomposed hundreds of years ago. Which reminds me, I've been meaning to get in touch  \n             with Yucca Mountain to see if they're interested in a partnership.\nimg        : http://imgs.xkcd.com/comics/trash.png  \ntitle      : Trash  \nday        : 16  \n</code></pre>\n\n<p>2) <code>Get-XKCD 1</code> or <code>Get-XKCD -num 1</code> <br />\nSpecify the number of specifc comic/s you want to access via the -num parameter (this is a positional parameter so it doesn't need to be explicitly used).</p>\n\n<p>3) <code>Get-XKCD -Random</code> or <code>Get-XKCD -Random -Min 1 -Max 10</code> <br />\nUse the -Random switch to get a Random comic. Optionally specify Min and Max if you want to restrict the randomisation to a specific range of comic numbers.</p>\n\n<p>4) <code>Get-XKCD -Newest 5</code> <br />\nUse the -Newest switch to get a specified number of the newest comics. Note this cannot be used with -Random (and vice versa).</p>\n\n<p>5) <code>Get-XKCD 1,5,10</code> or <code>10..20 | Get-XKCD</code> <br />\nThe number paramater accepts array input and pipeline input, so you can use either to return a specific selection in one hit.</p>\n\n<p>6) <code>Get-XKCD -Download</code> or <code>Get-XKCD 1337 -Download -Path C:\\XKCD</code> <br />\nUse the -Download switch to download the image/s of the returned comics. Optionally specify a path to download to, by default it uses the current directory. Note you can use -Download and -Path with any of the other parameters.</p>\n\n<p>7) <code>1..10 | % { Get-XKCD -Random -min 1 -max 100 | select num,img } | FT -AutoSize</code> <br />\nThis calls Get-XKCD 10 times in a foreach loop, returning the number and image URL of 10 random comics from the first 100 comics and presenting them as an autosized table.</p>\n\n<h2 id=\"furtherreading\">Further reading</h2>\n\n<p>There's lots of other great articles online about Parameter Sets as well as other ways to create dynamic parameters or add further validation:</p>\n\n<ul>\n<li><a href=\"http://blog.simonw.se/powershell-functions-and-parameter-sets/\">Powershell functions and Parameter Sets</a></li>\n<li><a href=\"http://www.jonathanmedd.net/2013/01/add-a-parameter-to-multiple-parameter-sets-in-powershell.html\">Add a Parameter to Multiple Parameter Sets in PowerShell</a></li>\n<li><a href=\"https://mcpmag.com/articles/2016/10/06/implement-dynamic-parameters.aspx?m=1\">How To Implement Dynamic Parameters in Your PowerShell Functions</a></li>\n</ul>",
                        "image": "/content/images/2017/01/xkcd-print-crop.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-01-17 20:46:38",
                        "created_by": 1,
                        "updated_at": "2017-01-18 20:44:07",
                        "updated_by": 1,
                        "published_at": "2017-01-18 20:00:33",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 45,
                        "uuid": "dbf8aa4b-a772-4328-aed5-c943a1a52c8a",
                        "title": "Using AppVeyor to build, test and deploy a PowerShell project",
                        "slug": "using-appveyor-to-build-test-and-deploy-a-powershell-project",
                        "markdown": "I have wanted to explore the topic of Continuous Integration (and particularly how it might apply to PowerShell) since reading the excellent [Release Pipeline Model Whitepaper](http://download.microsoft.com/download/C/4/A/C4A14099-FEA4-4CB3-8A8F-A0C2BE5A1219/The%20Release%20Pipeline%20Model.pdf) by Michael Greene and Steven Murawski.\n\n![](/content/images/2017/01/release-pipeline-build.png)\n\nI am relatively comfortable with Source Control (and it's many benefits) and I've played a fair bit with Pester so I have a framework for testing. However I've struggled to understand how the \"Build\" or \"Release\" phases might apply to my (typically) administrative scripts. What eventually encouraged me to dig deeper was a [friend](http://sammart.in) suggesting I put my latest project in to the PowerShell Gallery and it therefore seemed sensible to have an automated process that would ensure committing to Github would also trigger a publish to the Gallery and (more importantly) that changes to both still resulted in clean, functional code.\n\n*--Confession time: I have published to the PowerShell Gallery once before with my AD Audit project. Since doing that I've almost certainly made changes to the project in Github and I recently accepted a (much appreciated) pull request, neither of which I remembered to push in to the Gallery. Additionally, I definitely didn't perform any testing of the code after the latter.*\n\n## AppVeyor\n\nI recall looking at AppVeyor briefly before and dismissing it because I didn't think there was a free option as well as I was typically working on scripts that I couldn't easily open source.\n\nThere is a free option f..\n\n## Todo List\n\n- I~~mplement PSScriptAnalyzer in the pipeline.~~\n- Flesh out Pester tests (unit + integration - separate files?)\n- ~~Implement deployment in to PSGallery~~\n  - ~~Needs to factor in incrementing build #. Take that # from the build number used in AppVeyor? Be good if they match.~~\n- ~~Implement Badge on readme.md~~\n- Blog about how the green tick next to the commits was already helpful :)\n- Look in to using PSake -- alternative to appveyor.yml as a build script?\n- Merge together the 4 appveyor ps files? Need to then update the yml file.\n- ~~make it so it only publishes the module if it's changed. Work out best way to do this? Can we get some sort of hash?~~\n\n## Links\n- http://stackoverflow.com/questions/41184979/how-to-publish-a-powershell-module-to-powershellgallery-with-appveyor\n- http://ramblingcookiemonster.github.io/GitHub-Pester-AppVeyor/#appveyor\n- http://ramblingcookiemonster.github.io/Github-Pester-AppVeyor-Part-2/\n\nhttps://4sysops.com/archives/unit-tests-versus-integration-tests-in-pester/\n\nhttps://blogs.msdn.microsoft.com/jtarquino/2016/10/15/powershell-module-with-continuous-integration-static-analysis-and-automatic-publish-to-gallery/\n\nhttps://www.appveyor.com/docs/appveyor-yml/\n\nhttps://ci.appveyor.com/project/markwragg/powershell-xkcd\n\nhttp://javydekoning.com/getting-started-with-appveyor/",
                        "html": "<p>I have wanted to explore the topic of Continuous Integration (and particularly how it might apply to PowerShell) since reading the excellent <a href=\"http://download.microsoft.com/download/C/4/A/C4A14099-FEA4-4CB3-8A8F-A0C2BE5A1219/The%20Release%20Pipeline%20Model.pdf\">Release Pipeline Model Whitepaper</a> by Michael Greene and Steven Murawski.</p>\n\n<p><img src=\"/content/images/2017/01/release-pipeline-build.png\" alt=\"\" /></p>\n\n<p>I am relatively comfortable with Source Control (and it's many benefits) and I've played a fair bit with Pester so I have a framework for testing. However I've struggled to understand how the \"Build\" or \"Release\" phases might apply to my (typically) administrative scripts. What eventually encouraged me to dig deeper was a <a href=\"http://sammart.in\">friend</a> suggesting I put my latest project in to the PowerShell Gallery and it therefore seemed sensible to have an automated process that would ensure committing to Github would also trigger a publish to the Gallery and (more importantly) that changes to both still resulted in clean, functional code.</p>\n\n<p><em>--Confession time: I have published to the PowerShell Gallery once before with my AD Audit project. Since doing that I've almost certainly made changes to the project in Github and I recently accepted a (much appreciated) pull request, neither of which I remembered to push in to the Gallery. Additionally, I definitely didn't perform any testing of the code after the latter.</em></p>\n\n<h2 id=\"appveyor\">AppVeyor</h2>\n\n<p>I recall looking at AppVeyor briefly before and dismissing it because I didn't think there was a free option as well as I was typically working on scripts that I couldn't easily open source.</p>\n\n<p>There is a free option f..</p>\n\n<h2 id=\"todolist\">Todo List</h2>\n\n<ul>\n<li>I<del>mplement PSScriptAnalyzer in the pipeline.</del></li>\n<li>Flesh out Pester tests (unit + integration - separate files?)</li>\n<li><del>Implement deployment in to PSGallery</del>\n<ul><li><del>Needs to factor in incrementing build #. Take that # from the build number used in AppVeyor? Be good if they match.</del></li></ul></li>\n<li><del>Implement Badge on readme.md</del></li>\n<li>Blog about how the green tick next to the commits was already helpful :)</li>\n<li>Look in to using PSake -- alternative to appveyor.yml as a build script?</li>\n<li>Merge together the 4 appveyor ps files? Need to then update the yml file.</li>\n<li><del>make it so it only publishes the module if it's changed. Work out best way to do this? Can we get some sort of hash?</del></li>\n</ul>\n\n<h2 id=\"links\">Links</h2>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/41184979/how-to-publish-a-powershell-module-to-powershellgallery-with-appveyor\">http://stackoverflow.com/questions/41184979/how-to-publish-a-powershell-module-to-powershellgallery-with-appveyor</a></li>\n<li><a href=\"http://ramblingcookiemonster.github.io/GitHub-Pester-AppVeyor/#appveyor\">http://ramblingcookiemonster.github.io/GitHub-Pester-AppVeyor/#appveyor</a></li>\n<li><a href=\"http://ramblingcookiemonster.github.io/Github-Pester-AppVeyor-Part-2/\">http://ramblingcookiemonster.github.io/Github-Pester-AppVeyor-Part-2/</a></li>\n</ul>\n\n<p><a href=\"https://4sysops.com/archives/unit-tests-versus-integration-tests-in-pester/\">https://4sysops.com/archives/unit-tests-versus-integration-tests-in-pester/</a></p>\n\n<p><a href=\"https://blogs.msdn.microsoft.com/jtarquino/2016/10/15/powershell-module-with-continuous-integration-static-analysis-and-automatic-publish-to-gallery/\">https://blogs.msdn.microsoft.com/jtarquino/2016/10/15/powershell-module-with-continuous-integration-static-analysis-and-automatic-publish-to-gallery/</a></p>\n\n<p><a href=\"https://www.appveyor.com/docs/appveyor-yml/\">https://www.appveyor.com/docs/appveyor-yml/</a></p>\n\n<p><a href=\"https://ci.appveyor.com/project/markwragg/powershell-xkcd\">https://ci.appveyor.com/project/markwragg/powershell-xkcd</a></p>\n\n<p><a href=\"http://javydekoning.com/getting-started-with-appveyor/\">http://javydekoning.com/getting-started-with-appveyor/</a></p>",
                        "image": "/content/images/2017/01/Continuous-Narrow.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-01-20 10:47:13",
                        "created_by": 1,
                        "updated_at": "2017-01-26 15:55:18",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 46,
                        "uuid": "6954d694-2805-4c35-9120-0ffc5f37458b",
                        "title": "Issue uploading Pester test results in to AppVeyor",
                        "slug": "issue-uploading-pester-test-results-in-to-appveyor",
                        "markdown": "While experimenting with [AppVeyor](https://www.appveyor.com) to add Continuous Integration to my PowerShell projects (more detailed blog post to follow) I encountered a bug that meant that the [Pester](https://github.com/pester/Pester) test results (uploaded via the NUnit formatted results file) were not appearing in the \"Tests\" tab of my AppVeyor project.\n\n*-- **Update**: The [issue has now been closed](https://github.com/appveyor/ci/issues/1271) by the AppVeyor dev team and I've confirmed that the workaround described below is no longer necessary.*\n\n# The issue\n\nIn brief, doing this in isolation doesn't currently work for results generated by the latest version of Pester:\n\n```\n$wc = New-Object 'System.Net.WebClient'\n$wc.UploadFile(\"https://ci.appveyor.com/api/testresults/nunit/$($env:APPVEYOR_JOB_ID)\", (Resolve-Path .\\YourResultsFile.xml))\n```\nNo error is generated, the Tests tab in AppVeyor just says \"The build job has not produced any test results.\", although you can still see your results in the Console.\n\n![AppVeyor - The build job has not produced and test results](/content/images/2017/01/appveyor-no-test-results.png)\n\nThis appears to be due to a change in Pester (3.4.6 was recently released) and a case sensitivity issue which requires a change on AppVeyor's side to fix as discussed here:\n\n> http://help.appveyor.com/discussions/problems/5906-pester-test-not-showing-in-test-tab\n\n## The workaround\n\nIn the meantime, you can workaround the issue by doing the following after Pester has run but before you upload the result file/s:\n\n```\n[xml]$content = Get-Content \"C:\\projects\\YourProject\\YourResultsFile.xml\"\n$content.'test-results'.'test-suite'.type = \"Powershell\"\n$content.Save(\"C:\\projects\\YourProject\\YourResultsFile.xml\")\n```\n\nWatch this issue in Github to know when you can remove that workaround:\n\n> https://github.com/appveyor/ci/issues/1271\n\nIt's also worth noting that this issue doesn't break your builds, so if you can live without results in the Tests tab in the meantime you can just wait for it to be fixed.\n\n## A side note about AppVeyor support \n*--TL;DR - It's excellent.*\n\nI spent quite a while yesterday evening struggling with this issue as I assumed (because I was very new to AppVeyor) that something was wrong in my appveyor.yml file or due to some other eccentricity of my code/project. Shortly after midnight I gave up trying, sent an email to the AppVeyor support team for help and went to bed.\n\nThis morning I saw that [Ilya Finkelshteyn](https://github.com/IlyaFinkelshteyn) from AppVeyor had replied (within about an hour of me sending my email) with the information above. Further, he had forked my code in Github to confirm the workaround worked. I sent a reply this morning with some follow up questions and got another almost instantaneous response.\n\nConsidering I am a casual (and brand new) user on the free plan I was grateful to be entitled to support at all, but this really impressed me for both speed and thoroughness. As a first impression, it was a great one. :)",
                        "html": "<p>While experimenting with <a href=\"https://www.appveyor.com\">AppVeyor</a> to add Continuous Integration to my PowerShell projects (more detailed blog post to follow) I encountered a bug that meant that the <a href=\"https://github.com/pester/Pester\">Pester</a> test results (uploaded via the NUnit formatted results file) were not appearing in the \"Tests\" tab of my AppVeyor project.</p>\n\n<p><em>-- <strong>Update</strong>: The <a href=\"https://github.com/appveyor/ci/issues/1271\">issue has now been closed</a> by the AppVeyor dev team and I've confirmed that the workaround described below is no longer necessary.</em></p>\n\n<h1 id=\"theissue\">The issue</h1>\n\n<p>In brief, doing this in isolation doesn't currently work for results generated by the latest version of Pester:</p>\n\n<pre><code>$wc = New-Object 'System.Net.WebClient'\n$wc.UploadFile(\"https://ci.appveyor.com/api/testresults/nunit/$($env:APPVEYOR_JOB_ID)\", (Resolve-Path .\\YourResultsFile.xml))\n</code></pre>\n\n<p>No error is generated, the Tests tab in AppVeyor just says \"The build job has not produced any test results.\", although you can still see your results in the Console.</p>\n\n<p><img src=\"/content/images/2017/01/appveyor-no-test-results.png\" alt=\"AppVeyor - The build job has not produced and test results\" /></p>\n\n<p>This appears to be due to a change in Pester (3.4.6 was recently released) and a case sensitivity issue which requires a change on AppVeyor's side to fix as discussed here:</p>\n\n<blockquote>\n  <p><a href=\"http://help.appveyor.com/discussions/problems/5906-pester-test-not-showing-in-test-tab\">http://help.appveyor.com/discussions/problems/5906-pester-test-not-showing-in-test-tab</a></p>\n</blockquote>\n\n<h2 id=\"theworkaround\">The workaround</h2>\n\n<p>In the meantime, you can workaround the issue by doing the following after Pester has run but before you upload the result file/s:</p>\n\n<pre><code>[xml]$content = Get-Content \"C:\\projects\\YourProject\\YourResultsFile.xml\"\n$content.'test-results'.'test-suite'.type = \"Powershell\"\n$content.Save(\"C:\\projects\\YourProject\\YourResultsFile.xml\")\n</code></pre>\n\n<p>Watch this issue in Github to know when you can remove that workaround:</p>\n\n<blockquote>\n  <p><a href=\"https://github.com/appveyor/ci/issues/1271\">https://github.com/appveyor/ci/issues/1271</a></p>\n</blockquote>\n\n<p>It's also worth noting that this issue doesn't break your builds, so if you can live without results in the Tests tab in the meantime you can just wait for it to be fixed.</p>\n\n<h2 id=\"asidenoteaboutappveyorsupport\">A side note about AppVeyor support</h2>\n\n<p><em>--TL;DR - It's excellent.</em></p>\n\n<p>I spent quite a while yesterday evening struggling with this issue as I assumed (because I was very new to AppVeyor) that something was wrong in my appveyor.yml file or due to some other eccentricity of my code/project. Shortly after midnight I gave up trying, sent an email to the AppVeyor support team for help and went to bed.</p>\n\n<p>This morning I saw that <a href=\"https://github.com/IlyaFinkelshteyn\">Ilya Finkelshteyn</a> from AppVeyor had replied (within about an hour of me sending my email) with the information above. Further, he had forked my code in Github to confirm the workaround worked. I sent a reply this morning with some follow up questions and got another almost instantaneous response.</p>\n\n<p>Considering I am a casual (and brand new) user on the free plan I was grateful to be entitled to support at all, but this really impressed me for both speed and thoroughness. As a first impression, it was a great one. :)</p>",
                        "image": "/content/images/2017/01/bug.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-01-20 13:17:28",
                        "created_by": 1,
                        "updated_at": "2017-02-20 09:36:13",
                        "updated_by": 1,
                        "published_at": "2017-01-20 13:46:28",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 48,
                        "uuid": "db19da61-6547-4a2b-b850-4bb1540b8a1e",
                        "title": "Learn PowerShell in a Month of Lunches",
                        "slug": "tilfmol-things-i-learnt-from-learn-powershell-in-a-month-of-lunches",
                        "markdown": "I got started with PowerShell thanks to the encouragement and enthusiasm of a [friend and former colleague](http://sammart.in). Following that, I've largely developed my knowledge through the standard combinations of trial + error + googling and more recently (as is evident) by doing my best to engage with and give back to the community.\n\n[![XKCD: Code Quality](https://imgs.xkcd.com/comics/code_quality.png)](https://xkcd.com/1513/)\n\nWhen I meet people who are new to PowerShell and they ask me to advise them on [resources to help them get started](http://wragg.io/getting-started-with-powershell/) I often mention the [Learn PowerShell in a Month of Lunches](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition) book (which recently released as a 3rd edition) mostly because I've seen it recommended by others in the community. However I personally had not read it and that seemed.. well.. wrong.\n\nEqually it may seem a little bizarre that I read it now, but what I found was doing so helped me to confirm some things I knew (which has given me more confidence when attempting to explain them to others) and has clarified some things I was murky about. It also taught me a few things I didn't know at all and I was surprised (and impressed) by the depth and inclusion of some topics which I didn't expect in a book (I always assumed is) aimed at beginners.\n\n*-- When I put together the original list of topics I wanted to cover (see below)  I decided it was too long for a single blog post. I also found that they fell in to four broad topics and so to make things more specific for future googlers, each major topic is getting a blog post of it's own which I will link to when they are ready.*\n\nWithout further ado, here are some things I learnt from reading Learn PowerShell in a Month of Lunches (#TILFMOL):\n\n- #1 [The PowerShell Pipeline](http://wragg.io/tilfmol1-the-powershell-pipeline/)\n  - How pipeline input knows where to go\n  - If you can't get input to a parameter via the pipeline, there is another way\n  - One script one pipeline\n- #2 [PowerShell Help](http://wragg.io/tilfmol-2-powershell-help/)\n  - Help vs Get-Help\n  - The meaning of square brackets in help:\n     - Optional/Positional Parameters\n     - Array Input\n- #3 [PowerShell Remoting](http://wragg.io/tilfmol-3-powershell-remoting/)\n  - Executing remote commands on one or many machines\n  - Deserialised objects are the result of commands\n  - Creating Endpoints\n- #4 [PowerShell Jobs](http://wragg.io/tilfmol-4-powershell-jobs/)\n  - Local/background jobs\n  - Executing remote jobs with `Invoke-Command`\n  - Scheduled jobs\n\n---\n\nHappily, I can now confidently and personally recommend [Learn PowerShell in a Month of Lunches](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition), not only for beginners but to anyone looking to validate and further their knowledge of PowerShell.",
                        "html": "<p>I got started with PowerShell thanks to the encouragement and enthusiasm of a <a href=\"http://sammart.in\">friend and former colleague</a>. Following that, I've largely developed my knowledge through the standard combinations of trial + error + googling and more recently (as is evident) by doing my best to engage with and give back to the community.</p>\n\n<p><a href=\"https://xkcd.com/1513/\"><img src=\"https://imgs.xkcd.com/comics/code_quality.png\" alt=\"XKCD: Code Quality\" title=\"\" /></a></p>\n\n<p>When I meet people who are new to PowerShell and they ask me to advise them on <a href=\"http://wragg.io/getting-started-with-powershell/\">resources to help them get started</a> I often mention the <a href=\"https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition\">Learn PowerShell in a Month of Lunches</a> book (which recently released as a 3rd edition) mostly because I've seen it recommended by others in the community. However I personally had not read it and that seemed.. well.. wrong.</p>\n\n<p>Equally it may seem a little bizarre that I read it now, but what I found was doing so helped me to confirm some things I knew (which has given me more confidence when attempting to explain them to others) and has clarified some things I was murky about. It also taught me a few things I didn't know at all and I was surprised (and impressed) by the depth and inclusion of some topics which I didn't expect in a book (I always assumed is) aimed at beginners.</p>\n\n<p><em>-- When I put together the original list of topics I wanted to cover (see below)  I decided it was too long for a single blog post. I also found that they fell in to four broad topics and so to make things more specific for future googlers, each major topic is getting a blog post of it's own which I will link to when they are ready.</em></p>\n\n<p>Without further ado, here are some things I learnt from reading Learn PowerShell in a Month of Lunches (#TILFMOL):</p>\n\n<ul>\n<li>#1 <a href=\"http://wragg.io/tilfmol1-the-powershell-pipeline/\">The PowerShell Pipeline</a>\n<ul><li>How pipeline input knows where to go</li>\n<li>If you can't get input to a parameter via the pipeline, there is another way</li>\n<li>One script one pipeline</li></ul></li>\n<li>#2 <a href=\"http://wragg.io/tilfmol-2-powershell-help/\">PowerShell Help</a>\n<ul><li>Help vs Get-Help</li>\n<li>The meaning of square brackets in help:\n<ul><li>Optional/Positional Parameters</li>\n<li>Array Input</li></ul></li></ul></li>\n<li>#3 <a href=\"http://wragg.io/tilfmol-3-powershell-remoting/\">PowerShell Remoting</a>\n<ul><li>Executing remote commands on one or many machines</li>\n<li>Deserialised objects are the result of commands</li>\n<li>Creating Endpoints</li></ul></li>\n<li>#4 <a href=\"http://wragg.io/tilfmol-4-powershell-jobs/\">PowerShell Jobs</a>\n<ul><li>Local/background jobs</li>\n<li>Executing remote jobs with <code>Invoke-Command</code></li>\n<li>Scheduled jobs</li></ul></li>\n</ul>\n\n<hr />\n\n<p>Happily, I can now confidently and personally recommend <a href=\"https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition\">Learn PowerShell in a Month of Lunches</a>, not only for beginners but to anyone looking to validate and further their knowledge of PowerShell.</p>",
                        "image": "/content/images/2017/02/book-med.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Things I learnt from reading Learn PowerShell in a Month of Lunches",
                        "meta_description": "",
                        "author_id": 1,
                        "created_at": "2017-02-16 15:22:48",
                        "created_by": 1,
                        "updated_at": "2017-06-20 07:49:13",
                        "updated_by": 1,
                        "published_at": "2017-02-23 15:07:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 49,
                        "uuid": "b038d8a9-dc03-4b1b-832a-8f340e8412b5",
                        "title": "PowerShell Remoting over the Internet",
                        "slug": "powershell-remoting-over-the-internet",
                        "markdown": "https://4sysops.com/archives/powershell-remoting-over-https-with-a-self-signed-ssl-certificate/",
                        "html": "<p><a href=\"https://4sysops.com/archives/powershell-remoting-over-https-with-a-self-signed-ssl-certificate/\">https://4sysops.com/archives/powershell-remoting-over-https-with-a-self-signed-ssl-certificate/</a></p>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-02-21 16:04:23",
                        "created_by": 1,
                        "updated_at": "2017-02-23 12:46:40",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 51,
                        "uuid": "335b1321-df77-4e5a-a677-a1b0379caa7f",
                        "title": "TILFMOL #1 - The PowerShell Pipeline",
                        "slug": "tilfmol1-the-powershell-pipeline",
                        "markdown": "As introduced in my [previous post](http://wragg.io/tilfmol-things-i-learnt-from-learn-powershell-in-a-month-of-lunches/), this is part one of a four part series that documents some things I discovered or had clarified by reading the excellent [Learn PowerShell in a Month of Lunches](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition) book (recently released in 3rd edition).\n\nThis post covers some things I learnt about the PowerShell pipeline, which include:\n\n- How pipeline input knows where to go\n- If you can't get input to a property via the pipeline there's another way\n- the concept of 'one script one pipeline'\n\n\n# How pipeline input knows where to go\n\nLike 'how does something as heavy as an aeroplane stay in the sky?', pipeline input was something I was glad worked but tried not to think about too deeply. In fact it hadn't really seemed important, certain (similar named) cmdlets obviously work together (`get-service 'myservice' | stop-service`), it didn't seem to matter how. And then where it was less obvious how cmdlets might go together I had somehow learnt getting pipeline input in to a cmdlet was a matter of matching the relevant output and input types. \n\nThe (more in depth) answer is apparently, one of two ways:\n\n**1\\. By value (ByValue).** A cmdlet may have a single parameter of any particular type that accepts pipeline input by value. That is to say (for example) if there are three properties that accept string input, only one of them may accept pipeline input ByValue (but a single property of each different type can also accept input ByValue). When you send pipeline input to the cmdlet, if the input is of a matching type then that's the parameter it is bound to.\n\nYou can see if any parameters accept input ByValue in `get-help -full`\n\nFor example `Stop-Process` has the following parameter:\n```\n-InputObject <Process[]>\n    Specifies the process objects to stop.\n\n    Accept pipeline input?       True (ByValue)\n```\nThe parameter `-InputObject` expects one or more Process objects. The `get-process` cmdlet creates Process objects (as you can see via `get-process | get-member`):\n```\n   TypeName: System.Diagnostics.Process\n```\nAs as such, when piped they are bound to the `-InputObject` parameter of Stop-Process.\n\n**2\\. By name (ByPropertyName).** If (and only if) there is no property that matches ByValue, then it will attempt to match any properties that accept pipeline input by property name, again as seen in `get-help -full`.\n\nFor example, `get-service` has the following parameters:\n\n```\n-ComputerName <String[]>\n    Gets the services running on the specified computers.\n\n    Accept pipeline input?       True (ByPropertyName)\n\n-Name <String[]>\n    Specifies the service names of services to be retrieved. \n\n    Accept pipeline input?       True (ByPropertyName, ByValue)\n```\nNote that both of these have the same type. That doesn't matter, because here it knows where to bind different data based on the property names of the input object. For example, if I have a CSV file that has both ComputerName and Name columns and then did `import-csv myfile.csv | get-service` the contents of my CSV file would be bound to the relevant parameters.\n\nNotice that in the previous example `-Name` accepts input by both value and property name. ByValue comes first, so if I just pipeline input a string, e.g `\"spooler\" | get-service` then that input is going to be bound to the `-name` parameter only. In my CSV example, the \"value\" (type) of the pipeline input is a PSCustomObject (which is what is generated by Import-CSV) and as a result there is no ByValue match, and so it falls back to ByPropertyName and maps together every matching property name it has.\n\n# If you can't get input to a property via the pipeline there's another way\n\nIn the previous example, you can see that in `get-service` both `-name` and `-computername` accept pipeline input ByPropertyName and `-name` accepts input ByValue. So what if I have a list of strings and want to pipe them explicitly in to the `-computername` parameter? Well the short answer is you can't, but Month of Lunches suggested an alternative.\n\nIn the past, I might have done this with a ForEach-Object loop, say something like this:\n```\n$ Servers = Get-Content 'servers.txt'\n$ Servers | ForEach-Object {\n    Get-Service -ComputerName $_\n}\n```\nThe better/shorter option is this:\n```\nGet-Service -ComputerName (Get-Content 'servers.txt')\n```\nHere the brackets work much like in mathematics and result in their contents being executed first. Because `-ComputerName` accepts array (multiple) input, the Get-Content cmdlet creates an array of strings which are then sent to the -ComputerName parameter and the Get-Service cmdlet processes each. Note this only works in this way because `-ComputerName` accepts array input. If it didn't, i'd definitely have to fall back to a loop construct.\n\nThe concept of brackets forcing execution first was not new to me (e.g I'd used it plenty via subexpressions in strings) but I don't think i'd ever considered using it in this way to create more efficient/tidy/cool code.\n\n# One script one pipeline\n\nThis is something else that i'd never considered before. Most interestingly, it's a key difference between how commands execute when entered in the console compared to how they execute when you run a script. \n\nWhen you run cmdlets sequentially at the console, each one executes in its own pipeline (as soon as you press enter):\n\n```\nGet-Service <enter>\nGet-Process\n```\n\nWith the above you get two separate outputs, each formatted correctly for their different types.\n\nBut if you put those commands in a script and execute it, you'll get strange output when it gets to the `Get-Process` cmdlet as essentially both execute within the same pipeline and so get piped together to a single formatting cmdlet (the one for `Get-Service` as it was first).\n\nInterestingly, you can cause the same issue at the console by entering `Get-Service; Get-Process` and hitting enter at the console.\n\nThe point is, when writing scripts/functions it's important (for this reason as well as others) to only return one kind of result.\n\n---\nThis concludes part one. [In part two I cover some things I learnt about the PowerShell help system](http://wragg.io/tilfmol-2-powershell-help/) `get-help`.",
                        "html": "<p>As introduced in my <a href=\"http://wragg.io/tilfmol-things-i-learnt-from-learn-powershell-in-a-month-of-lunches/\">previous post</a>, this is part one of a four part series that documents some things I discovered or had clarified by reading the excellent <a href=\"https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition\">Learn PowerShell in a Month of Lunches</a> book (recently released in 3rd edition).</p>\n\n<p>This post covers some things I learnt about the PowerShell pipeline, which include:</p>\n\n<ul>\n<li>How pipeline input knows where to go</li>\n<li>If you can't get input to a property via the pipeline there's another way</li>\n<li>the concept of 'one script one pipeline'</li>\n</ul>\n\n<h1 id=\"howpipelineinputknowswheretogo\">How pipeline input knows where to go</h1>\n\n<p>Like 'how does something as heavy as an aeroplane stay in the sky?', pipeline input was something I was glad worked but tried not to think about too deeply. In fact it hadn't really seemed important, certain (similar named) cmdlets obviously work together (<code>get-service 'myservice' | stop-service</code>), it didn't seem to matter how. And then where it was less obvious how cmdlets might go together I had somehow learnt getting pipeline input in to a cmdlet was a matter of matching the relevant output and input types. </p>\n\n<p>The (more in depth) answer is apparently, one of two ways:</p>\n\n<p><strong>1. By value (ByValue).</strong> A cmdlet may have a single parameter of any particular type that accepts pipeline input by value. That is to say (for example) if there are three properties that accept string input, only one of them may accept pipeline input ByValue (but a single property of each different type can also accept input ByValue). When you send pipeline input to the cmdlet, if the input is of a matching type then that's the parameter it is bound to.</p>\n\n<p>You can see if any parameters accept input ByValue in <code>get-help -full</code></p>\n\n<p>For example <code>Stop-Process</code> has the following parameter:  </p>\n\n<pre><code>-InputObject &lt;Process[]&gt;\n    Specifies the process objects to stop.\n\n    Accept pipeline input?       True (ByValue)\n</code></pre>\n\n<p>The parameter <code>-InputObject</code> expects one or more Process objects. The <code>get-process</code> cmdlet creates Process objects (as you can see via <code>get-process | get-member</code>):  </p>\n\n<pre><code>   TypeName: System.Diagnostics.Process\n</code></pre>\n\n<p>As as such, when piped they are bound to the <code>-InputObject</code> parameter of Stop-Process.</p>\n\n<p><strong>2. By name (ByPropertyName).</strong> If (and only if) there is no property that matches ByValue, then it will attempt to match any properties that accept pipeline input by property name, again as seen in <code>get-help -full</code>.</p>\n\n<p>For example, <code>get-service</code> has the following parameters:</p>\n\n<pre><code>-ComputerName &lt;String[]&gt;\n    Gets the services running on the specified computers.\n\n    Accept pipeline input?       True (ByPropertyName)\n\n-Name &lt;String[]&gt;\n    Specifies the service names of services to be retrieved. \n\n    Accept pipeline input?       True (ByPropertyName, ByValue)\n</code></pre>\n\n<p>Note that both of these have the same type. That doesn't matter, because here it knows where to bind different data based on the property names of the input object. For example, if I have a CSV file that has both ComputerName and Name columns and then did <code>import-csv myfile.csv | get-service</code> the contents of my CSV file would be bound to the relevant parameters.</p>\n\n<p>Notice that in the previous example <code>-Name</code> accepts input by both value and property name. ByValue comes first, so if I just pipeline input a string, e.g <code>\"spooler\" | get-service</code> then that input is going to be bound to the <code>-name</code> parameter only. In my CSV example, the \"value\" (type) of the pipeline input is a PSCustomObject (which is what is generated by Import-CSV) and as a result there is no ByValue match, and so it falls back to ByPropertyName and maps together every matching property name it has.</p>\n\n<h1 id=\"ifyoucantgetinputtoapropertyviathepipelinetheresanotherway\">If you can't get input to a property via the pipeline there's another way</h1>\n\n<p>In the previous example, you can see that in <code>get-service</code> both <code>-name</code> and <code>-computername</code> accept pipeline input ByPropertyName and <code>-name</code> accepts input ByValue. So what if I have a list of strings and want to pipe them explicitly in to the <code>-computername</code> parameter? Well the short answer is you can't, but Month of Lunches suggested an alternative.</p>\n\n<p>In the past, I might have done this with a ForEach-Object loop, say something like this:  </p>\n\n<pre><code>$ Servers = Get-Content 'servers.txt'\n$ Servers | ForEach-Object {\n    Get-Service -ComputerName $_\n}\n</code></pre>\n\n<p>The better/shorter option is this:  </p>\n\n<pre><code>Get-Service -ComputerName (Get-Content 'servers.txt')  \n</code></pre>\n\n<p>Here the brackets work much like in mathematics and result in their contents being executed first. Because <code>-ComputerName</code> accepts array (multiple) input, the Get-Content cmdlet creates an array of strings which are then sent to the -ComputerName parameter and the Get-Service cmdlet processes each. Note this only works in this way because <code>-ComputerName</code> accepts array input. If it didn't, i'd definitely have to fall back to a loop construct.</p>\n\n<p>The concept of brackets forcing execution first was not new to me (e.g I'd used it plenty via subexpressions in strings) but I don't think i'd ever considered using it in this way to create more efficient/tidy/cool code.</p>\n\n<h1 id=\"onescriptonepipeline\">One script one pipeline</h1>\n\n<p>This is something else that i'd never considered before. Most interestingly, it's a key difference between how commands execute when entered in the console compared to how they execute when you run a script. </p>\n\n<p>When you run cmdlets sequentially at the console, each one executes in its own pipeline (as soon as you press enter):</p>\n\n<pre><code>Get-Service &lt;enter&gt;  \nGet-Process  \n</code></pre>\n\n<p>With the above you get two separate outputs, each formatted correctly for their different types.</p>\n\n<p>But if you put those commands in a script and execute it, you'll get strange output when it gets to the <code>Get-Process</code> cmdlet as essentially both execute within the same pipeline and so get piped together to a single formatting cmdlet (the one for <code>Get-Service</code> as it was first).</p>\n\n<p>Interestingly, you can cause the same issue at the console by entering <code>Get-Service; Get-Process</code> and hitting enter at the console.</p>\n\n<p>The point is, when writing scripts/functions it's important (for this reason as well as others) to only return one kind of result.</p>\n\n<hr />\n\n<p>This concludes part one. <a href=\"http://wragg.io/tilfmol-2-powershell-help/\">In part two I cover some things I learnt about the PowerShell help system</a> <code>get-help</code>.</p>",
                        "image": "/content/images/2017/02/pipes-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Things I learnt about PowerShell's Pipeline",
                        "meta_description": "A blog post about how PowerShell pipeline input binding works, getting input to a property without the pipeline and the concept of one script one pipeline.",
                        "author_id": 1,
                        "created_at": "2017-02-22 22:39:48",
                        "created_by": 1,
                        "updated_at": "2017-03-02 08:38:59",
                        "updated_by": 1,
                        "published_at": "2017-02-23 15:35:52",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 52,
                        "uuid": "008ef7fd-5911-4395-8457-5ede741a7d57",
                        "title": "TILFMOL #2 - PowerShell Help",
                        "slug": "tilfmol-2-powershell-help",
                        "markdown": "This is part two of a short series of posts about things I discovered or had clarified by reading the excellent [Learn PowerShell in a Month of Lunches book](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition) (recently released in 3rd edition).\n\nThis post focuses on things I learnt about the PowerShell help function, including:\n\n- The difference between running `help` and `get-help`\n- The meaning of square brackets in help:\n  - Optional/Positional Parameters\n  - Array Input\n- Other punctuation in help\n- Extending the output of help\n\nIn every beginners guide/book/course that I've seen for PowerShell, the help system comes highly recommended and is usually explained in great detail. PowerShell has been designed to be a very discoverable language (the standardisation of verbs such as \"get\", \"set\" etc. being a key example of this).\n\nHowever in the past I have (as I suspect have a lot of people) actively avoided running `get-help`. I think there a couple of reasons for this:\n\n1. 'Help' in Windows has always felt like something aimed at end users, not at IT pros. As a result there's a stigma to using it.\n2. Searching for a topic on the internet on the other hand has always felt like doing research, which is how professionals gain knowledge.\n3. The information returned is verbose and contains a lot of unexplained terminology and punctuation (more on this later).\n\nAs a result the `help` system is often overlooked, but once you do understand the basic structure and terminology it's a very powerful resource.\n\n# The difference between running help and get-help\n\nOne thing that I hadn't realised before reading Month of Lunches is that `Help` is not a straight up Alias for `Get-Help` (it sort of is, but not really). Both commands allow you to access the help system, but when in the PowerShell console `help` paginates by default (e.g if the content returned is longer than the window then it asks you to press a key for --More--):\n\n![](/content/images/2017/02/powershell-help-more.png)\n\nThis is equivalent of doing `get-help <command> | more` (but is -intentionally- a lot shorter).\n\nThis happens because `Help` is actually a function not an alias (but it has an alias of `Man` to help out those coming from a Unix background) which allows it to work differently (where as Aliases are just different names for the exact same functionality).\n\nThere's a small caveat to this: the same is not true in the ISE. Here both help and get-help do not paginate, even if you pipe the content to `More`. \n\n# Square brackets in help\n\nAs mentioned earlier, one of the reasons I think people avoid `Get-Help` in favour of searching the internet (I'm particularly fond of the no-nonsense content on https://ss64.com/ps/ for example) is that when you first run `get-help` for a command you get a SYNTAX block that has a LOT of square brackets and other punctuation that are not explained.\n\n*-- Also, I have (and have seen others) in the past tried to pipe a command to get-help, because the first thing you're told with PowerShell is that it's all about the pipeline. But `my-command | get-help` does not work (which makes sense, even if get-help accepted pipeline input, that would pipe it the results of the command not the command name itself).*\n\nLearn PowerShell in a Month of Lunches clarified for me what all that punctuation means in the help file (i'd come to some conclusions on my own, but having them verified was great). Square brackets are particularly confusing as they have two meanings.\n\nHere's an example:\n```\nSYNTAX\n    Get-Service [[-Name] <String[]>] [-ComputerName <String[]>] [-DependentServices] [-Exclude <String[]>] [-Include\n<String[]>] [-RequiredServices] [<CommonParameters>]\n```\nThat's a lot of square brackets (and punctuation in general). \n\nThere are a couple of meanings for the square brackets:\n\n**1\\. Around any parameter name or parameter value, they are indicating stuff that is optional.** In the example above, you can see that all of the parameters are optional (each one is surrounded entirely by square brackets). I can execute `get-service` on its own and it just spits out all the services.\n\nYou might have noticed that there's additional square brackets around `-Name` that aren't around most of the other parameter names. That's because `-Name` is a positional parameter, so I can provide that input without explicitly naming it, so long as I provide that input in the right position (in this case, first) vs any other positional parameters. E.g I can do:\n```\nGet-Service spooler\n```\nNote I can also do this:\n```\nGet-Service -Exclude sppsvc sp*\n```\nWhich is a little more confusing, but what's happening here is this:\n```\nGet-Service -Exclude sppsvc -name sp*\n```\nBecause -Exclude was explicitly named, my unnamed input still went to the first positional parameter `-name` like I intended.\n\nIf you look again at the earlier snippet of Parameter help text for `Get-Service`, you might be forgiven for thinking that `[-RequiredServices]` is a positional parameter, but it's not. It's a switch parameter, which means it's just true or false (you either provide it or not, with no input) so as a result the whole parameter name is obviously optional.\n\nThe other way to know if a parameter is positional is to use `get-help -full`. In the section that provides more detail about each Parameter you will see `Position?` with the value being either `named` or a numerical value representing it's position (starting at 0).\n\n**2\\. Next to any type name, square brackets indicate that the parameter accepts multiple/array input of that type.**. As demonstrated earlier, the Help for `Get-Service` shows that -ComputerName accepts string array input (per the inner square brackets below):\n\n```\nSYNTAX\n    Get-Service [-ComputerName <String[]>]\n```\nSo I can (for example) do this:\n```\nGet-Service -ComputerName 'Server1','Server2','Server3'\n```\nAnd the cmdlet will run multiple times internally for each input I provided.\n\nThese concepts weren't new to me, but this clarified what the help file was trying to tell me with all those square brackets (and whether my assumptions about them were correct). The result is knowing (through `get-help`, and with less trial and error) how to use cmdlets more efficiently.\n\n# Other punctuation in help\n\nIn case you're not aware, here's some explanations for the other punctuation you see via help:\n\n- Dashes '-' precede parameter names, just as they do when you execute the command via the command-line (this is the PowerShell version of a switch proceeded with a forward slash '/' in the old style DOS commands.\n  - Be aware you can still use DOS commands in PowerShell and when doing so you should revert to their original syntax such as / switches, as well as be aware that their output will not be a PowerShell object (the exception to this rule is where the old DOS command is now a PowerShell alias: e.g `dir` is an alias for Get-ChildItem, and so it is a true PowerShell command).\n- Angle brackets <> surround the type of input a parameter expects. E.g `<string>` expects a string of text input (and as mentioned earlier `<string[]>` means that the input accepts one or more strings: either a single string or an array).\n\n# Extending the output of help\n\nFinally, there's a few good ways to extend the output of help:\n\n- Use the `-detailed` parameter with `get-help` to see extra information about parameters, such as whether they accept pipeline input and whether that is by value, name or both (per my [previous article on the pipeline](http://wragg.io/tilfmol1-the-powershell-pipeline/)) and usage examples for the command. \n- Use the `-full` parameter to see all the help information about a command at once (this is pretty verbose).\n- You can also do `-parameter <parameter name>` to see just the information about a single parameter and `-examples` to just see examples.\n\nAlso you can open the help information in a pop up window (a way of keeping the it in your view while returning your console to being functional) via the `get-help` parameter `-showwindow` (there seems to be a bug with this in that it doesn't give you the parameter information that you get with `-detailed` or `-full` at the console even though it seems like it should).\n\nAlso if a help file has an online resource you can open that in a web browser by using the `-online` parameter with `get-help`.\n\n---\nThis concludes part two. There's a lot more to the PowerShell help system not covered here, as well as other commands you should be aware of such as `get-command` and (the very important) `get-member` which contribute greatly to the discoverability of PowerShell. These are covered in Month of Lunches, or give them a google.\n\nIn part three of this series I cover some [things I learnt about PowerShell Remoting](http://wragg.io/tilfmol-3-powershell-remoting/), which is something that I hadn't had a need to experiment with previously.\n\nIf you missed part one of this series, I covered some aspects of [the PowerShell Pipeline](http://wragg.io/tilfmol1-the-powershell-pipeline/).",
                        "html": "<p>This is part two of a short series of posts about things I discovered or had clarified by reading the excellent <a href=\"https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition\">Learn PowerShell in a Month of Lunches book</a> (recently released in 3rd edition).</p>\n\n<p>This post focuses on things I learnt about the PowerShell help function, including:</p>\n\n<ul>\n<li>The difference between running <code>help</code> and <code>get-help</code></li>\n<li>The meaning of square brackets in help:\n<ul><li>Optional/Positional Parameters</li>\n<li>Array Input</li></ul></li>\n<li>Other punctuation in help</li>\n<li>Extending the output of help</li>\n</ul>\n\n<p>In every beginners guide/book/course that I've seen for PowerShell, the help system comes highly recommended and is usually explained in great detail. PowerShell has been designed to be a very discoverable language (the standardisation of verbs such as \"get\", \"set\" etc. being a key example of this).</p>\n\n<p>However in the past I have (as I suspect have a lot of people) actively avoided running <code>get-help</code>. I think there a couple of reasons for this:</p>\n\n<ol>\n<li>'Help' in Windows has always felt like something aimed at end users, not at IT pros. As a result there's a stigma to using it.  </li>\n<li>Searching for a topic on the internet on the other hand has always felt like doing research, which is how professionals gain knowledge.  </li>\n<li>The information returned is verbose and contains a lot of unexplained terminology and punctuation (more on this later).</li>\n</ol>\n\n<p>As a result the <code>help</code> system is often overlooked, but once you do understand the basic structure and terminology it's a very powerful resource.</p>\n\n<h1 id=\"thedifferencebetweenrunninghelpandgethelp\">The difference between running help and get-help</h1>\n\n<p>One thing that I hadn't realised before reading Month of Lunches is that <code>Help</code> is not a straight up Alias for <code>Get-Help</code> (it sort of is, but not really). Both commands allow you to access the help system, but when in the PowerShell console <code>help</code> paginates by default (e.g if the content returned is longer than the window then it asks you to press a key for --More--):</p>\n\n<p><img src=\"/content/images/2017/02/powershell-help-more.png\" alt=\"\" /></p>\n\n<p>This is equivalent of doing <code>get-help &lt;command&gt; | more</code> (but is -intentionally- a lot shorter).</p>\n\n<p>This happens because <code>Help</code> is actually a function not an alias (but it has an alias of <code>Man</code> to help out those coming from a Unix background) which allows it to work differently (where as Aliases are just different names for the exact same functionality).</p>\n\n<p>There's a small caveat to this: the same is not true in the ISE. Here both help and get-help do not paginate, even if you pipe the content to <code>More</code>. </p>\n\n<h1 id=\"squarebracketsinhelp\">Square brackets in help</h1>\n\n<p>As mentioned earlier, one of the reasons I think people avoid <code>Get-Help</code> in favour of searching the internet (I'm particularly fond of the no-nonsense content on <a href=\"https://ss64.com/ps/\">https://ss64.com/ps/</a> for example) is that when you first run <code>get-help</code> for a command you get a SYNTAX block that has a LOT of square brackets and other punctuation that are not explained.</p>\n\n<p><em>-- Also, I have (and have seen others) in the past tried to pipe a command to get-help, because the first thing you're told with PowerShell is that it's all about the pipeline. But <code>my-command | get-help</code> does not work (which makes sense, even if get-help accepted pipeline input, that would pipe it the results of the command not the command name itself).</em></p>\n\n<p>Learn PowerShell in a Month of Lunches clarified for me what all that punctuation means in the help file (i'd come to some conclusions on my own, but having them verified was great). Square brackets are particularly confusing as they have two meanings.</p>\n\n<p>Here's an example:  </p>\n\n<pre><code>SYNTAX  \n    Get-Service [[-Name] &lt;String[]&gt;] [-ComputerName &lt;String[]&gt;] [-DependentServices] [-Exclude &lt;String[]&gt;] [-Include\n&lt;String[]&gt;] [-RequiredServices] [&lt;CommonParameters&gt;]  \n</code></pre>\n\n<p>That's a lot of square brackets (and punctuation in general). </p>\n\n<p>There are a couple of meanings for the square brackets:</p>\n\n<p><strong>1. Around any parameter name or parameter value, they are indicating stuff that is optional.</strong> In the example above, you can see that all of the parameters are optional (each one is surrounded entirely by square brackets). I can execute <code>get-service</code> on its own and it just spits out all the services.</p>\n\n<p>You might have noticed that there's additional square brackets around <code>-Name</code> that aren't around most of the other parameter names. That's because <code>-Name</code> is a positional parameter, so I can provide that input without explicitly naming it, so long as I provide that input in the right position (in this case, first) vs any other positional parameters. E.g I can do:  </p>\n\n<pre><code>Get-Service spooler  \n</code></pre>\n\n<p>Note I can also do this:  </p>\n\n<pre><code>Get-Service -Exclude sppsvc sp*  \n</code></pre>\n\n<p>Which is a little more confusing, but what's happening here is this:  </p>\n\n<pre><code>Get-Service -Exclude sppsvc -name sp*  \n</code></pre>\n\n<p>Because -Exclude was explicitly named, my unnamed input still went to the first positional parameter <code>-name</code> like I intended.</p>\n\n<p>If you look again at the earlier snippet of Parameter help text for <code>Get-Service</code>, you might be forgiven for thinking that <code>[-RequiredServices]</code> is a positional parameter, but it's not. It's a switch parameter, which means it's just true or false (you either provide it or not, with no input) so as a result the whole parameter name is obviously optional.</p>\n\n<p>The other way to know if a parameter is positional is to use <code>get-help -full</code>. In the section that provides more detail about each Parameter you will see <code>Position?</code> with the value being either <code>named</code> or a numerical value representing it's position (starting at 0).</p>\n\n<p><strong>2. Next to any type name, square brackets indicate that the parameter accepts multiple/array input of that type.</strong>. As demonstrated earlier, the Help for <code>Get-Service</code> shows that -ComputerName accepts string array input (per the inner square brackets below):</p>\n\n<pre><code>SYNTAX  \n    Get-Service [-ComputerName &lt;String[]&gt;]\n</code></pre>\n\n<p>So I can (for example) do this:  </p>\n\n<pre><code>Get-Service -ComputerName 'Server1','Server2','Server3'  \n</code></pre>\n\n<p>And the cmdlet will run multiple times internally for each input I provided.</p>\n\n<p>These concepts weren't new to me, but this clarified what the help file was trying to tell me with all those square brackets (and whether my assumptions about them were correct). The result is knowing (through <code>get-help</code>, and with less trial and error) how to use cmdlets more efficiently.</p>\n\n<h1 id=\"otherpunctuationinhelp\">Other punctuation in help</h1>\n\n<p>In case you're not aware, here's some explanations for the other punctuation you see via help:</p>\n\n<ul>\n<li>Dashes '-' precede parameter names, just as they do when you execute the command via the command-line (this is the PowerShell version of a switch proceeded with a forward slash '/' in the old style DOS commands.\n<ul><li>Be aware you can still use DOS commands in PowerShell and when doing so you should revert to their original syntax such as / switches, as well as be aware that their output will not be a PowerShell object (the exception to this rule is where the old DOS command is now a PowerShell alias: e.g <code>dir</code> is an alias for Get-ChildItem, and so it is a true PowerShell command).</li></ul></li>\n<li>Angle brackets &lt;> surround the type of input a parameter expects. E.g <code>&lt;string&gt;</code> expects a string of text input (and as mentioned earlier <code>&lt;string[]&gt;</code> means that the input accepts one or more strings: either a single string or an array).</li>\n</ul>\n\n<h1 id=\"extendingtheoutputofhelp\">Extending the output of help</h1>\n\n<p>Finally, there's a few good ways to extend the output of help:</p>\n\n<ul>\n<li>Use the <code>-detailed</code> parameter with <code>get-help</code> to see extra information about parameters, such as whether they accept pipeline input and whether that is by value, name or both (per my <a href=\"http://wragg.io/tilfmol1-the-powershell-pipeline/\">previous article on the pipeline</a>) and usage examples for the command. </li>\n<li>Use the <code>-full</code> parameter to see all the help information about a command at once (this is pretty verbose).</li>\n<li>You can also do <code>-parameter &lt;parameter name&gt;</code> to see just the information about a single parameter and <code>-examples</code> to just see examples.</li>\n</ul>\n\n<p>Also you can open the help information in a pop up window (a way of keeping the it in your view while returning your console to being functional) via the <code>get-help</code> parameter <code>-showwindow</code> (there seems to be a bug with this in that it doesn't give you the parameter information that you get with <code>-detailed</code> or <code>-full</code> at the console even though it seems like it should).</p>\n\n<p>Also if a help file has an online resource you can open that in a web browser by using the <code>-online</code> parameter with <code>get-help</code>.</p>\n\n<hr />\n\n<p>This concludes part two. There's a lot more to the PowerShell help system not covered here, as well as other commands you should be aware of such as <code>get-command</code> and (the very important) <code>get-member</code> which contribute greatly to the discoverability of PowerShell. These are covered in Month of Lunches, or give them a google.</p>\n\n<p>In part three of this series I cover some <a href=\"http://wragg.io/tilfmol-3-powershell-remoting/\">things I learnt about PowerShell Remoting</a>, which is something that I hadn't had a need to experiment with previously.</p>\n\n<p>If you missed part one of this series, I covered some aspects of <a href=\"http://wragg.io/tilfmol1-the-powershell-pipeline/\">the PowerShell Pipeline</a>.</p>",
                        "image": "/content/images/2017/02/help-keyboard-2.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Things I learnt about the PowerShell Help System from Month of Lunches",
                        "meta_description": "A blog post about the PowerShell help system, including the difference between help and get-help and the meaning of square brackets and other punctuation.",
                        "author_id": 1,
                        "created_at": "2017-02-23 06:57:04",
                        "created_by": 1,
                        "updated_at": "2017-03-16 21:32:41",
                        "updated_by": 1,
                        "published_at": "2017-03-02 08:00:00",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 53,
                        "uuid": "160c23cc-bb79-4b06-8dfb-0a60d2d18c21",
                        "title": "TILFMOL #3 - PowerShell Remoting",
                        "slug": "tilfmol-3-powershell-remoting",
                        "markdown": "This is part three of a short series of posts about things I discovered from reading [Learn PowerShell in a Month of Lunches](https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition) (recently released in 3rd edition).\n\nThis post focuses on things I learnt about PowerShell remoting, including:\n\n- Executing remote commands on one or many machines\n- Deserialised objects are the result of commands\n- Creating Endpoints\n\n# Quick Intro to Remoting\n\nPowerShell Remoting provides a standard way of executing cmdlets remotely and retrieving the results. It means [that the communication ports required are standardised](https://blogs.msdn.microsoft.com/wmi/2009/07/22/new-default-ports-for-ws-management-and-powershell-remoting/) and cmdlet developers don't need to code their own remote execution in to their command (that's not to say some developers still don't). Some built-in cmdlets in PowerShell have a `-computername` parameter and this is often because they date from the pre-Remoting era, but PS Remoting now makes that generally unnecessary.\n\n*-- If you want to play around with PS Remoting it may first need to be enabled. This is usually a simple case of entering `enable-psremoting` in a PowerShell window (with appropriate privileges).*\n\n*There can be some complexities in getting it working if your source and destination machines aren't on the same domain or are separated by a complex network.*\n\n![Enabling PowerShell Remoting](/content/images/2017/03/Enable-PSRemoting.png)\n\n## One-to-one remoting\n\nOnce you have PS Remoting enabled, the simplest way to use it is use the `Enter-PSSession` and `Exit-PSSession` cmdlets, which allow you to open and connect to a PowerShell session on a remote computer (similar to SSH or a command-line only RDP). Your prompt changes to include the remote server name so you can tell where you are.\n\n## One-to-many remoting\n\nThe true power of remoting is the ability to run a command and have it execute simultaneously on multiple machines (by default up to 32 at once). You do this with the `Invoke-Command` cmdlet which has a `-ComputerName` parameter that accepts one or more names and a `-ScriptBlock` parameter that is the block of code that you want to execute (or with an alternative parameter you can use a script file). \n\n*-- You can of course also use `Invoke-Command` to run commands on a single machine, without the need to open an interactive session per `Enter-PSSession`. You can also use Invoke-Command on a local computer to evaluate or run a string in a script block as a command.*\n\nHere's an example usage of `Invoke-Command` which return the latest 10 System log entries from three servers:\n```\nInvoke-Command -ComputerName Server1,Server2,Server3 -ScriptBlock { \n    Get-EventLog System -Newest 10\n}\n```\n\nThe [official documentation for Invoke-Command](https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.core/invoke-command) has lots of great explanations and examples of the different uses.\n\n# Deserialised objects\n\nThe results returned from remote commands are deserialised which just means they are static and have no methods to enable interactions (as they do when retrieved locally). This isn't particularly surprising, but it was interesting to have called out in MOL.\n\nYou can see this is the case by using the trusty `| get-member` command on your results, by which you will observe the lack of methods (except `ToString()`) but also that the typename at the top of the object has changed to be `Deserialized.whatever.object`. \n\nFor example:\n\n```\nPS C:\\> $s = New-PSSession localhost\nPS C:\\> Invoke-Command $s { Get-Process } | Get-Member\n\n\n   TypeName: Deserialized.System.Diagnostics.Process\n```\n\nThe takeaway from this is that if you need/want to execute the method of an object (and assumedly you want to do so on the machine it originated from) you generally want to make sure you do so remotely (as part of the `invoke-command` scriptblock), before the result is returned.\n\nThere's a great blog post about [how objects are sent to and from remote sessions](https://blogs.msdn.microsoft.com/powershell/2010/01/07/how-objects-are-sent-to-and-from-remote-sessions/) by the PowerShell team that covers this in more detail.\n\n# Creating Endpoints\n\n> \"A computer can contain multiple endpoints, which PowerShell refers to as session configurations. For example, enabling remoting on a 64-bit machine enables an endpoint for 32-bit PowerShell as well as for 64-bit PowerShell, with 64-bit being the default.\" \n>\n> -- Learn PowerShell in a Month of Lunches\n\nYou can see what endpoints currently exist on your machine by running `Get-PSSessionConfiguration` from an Administrator-privileged PowerShell window: \n\n![](/content/images/2017/03/PowerShell-Endpoints.png)\n\nYou can also create your own custom endpoints. This is a two stage process:\n\n1. Run `New-PSSessionConfigurationfile` to create a config file with a .PSSC extension that defines the characteristics of the endpoint (e.g which commands and capabilities it includes).\n2. Use `Register-PSSessionConfiguration` to load the .PSSC file and create the endpoint with the WinRM service. During registration you can set other parameters such as who may connect to the endpoint.\n\nMOL points out that is likely most useful for delegated administration, where you want to give a set of users (such as your Tier 1 / Service Desk) a subset of commands to run.\n\nIn an increasingly security sensitive world, this is a growing topic and Microsoft have more recently been building on this with a project titled [Just Enough Administration](https://msdn.microsoft.com/en-us/powershell/jea/overview) which is also an [open source project in GitHub](https://github.com/PowerShell/JEA).\n\n---\n\nThis has obviously been a very high-level overview of some of the remoting features of PowerShell and I hope it inspires you to read further. \n\nIn my next and final chapter for this series I to look at [PowerShell Jobs](http://wragg.io/tilfmol-4-powershell-jobs/), the background task engine of PowerShell.\n\nHere are some links in case you missed my previous posts in this series on the [PowerShell Pipeline](http://wragg.io/tilfmol1-the-powershell-pipeline/) and the [PowerShell Help System](http://wragg.io/tilfmol-2-powershell-help/).\n",
                        "html": "<p>This is part three of a short series of posts about things I discovered from reading <a href=\"https://www.manning.com/books/learn-windows-powershell-in-a-month-of-lunches-third-edition\">Learn PowerShell in a Month of Lunches</a> (recently released in 3rd edition).</p>\n\n<p>This post focuses on things I learnt about PowerShell remoting, including:</p>\n\n<ul>\n<li>Executing remote commands on one or many machines</li>\n<li>Deserialised objects are the result of commands</li>\n<li>Creating Endpoints</li>\n</ul>\n\n<h1 id=\"quickintrotoremoting\">Quick Intro to Remoting</h1>\n\n<p>PowerShell Remoting provides a standard way of executing cmdlets remotely and retrieving the results. It means <a href=\"https://blogs.msdn.microsoft.com/wmi/2009/07/22/new-default-ports-for-ws-management-and-powershell-remoting/\">that the communication ports required are standardised</a> and cmdlet developers don't need to code their own remote execution in to their command (that's not to say some developers still don't). Some built-in cmdlets in PowerShell have a <code>-computername</code> parameter and this is often because they date from the pre-Remoting era, but PS Remoting now makes that generally unnecessary.</p>\n\n<p><em>-- If you want to play around with PS Remoting it may first need to be enabled. This is usually a simple case of entering <code>enable-psremoting</code> in a PowerShell window (with appropriate privileges).</em></p>\n\n<p><em>There can be some complexities in getting it working if your source and destination machines aren't on the same domain or are separated by a complex network.</em></p>\n\n<p><img src=\"/content/images/2017/03/Enable-PSRemoting.png\" alt=\"Enabling PowerShell Remoting\" /></p>\n\n<h2 id=\"onetooneremoting\">One-to-one remoting</h2>\n\n<p>Once you have PS Remoting enabled, the simplest way to use it is use the <code>Enter-PSSession</code> and <code>Exit-PSSession</code> cmdlets, which allow you to open and connect to a PowerShell session on a remote computer (similar to SSH or a command-line only RDP). Your prompt changes to include the remote server name so you can tell where you are.</p>\n\n<h2 id=\"onetomanyremoting\">One-to-many remoting</h2>\n\n<p>The true power of remoting is the ability to run a command and have it execute simultaneously on multiple machines (by default up to 32 at once). You do this with the <code>Invoke-Command</code> cmdlet which has a <code>-ComputerName</code> parameter that accepts one or more names and a <code>-ScriptBlock</code> parameter that is the block of code that you want to execute (or with an alternative parameter you can use a script file). </p>\n\n<p><em>-- You can of course also use <code>Invoke-Command</code> to run commands on a single machine, without the need to open an interactive session per <code>Enter-PSSession</code>. You can also use Invoke-Command on a local computer to evaluate or run a string in a script block as a command.</em></p>\n\n<p>Here's an example usage of <code>Invoke-Command</code> which return the latest 10 System log entries from three servers:  </p>\n\n<pre><code>Invoke-Command -ComputerName Server1,Server2,Server3 -ScriptBlock {  \n    Get-EventLog System -Newest 10\n}\n</code></pre>\n\n<p>The <a href=\"https://msdn.microsoft.com/en-us/powershell/reference/5.1/microsoft.powershell.core/invoke-command\">official documentation for Invoke-Command</a> has lots of great explanations and examples of the different uses.</p>\n\n<h1 id=\"deserialisedobjects\">Deserialised objects</h1>\n\n<p>The results returned from remote commands are deserialised which just means they are static and have no methods to enable interactions (as they do when retrieved locally). This isn't particularly surprising, but it was interesting to have called out in MOL.</p>\n\n<p>You can see this is the case by using the trusty <code>| get-member</code> command on your results, by which you will observe the lack of methods (except <code>ToString()</code>) but also that the typename at the top of the object has changed to be <code>Deserialized.whatever.object</code>. </p>\n\n<p>For example:</p>\n\n<pre><code>PS C:\\&gt; $s = New-PSSession localhost  \nPS C:\\&gt; Invoke-Command $s { Get-Process } | Get-Member\n\n\n   TypeName: Deserialized.System.Diagnostics.Process\n</code></pre>\n\n<p>The takeaway from this is that if you need/want to execute the method of an object (and assumedly you want to do so on the machine it originated from) you generally want to make sure you do so remotely (as part of the <code>invoke-command</code> scriptblock), before the result is returned.</p>\n\n<p>There's a great blog post about <a href=\"https://blogs.msdn.microsoft.com/powershell/2010/01/07/how-objects-are-sent-to-and-from-remote-sessions/\">how objects are sent to and from remote sessions</a> by the PowerShell team that covers this in more detail.</p>\n\n<h1 id=\"creatingendpoints\">Creating Endpoints</h1>\n\n<blockquote>\n  <p>\"A computer can contain multiple endpoints, which PowerShell refers to as session configurations. For example, enabling remoting on a 64-bit machine enables an endpoint for 32-bit PowerShell as well as for 64-bit PowerShell, with 64-bit being the default.\" </p>\n  \n  <p>-- Learn PowerShell in a Month of Lunches</p>\n</blockquote>\n\n<p>You can see what endpoints currently exist on your machine by running <code>Get-PSSessionConfiguration</code> from an Administrator-privileged PowerShell window: </p>\n\n<p><img src=\"/content/images/2017/03/PowerShell-Endpoints.png\" alt=\"\" /></p>\n\n<p>You can also create your own custom endpoints. This is a two stage process:</p>\n\n<ol>\n<li>Run <code>New-PSSessionConfigurationfile</code> to create a config file with a .PSSC extension that defines the characteristics of the endpoint (e.g which commands and capabilities it includes).  </li>\n<li>Use <code>Register-PSSessionConfiguration</code> to load the .PSSC file and create the endpoint with the WinRM service. During registration you can set other parameters such as who may connect to the endpoint.</li>\n</ol>\n\n<p>MOL points out that is likely most useful for delegated administration, where you want to give a set of users (such as your Tier 1 / Service Desk) a subset of commands to run.</p>\n\n<p>In an increasingly security sensitive world, this is a growing topic and Microsoft have more recently been building on this with a project titled <a href=\"https://msdn.microsoft.com/en-us/powershell/jea/overview\">Just Enough Administration</a> which is also an <a href=\"https://github.com/PowerShell/JEA\">open source project in GitHub</a>.</p>\n\n<hr />\n\n<p>This has obviously been a very high-level overview of some of the remoting features of PowerShell and I hope it inspires you to read further. </p>\n\n<p>In my next and final chapter for this series I to look at <a href=\"http://wragg.io/tilfmol-4-powershell-jobs/\">PowerShell Jobs</a>, the background task engine of PowerShell.</p>\n\n<p>Here are some links in case you missed my previous posts in this series on the <a href=\"http://wragg.io/tilfmol1-the-powershell-pipeline/\">PowerShell Pipeline</a> and the <a href=\"http://wragg.io/tilfmol-2-powershell-help/\">PowerShell Help System</a>.</p>",
                        "image": "/content/images/2017/02/global-network-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Things I learnt about PowerShell Remoting from Month of Lunches",
                        "meta_description": "This post focuses on things I learnt from MOL about PowerShell remoting, including executing remote commands, deserialised objects and\ncreating endpoints.",
                        "author_id": 1,
                        "created_at": "2017-02-23 12:23:10",
                        "created_by": 1,
                        "updated_at": "2018-01-23 16:29:07",
                        "updated_by": 1,
                        "published_at": "2017-03-16 21:29:50",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 54,
                        "uuid": "40130e50-d887-473d-b37a-41f9b25c27e6",
                        "title": "TILFMOL #4 - PowerShell Jobs",
                        "slug": "tilfmol-4-powershell-jobs",
                        "markdown": "This is the fourth and final part of a [short series of posts about things I discovered by reading the excellent Learn PowerShell in a Month of Lunches](http://wragg.io/tilfmol-things-i-learnt-from-learn-powershell-in-a-month-of-lunches/) book (recently released in 3rd edition) as a not quite beginner.\n\nThis post focuses on things I learnt about PowerShell jobs, which is a feature I'd never used but could see being very useful. PowerShell is by nature, single threaded. However jobs allow you to multitask by pushing one or more commands in to the background thereby allowing them to run sequentially. \n\n## Local Jobs\n\nThere are a couple of different ways to use Jobs. Most simply you can use the `Start-Job` command to execute a script block in the background as follows:\n\n    Start-Job -ScriptBlock { Get-ChildItem C:\\ -Recurse }\n\nExplicitly naming the `-ScriptBlock` parameter is optional as it is positional. The output you get from this command will look like this:\n\n![](/content/images/2017/06/PowerShell-Start-job-Example.png)\n\nWhich is a listing of the job object that was created. Your job is now running in the background and to check on it's progress you use `Get-Job`. This will list all jobs that exist within the current session and their status (exactly the same as shown above). You can also request a specific job by passing a name or ID.\n\nTo retrieve the results of a job you use `Receive-Job <id>`. You usually want to do this when the job is complete (as indicated by `Get-Job`) but there's nothing to stop you doing it while it's in progress and you'd see any output being produced that you would see had you run it interactively.\n\nYou can stop a running job with `Stop-Job` and remove them (and their results) with `Remove-Job`. You can also `Suspend-Job` and `Resume-Job`. Finally if you want a session to wait for one or more jobs to complete before doing anything else you can use `Wait-Job`. This might be useful if you had a script that executed a number of distinct tasks simultaneously but you then wanted to wait for them to complete before doing anything else (or before allowing the script to terminate, as the end of the PowerShell session/script will also kill all running jobs whether they finished or not). To do that you might do this: `Get-Job | Wait-Job`.\n\nI can see jobs being useful also as we move into the containerized Windows Nano era of Windows computing. With a command-line only single console version of Windows you might as an administrator want to kick off some tasks without losing the interactivity of your console. Jobs would be your friend here.\n\n## Remote jobs\n\nOf course the greatest benefit is gained by using the jobs functionality to do multiple tasks sequentially across multiple machines. For this you can leverage PowerShell's remoting functionality via `Invoke-Command`. Again, you pass `Invoke-Command` a `-ScriptBlock` to execute and use the `-AsJob` parameter to run the command in parallel on (by default) up to 32 computers (defined via the `-ComputerName` parameter) at once. You can use `-ThrottleLimit` to change this limit (up or down). When the limit is hit, the jobs are queued and as one finishes another is started.\n\nFor example:\n\n    Invoke-Command -ScriptBlock {Get-EventLog System -Newest 1} -ComputerName server1,server2 -AsJob\n\nWill get me the latest System event log event from two computers simultaneously. These commands are executed on the remote computers with just the results returned to mine, which is obviously more efficient for anything long running. You'll notice also when you use `Receive-Job` to get the results that a `PSComputerName` property has been automatically added to the object so you can see which results came from where.\n\nLook out for the `-AsJob` parameter switch on other cmdlets (e.g `Get-WMIObject` has it) as a shorter way to invoke jobs without needing `Invoke-Command`. There's also[ this MSDN page on background jobs](https://msdn.microsoft.com/en-us/library/dd878288(v=vs.85).aspx) that indicates how PowerShell toolmakers can add the functionality to their own cmdlets.\n\n## Scheduled Jobs\n\nYou can also use built-in PowerShell functionality to create scheduled jobs. This is distinct from the Task Scheduler functionality of Windows (which you can leverage from PowerShell). These are a little complex to build as you need to create a Job Trigger that defines when the task will run. You do this with `New-JobTrigger`. You can also configure complex options for the job via the `New-ScheduledTaskOption` cmdlet. Finally you register your scheduled job with (unsurprisingly) `Register-ScheduledJob`.\n\nHere's an example from the book:\n\n     Register-ScheduledJob -Name DailyProcList `\n          -ScriptBlock { Get-Process } `\n          -Trigger (New-JobTrigger -Daily -At 2am) `\n          -ScheduledJobOption (New-ScheduledJobOption -WakeToRun -RunElevated)\n\nYou can see that the JobTrigger and ScheduledJobOption cmdlets are being used in line (in brackets so they execute first) to create the objects necessary for those parameters.\n\nThis task will create a job that runs every day at 2am and stores a snapshot of running processes, waking the computer if necessary to do so.\n\nAs previously, you see/pick up the result of this job via `Get-Job` and `Receive-Job`. Beware that unlike the other jobs above, Scheduled Jobs are stored on disk (under your profile) so anyone with adequate permissions can see/modify/retrieve them. This of course means that they also differ in that the PowerShell session doesn't need to remain for them to persist.\n\nYou can remove a scheduled job via `Unregister-ScheduledJob`. You can also `Disable-ScheduledJob` and `Enable-ScheduledJob` them.\n\nThis has been a very brief tour of PowerShell jobs that I hope you found a useful introduction. To learn more, pick up Month of Lunches or explore all the job commands via `Get-Command *job` and their `Get-Help` pages.",
                        "html": "<p>This is the fourth and final part of a <a href=\"http://wragg.io/tilfmol-things-i-learnt-from-learn-powershell-in-a-month-of-lunches/\">short series of posts about things I discovered by reading the excellent Learn PowerShell in a Month of Lunches</a> book (recently released in 3rd edition) as a not quite beginner.</p>\n\n<p>This post focuses on things I learnt about PowerShell jobs, which is a feature I'd never used but could see being very useful. PowerShell is by nature, single threaded. However jobs allow you to multitask by pushing one or more commands in to the background thereby allowing them to run sequentially. </p>\n\n<h2 id=\"localjobs\">Local Jobs</h2>\n\n<p>There are a couple of different ways to use Jobs. Most simply you can use the <code>Start-Job</code> command to execute a script block in the background as follows:</p>\n\n<pre><code>Start-Job -ScriptBlock { Get-ChildItem C:\\ -Recurse }\n</code></pre>\n\n<p>Explicitly naming the <code>-ScriptBlock</code> parameter is optional as it is positional. The output you get from this command will look like this:</p>\n\n<p><img src=\"/content/images/2017/06/PowerShell-Start-job-Example.png\" alt=\"\" /></p>\n\n<p>Which is a listing of the job object that was created. Your job is now running in the background and to check on it's progress you use <code>Get-Job</code>. This will list all jobs that exist within the current session and their status (exactly the same as shown above). You can also request a specific job by passing a name or ID.</p>\n\n<p>To retrieve the results of a job you use <code>Receive-Job &lt;id&gt;</code>. You usually want to do this when the job is complete (as indicated by <code>Get-Job</code>) but there's nothing to stop you doing it while it's in progress and you'd see any output being produced that you would see had you run it interactively.</p>\n\n<p>You can stop a running job with <code>Stop-Job</code> and remove them (and their results) with <code>Remove-Job</code>. You can also <code>Suspend-Job</code> and <code>Resume-Job</code>. Finally if you want a session to wait for one or more jobs to complete before doing anything else you can use <code>Wait-Job</code>. This might be useful if you had a script that executed a number of distinct tasks simultaneously but you then wanted to wait for them to complete before doing anything else (or before allowing the script to terminate, as the end of the PowerShell session/script will also kill all running jobs whether they finished or not). To do that you might do this: <code>Get-Job | Wait-Job</code>.</p>\n\n<p>I can see jobs being useful also as we move into the containerized Windows Nano era of Windows computing. With a command-line only single console version of Windows you might as an administrator want to kick off some tasks without losing the interactivity of your console. Jobs would be your friend here.</p>\n\n<h2 id=\"remotejobs\">Remote jobs</h2>\n\n<p>Of course the greatest benefit is gained by using the jobs functionality to do multiple tasks sequentially across multiple machines. For this you can leverage PowerShell's remoting functionality via <code>Invoke-Command</code>. Again, you pass <code>Invoke-Command</code> a <code>-ScriptBlock</code> to execute and use the <code>-AsJob</code> parameter to run the command in parallel on (by default) up to 32 computers (defined via the <code>-ComputerName</code> parameter) at once. You can use <code>-ThrottleLimit</code> to change this limit (up or down). When the limit is hit, the jobs are queued and as one finishes another is started.</p>\n\n<p>For example:</p>\n\n<pre><code>Invoke-Command -ScriptBlock {Get-EventLog System -Newest 1} -ComputerName server1,server2 -AsJob\n</code></pre>\n\n<p>Will get me the latest System event log event from two computers simultaneously. These commands are executed on the remote computers with just the results returned to mine, which is obviously more efficient for anything long running. You'll notice also when you use <code>Receive-Job</code> to get the results that a <code>PSComputerName</code> property has been automatically added to the object so you can see which results came from where.</p>\n\n<p>Look out for the <code>-AsJob</code> parameter switch on other cmdlets (e.g <code>Get-WMIObject</code> has it) as a shorter way to invoke jobs without needing <code>Invoke-Command</code>. There's also<a href=\"https://msdn.microsoft.com/en-us/library/dd878288(v=vs.85).aspx\"> this MSDN page on background jobs</a> that indicates how PowerShell toolmakers can add the functionality to their own cmdlets.</p>\n\n<h2 id=\"scheduledjobs\">Scheduled Jobs</h2>\n\n<p>You can also use built-in PowerShell functionality to create scheduled jobs. This is distinct from the Task Scheduler functionality of Windows (which you can leverage from PowerShell). These are a little complex to build as you need to create a Job Trigger that defines when the task will run. You do this with <code>New-JobTrigger</code>. You can also configure complex options for the job via the <code>New-ScheduledTaskOption</code> cmdlet. Finally you register your scheduled job with (unsurprisingly) <code>Register-ScheduledJob</code>.</p>\n\n<p>Here's an example from the book:</p>\n\n<pre><code> Register-ScheduledJob -Name DailyProcList `\n      -ScriptBlock { Get-Process } `\n      -Trigger (New-JobTrigger -Daily -At 2am) `\n      -ScheduledJobOption (New-ScheduledJobOption -WakeToRun -RunElevated)\n</code></pre>\n\n<p>You can see that the JobTrigger and ScheduledJobOption cmdlets are being used in line (in brackets so they execute first) to create the objects necessary for those parameters.</p>\n\n<p>This task will create a job that runs every day at 2am and stores a snapshot of running processes, waking the computer if necessary to do so.</p>\n\n<p>As previously, you see/pick up the result of this job via <code>Get-Job</code> and <code>Receive-Job</code>. Beware that unlike the other jobs above, Scheduled Jobs are stored on disk (under your profile) so anyone with adequate permissions can see/modify/retrieve them. This of course means that they also differ in that the PowerShell session doesn't need to remain for them to persist.</p>\n\n<p>You can remove a scheduled job via <code>Unregister-ScheduledJob</code>. You can also <code>Disable-ScheduledJob</code> and <code>Enable-ScheduledJob</code> them.</p>\n\n<p>This has been a very brief tour of PowerShell jobs that I hope you found a useful introduction. To learn more, pick up Month of Lunches or explore all the job commands via <code>Get-Command *job</code> and their <code>Get-Help</code> pages.</p>",
                        "image": "/content/images/2017/10/Automation.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-02-23 12:23:20",
                        "created_by": 1,
                        "updated_at": "2017-10-05 08:24:28",
                        "updated_by": 1,
                        "published_at": "2017-06-19 22:57:21",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 55,
                        "uuid": "f1ed8727-a07d-4e93-86a4-00f780871f66",
                        "title": "A PowerShell Tool Scorecard",
                        "slug": "the-powershell-tool-scorecard",
                        "markdown": "This post contains a PowerShell tool-making scorecard: a series of short questions to assess whether your custom cmdlet/function/tool is following some (generally considered) best practice design choices.\n\n> By \"tool-making\" I am referring the concept of creating one or more PowerShell functions that are intended to be used by end users in the same way as any other built-in (or 3rd party) cmdlet that you might use. Some of the questions below don't necessarily apply for private/helper type functions that can (and often should) be much more simplistic.\n\nThe idea for this scorecard was inspired by several others i've seen, notably the [Joel Test](https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/) which is a scorecard for Developers to evaluate the maturity of a workplace, as well as the [Operations Report Card](https://gist.github.com/markwragg/c6542459244e02ce231d3e58d5d31284) which similarly can be used to assess the operational sophistication of an Ops team.\n\nI also recently read the excellent [PowerShell Toolmaking in a Month of Lunches](https://www.amazon.co.uk/gp/product/1617291161/ref=as_li_tl?ie=UTF8&camp=1634&creative=6738&creativeASIN=1617291161&linkCode=as2&tag=exsite0a-21&linkId=a9836e9a1ffc6c11df0158003580775c) book which recommends following a number of these best  practices. There's also a great list on [this blog post](https://blogs.technet.microsoft.com/pstips/2014/06/17/powershell-scripting-best-practices/) which while a few years old is still very relevant. If you want a very detailed guide I recommend reading the [The PowerShell Best Practices and Style Guide in GitHub](https://github.com/PoshCode/PowerShellPracticeAndStyle). There are lots other sources of PowerShell best practice on the web, and the items below are far from an exhaustive list but (I hope) represent a good starting point. \n\n*[-- If you're unfamiliar with any of the concepts below, scroll further down as I have briefly explained them.]*\n\n**PowerShell Tool Scorecard (+1 point each):**\n\n> 1. Have you declared `[cmdletbinding()]`?\n- Does your function include comment-based or external help?\n- Are your input parameters declared as specific types?\n- Are any mandatory parameters declared as such?\n- Do your parameters have sensible defaults? (if possible)\n- Does your function do just one thing?\n- Does your function include `write-verbose` (or `write-debug`) statements? (if needed)\n- Do you [filter left](https://technet.microsoft.com/en-us/library/2009.09.windowspowershell.aspx)? (where applicable)\n- Does your function support `-whatif` and `-confirm` for any code that modifies something?\n- Is your function named `verb-noun` and using an [approved verb](https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx)?\n\n> Score: __ / 10\n\n**Bonus Round (+5 points each):**\n\n> 1. Are there [Pester](https://github.com/pester/Pester) (or other testing framework) tests?\n- Is the code coverage 100%?\n- Does a check with [PSScriptAnalyzer](https://github.com/PowerShell/PSScriptAnalyzer) pass?\n- Is the code checked in to in a Source Control system?\n- Are you using a CI pipeline to test commits?\n- Are you publishing your script/module to the [PowerShell Gallery](https://www.powershellgallery.com/) (or other package repository)?\n- Is the module automatically published after a successful run of the CI pipeline?\n- Is there a readme.md, Wiki and/or other documentation?\n\n> Score: __ / 40\n\n**Penalty Points (-5 points each):**\n\n> 1. Does your function use `Write-Host` or a `Format-*` cmdlet where it should instead output an object?\n- Are you using cmdlet aliases?\n- Are there any non-meaningful variable names? e.g `$s` `$c`\n- Do you use the `Return` keyword?\n- Have you disabled/suppressed standard errors? e.g with `$ErrorActionPreference = \"SilentlyContinue\"` or via `-ErrorAction`\n\n> Score: __ / -25\n\n---\n\n> **Final Score: __ / 50**\n\n---\nIn case any of the concepts above are unfamiliar to you below are explanations of them and why they are considered important:\n\n**1. Have you declared `[cmdletbinding()]`?**\n\n```\n[cmdletbinding()]\nParam( .. )\n```\n\nBy adding `[cmdletbinding()]` above the `Param()` block of a Function you gain a set of default functionality: the ability to add Write-Verbose statements to output helpful text when `-Verbose` is used, the same for `-Debug` as well as the ability to add `-WhatIf` and `-Confirm` functionality for functions that change the state of something. Even if you don't use any of these things, your function also gets the `-ErrorAction` and `-ErrorVariable` parameters which give the end user options for how they handle any errors your Function generates. There's no good reason I can think of to not use `[cmdletbinding()]` on any function that an end user interacts with.\n\n**2. Does your function include comment-based or external help?**\n\n```\nFunction Do-Something { ..\n  <#\n  .SYNOPSIS\n      A function that does something.\n  .EXAMPLE\n      'Something' | Do-Something\n  #>\n    \n```\n\n\nIt's really easy to add help output to a PowerShell function via a block of comments at the top of the function. Not only is this helpful to the end user who might use `Get-Help yourfunction` to work out how it works, but for anyone modifying your code in the future it gives them an excellent intro to what the function does and how it works.\n\n**3. Are your input parameters declared as specific types?**\n\n```\n[datetime]\n$Date\n```\n\nThis is just a very easy way to do some input validation. There might occasionally be a scenario where you want a parameter to handle any type of input and then convert it to the relevant object in the Function and thats ok. You could probably though still declare that Parameter as a `[object]` type so that it's explicit that is what you are doing.\n\n**4. Are any mandatory parameters declared as such?**\n\n```\n[parameter(Mandatory)]\n$InputObject\n```\n\nIf your function is just going to fail if one or more parameters aren't provided then it should inform the user gracefully of that before it even starts. Adding `[Parameter(Mandatory=$true)]` or (my preference) `[Parameter(Mandatory)]` before a mandatory parameter allows you to do that and let PowerShell handle the work of informing the user what they've done wrong.\n\n**5. Do your parameters have sensible defaults? (if possible)**\n\n```\n[ipaddress]\n$IPAddress = '127.0.0.1'\n```\n\nIf there are some sensible default values for your parameters then you should set them within the `Param` block and ensure that you don't make those parameters mandatory (which is unnecessary and would render your default irrelevant). Sensible defaults make your tool easier to use, but make sure you expose those default values via the help text for those parameters so that it's transparent to the end user.\n\n**6. Does your function do just one thing?**\n\nThis was a core concept covered in the Toolmaking book. As far as I interpret it, it doesn't suggest that your function can't do multiple transformations, but the idea is that your function should perform one distinct action. For example `Get-Service` only returns Service objects, it doesn't also allow a user to modify those services. That's what `Set-Service` is for. It might be that `Get-Service` queries different sources to get the properties of the service objects, but ultimately its job is to get services so that is all it does.\n\n**7. Does your function include `write-verbose` (or `write-debug`) statements? (if needed)**\n\nPer #1, cmdletbinding gives you the option to use `Write-Verbose` and/or `Write-Debug` to show additional output or to halt the script at key points when the end user uses the equivalent parameters on your command. In particular, `Write-Verbose` is preferable to anywhere you might have put in a single line comment in your code to explain a section. Using `Write-Verbose` still has the benefit of adding those explanations but it also exposes them to an end user when they choose to see them.\n\n**8. Do you [filter left](https://technet.microsoft.com/en-us/library/2009.09.windowspowershell.aspx)? (where applicable)**\n\nAnywhere you are filtering the result of a cmdlet you should do it as far left as possible. For example if i'm using `Get-ChildItem` and I want to return only .ps1 scripts then I should use the filtering parameter of that cmdlet. Or if I want to get services where the displayname property contains certain text and then iterate over those services, the `Where-Object` cmdlet that filters by displayname should occur directly after the `Get-Service` cmdlet, not further down the pipeline. Doing so will improve the performance of your script/function.\n\n**9. Does your function support `-whatif` and `-confirm` for any code that modifies something?**\n\n```\n[cmdletbinding(SupportsShouldProcess)]\nParam( .. )\n\n..\n\nif ($PSCmdlet.ShouldProcess('something')) { .. }\n```\n\nAgain per #1, this feature is made possible by adding cmdletbinding to your script. If your script makes changes to something, you should ensure that you've added the relevant code that stops those changes from occurring (or prompts a user) if they use `-whatif` or `-confirm`. This is done by adding a `If ($PSCmdlet.ShouldProcess(\"Message\"))` block around that part of your function that checks if these parameters have been used. Note that this should be around just the smallest part of your script possible that makes a modification, so that other (non target modifying) code still executes.\n\n**10. Is your function named `verb-noun` and using an [approved verb](https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx)?**\n\nEnsuring that you've followed the PowerShell standard of naming your function using a `Verb-Noun` structure and using one of the [approved verbs](https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx) will help make your cmdlet more consistent with the standard set of PowerShell cmdlets as well as others following best practice on the web.\n\n![PowerShell Core now includes a description column output by Get-Verb](/content/images/2018/01/PowerShell-Core-Get-Verb.jpg)\n\n This makes your function easier to understand and easier to discover. Equally you should try and follow existing PowerShell practice when naming your parameters. For example generally PowerShell uses `-ComputerName` when a cmdlet can be targeted at a remote machine, you should do the same vs something like `-Host` or `-Server`.\n\n### Bonus Questions\n\n**1. Are there [Pester](https://github.com/pester/Pester) (or other testing framework) tests?**\n\n[Pester](https://github.com/pester/Pester) is a fantastic tool for writing tests for your code to ensure they perform the way your expect. You should consider writing both unit tests (which test each part of your code) and integration tests (which test how the code actually performs, ideally in some sort of test environment/container). At a minimum unit tests should be used and you can do so even where you don't have access to any dependencies by using mocking.\n\n**2. Is the code coverage 100%?**\n\nPester has a `-CodeCoverage` switch that allows you to see exactly what part of your code is covered by your existing tests. Getting this to 100% still isn't a guarantee that your code doesn't have bugs but its better than anything less.\n\n**3. Does a check with [PSScriptAnalyzer](https://github.com/PowerShell/PSScriptAnalyzer) pass?**\n\nUsing ScriptAnalyzer to evaluate your code helps enforce a number of these best practices. It also checks your code for dangerous behaviour, such as handling credentials as plain text instead of via a credential object.\n\n**4. Is the code checked in to in a Source Control system?**\n\nEven if you are the only person using the code, having it in a source control system means that you have version control and the ability to step backwards if things go wonky. If you aren't the only person using the code, source control allows others to review it and contribute to it.\n\n**5. Are you using a CI pipeline to test commits?**\n\nAs well as writing Pester (or some other framework) tests for your code you should make sure these tests are run every time the code changes. A CI pipeline (e.g with something like AppVeyor) can automate this process. I also use a CI pipeline to execute ScriptAnalyzer on every commit.\n\n**6. Are you publishing your script/module to the [PowerShell Gallery](https://www.powershellgallery.com/) (or other package repository)?**\n\nIf you've written a tool that others might find useful or you want to distribute it (publicly) do so via the PowerShell Gallery so that installing your tool can be done simply via `Install-Module` or `Install-Script`. If you are working on private code, a private package management repository is probably still a good idea to manage distribution and production versions of your code.\n\n**7. Is the module automatically published after a successful run of the CI pipeline?**\n\nPer the above, a CI pipeline should also publich the module to the appropriate package management tool so that this isn't a manual process. This should only occur if all your tests have passed.\n\n**8. Is there a readme.md, Wiki and/or other documentation?**\n\nIf you're using GitHub, your project should have a `readme.md` that explains what the project is about, how to install it and some basic intro for how it works. If the project is of significant complexity a wiki or other documentation might be justifiable.\n\n### Penalty Points\n\n*-- Disclaimer: There's always an exception to a rule and no one is perfect.*\n\n**1. Does your function use `Write-Host` or a `Format-*` cmdlet where it should instead output a usable object?**\n\nGenerally (but not exclusively) using `Write-Host` is considered bad practice because it sends output to the console rather than returning an object that the end user can manipulate or send on to another cmdlet. Equally making use of the `Format` cmdlets in your function gives the user no options for how they handle your output. Instead you should return an object and let them pipe it to a `Format` cmdlet if that's what they want. Or use the format XML feature to define how output should appear by default while still returning it as an object.\n\n**2. Are you using cmdlet aliases?**\n\n![](/content/images/2018/01/PowerShell-Core-Get-Alias.jpg)\n\nPSScriptAnalyzer will complain if you do this also (unless you suppress the relevant rule). Its considered an anti-pattern because aliases are less explicit to anyone reading your code. Generally you should only use aliases at the console to shorten your typing, but when writing a script you should always use the full cmdlet names.\n\n**3. Are there any non-meaningful variable names? e.g `$s` `$c`**\n\nYour variable names should be descriptive regarding their purpose. Non descriptive names are unhelpful to anyone else reading your code.\n\n**4. Do you use the `Return` keyword?**\n\nThe return keyword is generally not needed and probably doesn't do what you think it does.\n\n**5. Have you disabled/suppressed standard errors? e.g with `$ErrorActionPreference = \"SilentlyContinue\"` or via `-ErrorAction`**\n\nYou ideally shouldn't suppress the standard error output of the cmdlets you use in your function. By all means handle them via `Try..Catch` but its generally considered a bad idea to hide error states from end users.\n\n---\n\nIf you disagree with any of the above (or spot any inaccuracies) please let me know via the comments below.\n\n\n",
                        "html": "<p>This post contains a PowerShell tool-making scorecard: a series of short questions to assess whether your custom cmdlet/function/tool is following some (generally considered) best practice design choices.</p>\n\n<blockquote>\n  <p>By \"tool-making\" I am referring the concept of creating one or more PowerShell functions that are intended to be used by end users in the same way as any other built-in (or 3rd party) cmdlet that you might use. Some of the questions below don't necessarily apply for private/helper type functions that can (and often should) be much more simplistic.</p>\n</blockquote>\n\n<p>The idea for this scorecard was inspired by several others i've seen, notably the <a href=\"https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/\">Joel Test</a> which is a scorecard for Developers to evaluate the maturity of a workplace, as well as the <a href=\"https://gist.github.com/markwragg/c6542459244e02ce231d3e58d5d31284\">Operations Report Card</a> which similarly can be used to assess the operational sophistication of an Ops team.</p>\n\n<p>I also recently read the excellent <a href=\"https://www.amazon.co.uk/gp/product/1617291161/ref=as_li_tl?ie=UTF8&amp;camp=1634&amp;creative=6738&amp;creativeASIN=1617291161&amp;linkCode=as2&amp;tag=exsite0a-21&amp;linkId=a9836e9a1ffc6c11df0158003580775c\">PowerShell Toolmaking in a Month of Lunches</a> book which recommends following a number of these best  practices. There's also a great list on <a href=\"https://blogs.technet.microsoft.com/pstips/2014/06/17/powershell-scripting-best-practices/\">this blog post</a> which while a few years old is still very relevant. If you want a very detailed guide I recommend reading the <a href=\"https://github.com/PoshCode/PowerShellPracticeAndStyle\">The PowerShell Best Practices and Style Guide in GitHub</a>. There are lots other sources of PowerShell best practice on the web, and the items below are far from an exhaustive list but (I hope) represent a good starting point. </p>\n\n<p><em>[-- If you're unfamiliar with any of the concepts below, scroll further down as I have briefly explained them.]</em></p>\n\n<p><strong>PowerShell Tool Scorecard (+1 point each):</strong></p>\n\n<blockquote>\n  <ol>\n  <li>Have you declared <code>[cmdletbinding()]</code>?</li>\n  <li>Does your function include comment-based or external help?</li>\n  <li>Are your input parameters declared as specific types?</li>\n  <li>Are any mandatory parameters declared as such?</li>\n  <li>Do your parameters have sensible defaults? (if possible)</li>\n  <li>Does your function do just one thing?</li>\n  <li>Does your function include <code>write-verbose</code> (or <code>write-debug</code>) statements? (if needed)</li>\n  <li>Do you <a href=\"https://technet.microsoft.com/en-us/library/2009.09.windowspowershell.aspx\">filter left</a>? (where applicable)</li>\n  <li>Does your function support <code>-whatif</code> and <code>-confirm</code> for any code that modifies something?</li>\n  <li>Is your function named <code>verb-noun</code> and using an <a href=\"https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx\">approved verb</a>?</li>\n  </ol>\n  \n  <p>Score: __ / 10</p>\n</blockquote>\n\n<p><strong>Bonus Round (+5 points each):</strong></p>\n\n<blockquote>\n  <ol>\n  <li>Are there <a href=\"https://github.com/pester/Pester\">Pester</a> (or other testing framework) tests?</li>\n  <li>Is the code coverage 100%?</li>\n  <li>Does a check with <a href=\"https://github.com/PowerShell/PSScriptAnalyzer\">PSScriptAnalyzer</a> pass?</li>\n  <li>Is the code checked in to in a Source Control system?</li>\n  <li>Are you using a CI pipeline to test commits?</li>\n  <li>Are you publishing your script/module to the <a href=\"https://www.powershellgallery.com/\">PowerShell Gallery</a> (or other package repository)?</li>\n  <li>Is the module automatically published after a successful run of the CI pipeline?</li>\n  <li>Is there a readme.md, Wiki and/or other documentation?</li>\n  </ol>\n  \n  <p>Score: __ / 40</p>\n</blockquote>\n\n<p><strong>Penalty Points (-5 points each):</strong></p>\n\n<blockquote>\n  <ol>\n  <li>Does your function use <code>Write-Host</code> or a <code>Format-*</code> cmdlet where it should instead output an object?</li>\n  <li>Are you using cmdlet aliases?</li>\n  <li>Are there any non-meaningful variable names? e.g <code>$s</code> <code>$c</code></li>\n  <li>Do you use the <code>Return</code> keyword?</li>\n  <li>Have you disabled/suppressed standard errors? e.g with <code>$ErrorActionPreference = \"SilentlyContinue\"</code> or via <code>-ErrorAction</code></li>\n  </ol>\n  \n  <p>Score: __ / -25</p>\n</blockquote>\n\n<hr />\n\n<blockquote>\n  <p><strong>Final Score: __ / 50</strong></p>\n</blockquote>\n\n<hr />\n\n<p>In case any of the concepts above are unfamiliar to you below are explanations of them and why they are considered important:</p>\n\n<p><strong>1. Have you declared <code>[cmdletbinding()]</code>?</strong></p>\n\n<pre><code>[cmdletbinding()]\nParam( .. )  \n</code></pre>\n\n<p>By adding <code>[cmdletbinding()]</code> above the <code>Param()</code> block of a Function you gain a set of default functionality: the ability to add Write-Verbose statements to output helpful text when <code>-Verbose</code> is used, the same for <code>-Debug</code> as well as the ability to add <code>-WhatIf</code> and <code>-Confirm</code> functionality for functions that change the state of something. Even if you don't use any of these things, your function also gets the <code>-ErrorAction</code> and <code>-ErrorVariable</code> parameters which give the end user options for how they handle any errors your Function generates. There's no good reason I can think of to not use <code>[cmdletbinding()]</code> on any function that an end user interacts with.</p>\n\n<p><strong>2. Does your function include comment-based or external help?</strong></p>\n\n<pre><code>Function Do-Something { ..  \n  &lt;#\n  .SYNOPSIS\n      A function that does something.\n  .EXAMPLE\n      'Something' | Do-Something\n  #&gt;\n</code></pre>\n\n<p>It's really easy to add help output to a PowerShell function via a block of comments at the top of the function. Not only is this helpful to the end user who might use <code>Get-Help yourfunction</code> to work out how it works, but for anyone modifying your code in the future it gives them an excellent intro to what the function does and how it works.</p>\n\n<p><strong>3. Are your input parameters declared as specific types?</strong></p>\n\n<pre><code>[datetime]\n$Date\n</code></pre>\n\n<p>This is just a very easy way to do some input validation. There might occasionally be a scenario where you want a parameter to handle any type of input and then convert it to the relevant object in the Function and thats ok. You could probably though still declare that Parameter as a <code>[object]</code> type so that it's explicit that is what you are doing.</p>\n\n<p><strong>4. Are any mandatory parameters declared as such?</strong></p>\n\n<pre><code>[parameter(Mandatory)]\n$InputObject\n</code></pre>\n\n<p>If your function is just going to fail if one or more parameters aren't provided then it should inform the user gracefully of that before it even starts. Adding <code>[Parameter(Mandatory=$true)]</code> or (my preference) <code>[Parameter(Mandatory)]</code> before a mandatory parameter allows you to do that and let PowerShell handle the work of informing the user what they've done wrong.</p>\n\n<p><strong>5. Do your parameters have sensible defaults? (if possible)</strong></p>\n\n<pre><code>[ipaddress]\n$IPAddress = '127.0.0.1'\n</code></pre>\n\n<p>If there are some sensible default values for your parameters then you should set them within the <code>Param</code> block and ensure that you don't make those parameters mandatory (which is unnecessary and would render your default irrelevant). Sensible defaults make your tool easier to use, but make sure you expose those default values via the help text for those parameters so that it's transparent to the end user.</p>\n\n<p><strong>6. Does your function do just one thing?</strong></p>\n\n<p>This was a core concept covered in the Toolmaking book. As far as I interpret it, it doesn't suggest that your function can't do multiple transformations, but the idea is that your function should perform one distinct action. For example <code>Get-Service</code> only returns Service objects, it doesn't also allow a user to modify those services. That's what <code>Set-Service</code> is for. It might be that <code>Get-Service</code> queries different sources to get the properties of the service objects, but ultimately its job is to get services so that is all it does.</p>\n\n<p><strong>7. Does your function include <code>write-verbose</code> (or <code>write-debug</code>) statements? (if needed)</strong></p>\n\n<p>Per #1, cmdletbinding gives you the option to use <code>Write-Verbose</code> and/or <code>Write-Debug</code> to show additional output or to halt the script at key points when the end user uses the equivalent parameters on your command. In particular, <code>Write-Verbose</code> is preferable to anywhere you might have put in a single line comment in your code to explain a section. Using <code>Write-Verbose</code> still has the benefit of adding those explanations but it also exposes them to an end user when they choose to see them.</p>\n\n<p><strong>8. Do you <a href=\"https://technet.microsoft.com/en-us/library/2009.09.windowspowershell.aspx\">filter left</a>? (where applicable)</strong></p>\n\n<p>Anywhere you are filtering the result of a cmdlet you should do it as far left as possible. For example if i'm using <code>Get-ChildItem</code> and I want to return only .ps1 scripts then I should use the filtering parameter of that cmdlet. Or if I want to get services where the displayname property contains certain text and then iterate over those services, the <code>Where-Object</code> cmdlet that filters by displayname should occur directly after the <code>Get-Service</code> cmdlet, not further down the pipeline. Doing so will improve the performance of your script/function.</p>\n\n<p><strong>9. Does your function support <code>-whatif</code> and <code>-confirm</code> for any code that modifies something?</strong></p>\n\n<pre><code>[cmdletbinding(SupportsShouldProcess)]\nParam( .. )\n\n..\n\nif ($PSCmdlet.ShouldProcess('something')) { .. }  \n</code></pre>\n\n<p>Again per #1, this feature is made possible by adding cmdletbinding to your script. If your script makes changes to something, you should ensure that you've added the relevant code that stops those changes from occurring (or prompts a user) if they use <code>-whatif</code> or <code>-confirm</code>. This is done by adding a <code>If ($PSCmdlet.ShouldProcess(\"Message\"))</code> block around that part of your function that checks if these parameters have been used. Note that this should be around just the smallest part of your script possible that makes a modification, so that other (non target modifying) code still executes.</p>\n\n<p><strong>10. Is your function named <code>verb-noun</code> and using an <a href=\"https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx\">approved verb</a>?</strong></p>\n\n<p>Ensuring that you've followed the PowerShell standard of naming your function using a <code>Verb-Noun</code> structure and using one of the <a href=\"https://msdn.microsoft.com/en-us/library/ms714428(v=vs.85).aspx\">approved verbs</a> will help make your cmdlet more consistent with the standard set of PowerShell cmdlets as well as others following best practice on the web.</p>\n\n<p><img src=\"/content/images/2018/01/PowerShell-Core-Get-Verb.jpg\" alt=\"PowerShell Core now includes a description column output by Get-Verb\" /></p>\n\n<p>This makes your function easier to understand and easier to discover. Equally you should try and follow existing PowerShell practice when naming your parameters. For example generally PowerShell uses <code>-ComputerName</code> when a cmdlet can be targeted at a remote machine, you should do the same vs something like <code>-Host</code> or <code>-Server</code>.</p>\n\n<h3 id=\"bonusquestions\">Bonus Questions</h3>\n\n<p><strong>1. Are there <a href=\"https://github.com/pester/Pester\">Pester</a> (or other testing framework) tests?</strong></p>\n\n<p><a href=\"https://github.com/pester/Pester\">Pester</a> is a fantastic tool for writing tests for your code to ensure they perform the way your expect. You should consider writing both unit tests (which test each part of your code) and integration tests (which test how the code actually performs, ideally in some sort of test environment/container). At a minimum unit tests should be used and you can do so even where you don't have access to any dependencies by using mocking.</p>\n\n<p><strong>2. Is the code coverage 100%?</strong></p>\n\n<p>Pester has a <code>-CodeCoverage</code> switch that allows you to see exactly what part of your code is covered by your existing tests. Getting this to 100% still isn't a guarantee that your code doesn't have bugs but its better than anything less.</p>\n\n<p><strong>3. Does a check with <a href=\"https://github.com/PowerShell/PSScriptAnalyzer\">PSScriptAnalyzer</a> pass?</strong></p>\n\n<p>Using ScriptAnalyzer to evaluate your code helps enforce a number of these best practices. It also checks your code for dangerous behaviour, such as handling credentials as plain text instead of via a credential object.</p>\n\n<p><strong>4. Is the code checked in to in a Source Control system?</strong></p>\n\n<p>Even if you are the only person using the code, having it in a source control system means that you have version control and the ability to step backwards if things go wonky. If you aren't the only person using the code, source control allows others to review it and contribute to it.</p>\n\n<p><strong>5. Are you using a CI pipeline to test commits?</strong></p>\n\n<p>As well as writing Pester (or some other framework) tests for your code you should make sure these tests are run every time the code changes. A CI pipeline (e.g with something like AppVeyor) can automate this process. I also use a CI pipeline to execute ScriptAnalyzer on every commit.</p>\n\n<p><strong>6. Are you publishing your script/module to the <a href=\"https://www.powershellgallery.com/\">PowerShell Gallery</a> (or other package repository)?</strong></p>\n\n<p>If you've written a tool that others might find useful or you want to distribute it (publicly) do so via the PowerShell Gallery so that installing your tool can be done simply via <code>Install-Module</code> or <code>Install-Script</code>. If you are working on private code, a private package management repository is probably still a good idea to manage distribution and production versions of your code.</p>\n\n<p><strong>7. Is the module automatically published after a successful run of the CI pipeline?</strong></p>\n\n<p>Per the above, a CI pipeline should also publich the module to the appropriate package management tool so that this isn't a manual process. This should only occur if all your tests have passed.</p>\n\n<p><strong>8. Is there a readme.md, Wiki and/or other documentation?</strong></p>\n\n<p>If you're using GitHub, your project should have a <code>readme.md</code> that explains what the project is about, how to install it and some basic intro for how it works. If the project is of significant complexity a wiki or other documentation might be justifiable.</p>\n\n<h3 id=\"penaltypoints\">Penalty Points</h3>\n\n<p><em>-- Disclaimer: There's always an exception to a rule and no one is perfect.</em></p>\n\n<p><strong>1. Does your function use <code>Write-Host</code> or a <code>Format-*</code> cmdlet where it should instead output a usable object?</strong></p>\n\n<p>Generally (but not exclusively) using <code>Write-Host</code> is considered bad practice because it sends output to the console rather than returning an object that the end user can manipulate or send on to another cmdlet. Equally making use of the <code>Format</code> cmdlets in your function gives the user no options for how they handle your output. Instead you should return an object and let them pipe it to a <code>Format</code> cmdlet if that's what they want. Or use the format XML feature to define how output should appear by default while still returning it as an object.</p>\n\n<p><strong>2. Are you using cmdlet aliases?</strong></p>\n\n<p><img src=\"/content/images/2018/01/PowerShell-Core-Get-Alias.jpg\" alt=\"\" /></p>\n\n<p>PSScriptAnalyzer will complain if you do this also (unless you suppress the relevant rule). Its considered an anti-pattern because aliases are less explicit to anyone reading your code. Generally you should only use aliases at the console to shorten your typing, but when writing a script you should always use the full cmdlet names.</p>\n\n<p><strong>3. Are there any non-meaningful variable names? e.g <code>$s</code> <code>$c</code></strong></p>\n\n<p>Your variable names should be descriptive regarding their purpose. Non descriptive names are unhelpful to anyone else reading your code.</p>\n\n<p><strong>4. Do you use the <code>Return</code> keyword?</strong></p>\n\n<p>The return keyword is generally not needed and probably doesn't do what you think it does.</p>\n\n<p><strong>5. Have you disabled/suppressed standard errors? e.g with <code>$ErrorActionPreference = \"SilentlyContinue\"</code> or via <code>-ErrorAction</code></strong></p>\n\n<p>You ideally shouldn't suppress the standard error output of the cmdlets you use in your function. By all means handle them via <code>Try..Catch</code> but its generally considered a bad idea to hide error states from end users.</p>\n\n<hr />\n\n<p>If you disagree with any of the above (or spot any inaccuracies) please let me know via the comments below.</p>",
                        "image": "/content/images/2017/03/CheckList.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-03-14 13:54:50",
                        "created_by": 1,
                        "updated_at": "2018-01-17 13:41:58",
                        "updated_by": 1,
                        "published_at": "2018-01-17 13:38:30",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 57,
                        "uuid": "c542c956-790d-4fcd-a858-8845eabf96dd",
                        "title": "Random",
                        "slug": "random",
                        "markdown": "Getting a single random item is easy\n\n    Get-Service | Get-Random\n\nBut what if you want to return more than one? You could do a loop: \n\n    1..3 | ForEach-Object { Get-Service | Get-Random }\n\nBut you might get the same one come back twice. What if you want them to be unique?\n\nYou can do this:\n\n    Get-Service | Sort-Object {Get-Random} | Select -First 3\n\nSource: http://ilovepowershell.com/2015/01/24/easiest-way-shuffle-array-powershell/\n\n1..10 | ForEach-Object { Get-Random -Min 1 -Max 10 }\n\n$Heroes | Sort-Object {Get-Random}\n\nGet-Random -Min 1 -Max 100\n\n\n$Heroes = @('Tony','Bruce','Clark','Peter','Diana')\n\nGet-Random $Heroes -Count $Heroes.count\n\n$Heroes | Sort-Object {Get-Random}\n\n\n\n1..10 | Get-Random -Count ([int]::MaxValue)",
                        "html": "<p>Getting a single random item is easy</p>\n\n<pre><code>Get-Service | Get-Random\n</code></pre>\n\n<p>But what if you want to return more than one? You could do a loop: </p>\n\n<pre><code>1..3 | ForEach-Object { Get-Service | Get-Random }\n</code></pre>\n\n<p>But you might get the same one come back twice. What if you want them to be unique?</p>\n\n<p>You can do this:</p>\n\n<pre><code>Get-Service | Sort-Object {Get-Random} | Select -First 3\n</code></pre>\n\n<p>Source: <a href=\"http://ilovepowershell.com/2015/01/24/easiest-way-shuffle-array-powershell/\">http://ilovepowershell.com/2015/01/24/easiest-way-shuffle-array-powershell/</a></p>\n\n<p>1..10 | ForEach-Object { Get-Random -Min 1 -Max 10 }</p>\n\n<p>$Heroes | Sort-Object {Get-Random}</p>\n\n<p>Get-Random -Min 1 -Max 100</p>\n\n<p>$Heroes = @('Tony','Bruce','Clark','Peter','Diana')</p>\n\n<p>Get-Random $Heroes -Count $Heroes.count</p>\n\n<p>$Heroes | Sort-Object {Get-Random}</p>\n\n<p>1..10 | Get-Random -Count ([int]::MaxValue)</p>",
                        "image": "/content/images/2017/06/dice_game_black_background_close_up__u_2560x1600.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-05-29 15:40:58",
                        "created_by": 1,
                        "updated_at": "2017-07-06 08:25:43",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 59,
                        "uuid": "2dc42b78-a455-483a-b76a-ebbf142d028b",
                        "title": "Unit testing with Pester",
                        "slug": "unit-testing-with-pester",
                        "markdown": "I have previously written about getting started with Pester and a couple of Pester uses, focussed around infrastructure and application testing. However while these are valid uses, Pester was created to be a BDD style testing framework for PowerShell.\n\nhttps://codeutopia.net/blog/2015/03/01/unit-testing-tdd-and-bdd/\n\n- TDD\n- Unit Testing\n- Creating unit tests for PowerShell\n- Mocking\n- Code Coverage\n- Integration Testing",
                        "html": "<p>I have previously written about getting started with Pester and a couple of Pester uses, focussed around infrastructure and application testing. However while these are valid uses, Pester was created to be a BDD style testing framework for PowerShell.</p>\n\n<p><a href=\"https://codeutopia.net/blog/2015/03/01/unit-testing-tdd-and-bdd/\">https://codeutopia.net/blog/2015/03/01/unit-testing-tdd-and-bdd/</a></p>\n\n<ul>\n<li>TDD</li>\n<li>Unit Testing</li>\n<li>Creating unit tests for PowerShell</li>\n<li>Mocking</li>\n<li>Code Coverage</li>\n<li>Integration Testing</li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-06-20 20:47:01",
                        "created_by": 1,
                        "updated_at": "2017-06-20 20:51:10",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 60,
                        "uuid": "2cfc46d0-6af5-4e4c-923e-7bb5a3e0c77d",
                        "title": "Managing SSRS Subscriptions with PowerShell",
                        "slug": "managing-sql-server-reporting-services-subscriptions-with-powershell",
                        "markdown": "I was recently tasked with investigating how we could store the configuration of our SQL Server Reporting Services report subscriptions  in a source control and then automate the process of (later) configuring them in one or more new SSRS servers.\n\nMy first instinct was to look for a PowerShell module and I discovered a [Microsoft maintained module named ReportingServicesTools](https://github.com/Microsoft/ReportingServicesTools) in Github and published to the Powershell Gallery.\n\nIf you have Powershell 5 or newer you can install the module with:\n\n    Install-Module ReportingServicesTools\n\nFor older versions of PowerShell the module has an `install.ps1` script that you can invoke as follows:\n\n    Invoke-Expression (Invoke-WebRequest https://aka.ms/rstools)\n\nI was pleased to discover that [Cláudio Silva](https://claudioessilva.eu/) had recently contributed two cmdlets to the module that looked to meet my need:\n\n- `Get-RsSubscription` -- retrieves subscriptions from a specified report path or folder. \n- `Set-RsSubscription` -- creates subscriptions that have been retrieved and piped in via the aforementioned command, in a specified server and folder.\n\nThese cmdlets were written to allow someone to clone reports or copy them between systems by simply doing:\n\n    Get-RsSubscription -Path '/old/report' | Set-RsSubscription -Path '/new/report'\n\nHowever unfortunately they only work if the action is done in a single session, exporting the result of `Get-RsSubscription` to disk via `Export-CliXML` (per my need) and then reimporting it with `Import-CliXML` made the resultant object invalid for the `Set-RsSubscription` cmdlet due to the object being deserialized and the SOAP API requiring strongly typed inputs.\n\n---\n\n**TL;DR:** As a result, I have [updated the ReportingServicesTools module](https://github.com/Microsoft/ReportingServicesTools/pull/79) to add cmdlets which enable subscriptions to be stored on disk:\n\n- `Export-RsSubscriptionXml` -- exports subscriptions to disk using Export-CliXML with `-Depth` set to 3.\n- `Import-RsSubscriptionXml` -- imports subscriptions from one or more XML files on disk and recreates a number of the properties to make them valid object types for `Set-RsSubscription`. \n\nI also had a need to retrieve subscriptions from a SQL Server 2008 SSRS system (which uses an older version of the SOAP API) and as such amended the module to add some backwards compatibility support for this API via an `-ApiVersion` parameter.\n\nAs a result you can now save subscriptions to disk by doing:\n\n    Get-RsSubscription -Path '/old/report' -ApiVersion 2005 | Export-RsSubscription SomeFile.xml\n\n*-- `-ApiVersion` is not needed if you're exporting from a SQL Server 2008 R2 or newer system.*\n\nAnd import them in later by doing:\n\n    Import-RsSubscription SomeFile.xml | Set-RsSubscription -Path '/new/report'\n\nI've also added the following two cmdlets to allow subscriptions to be created from scratch directly with PowerShell:\n\n- `New-RsSubscription` -- creates subscriptions in a specified location using settings defined by the cmdlets various parameters.\n- `New-RsScheduleXml` -- use this helper function to generate the XML that you need to pass to the `New-RsSubscription -Schedule` parameter if you want to create a scheduled/recurring subscription.\n\nRead on below for more in depth information about how these work.\n\n> Were it not for Cláudio's cmdlets as well as the work of the others in the module this would have been much more difficult to figure out. I'm also really grateful for the help and support he offered me via Twitter, reviewing my code (while on holiday no less!).\n>\n> It's also great that Microsoft are maintaining things like this as open source and are supportive in helping new contributors understand how to meet the standards for the project.\n\n---\n\nSSRS uses a SOAP based API for interacting with the system config. It also has a REST API, but that is for interacting with SSRS setup/config. \n\nThese new cmdlets (where applicable) interact with the SOAP API.\n\n## Export-RsSubscriptionXml\n[[Source code]](https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/Export-RsSubscriptionXml.ps1)\n\n```\nSYNTAX\n    Export-RsSubscriptionXml [-Path] <String> -Subscription <Object> [-WhatIf] [-Confirm] [<CommonParameters>]\n```\nThis cmdlet really only exists to compliment the more complex and necessary `Import-RsSubscriptionXml` cmdlet described below. This cmdlet is just a wrapper around `Export-CliXML` that ensures you also set `-Depth 3`, as the default depth of 2 doesn't result in fully exporting all properties of the subscription.\n\n**Usage Examples**:\n\nExport the current set of subscriptions contained in '/path/to/my/report' to an XML file named MySubscriptions.xml: \n```\nGet-RsSubscription -path '/path/to/my/report' | Export-RsSubscriptionXml .\\MySubscriptions.xml\n```\n\n## Import-RsSubscriptionXml\n\n[[Source code]](https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/Import-RsSubscriptionXml.ps1)\n\n```\nSYNTAX\n    Import-RsSubscriptionXml [-Path] <String> [-ReportServerUri <String>] [-Credential <PSCredential>] [-Proxy <Object>] [<CommonParameters>]\n```\n\nThis cmdlet uses `Import-CliXml` to read subscriptions from one or more XML files on disk as saved by the above cmdlet. At this point the subscription object has been recreated, but a number of sub-properties are rendered invalid as input to the Reporting Services SOAP API due to being deserialized. As such, this cmdlet then recreates each of those properties. It does this by first creating a web proxy connection to the SOAP web service which it then uses to access the class and methods of the API, as [documented here](https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/methods/report-server-web-service-methods).\n\nThe properties that get recreated are:\n\n- `.DeliverySettings`\n  - `.ParameterValues` : these get recreated as either a `.ParameterValue` or a `.ParameterFieldReference` object.\n- `.DataRetrievalPlan` (for Data Driven subscriptions) and it's sub properties:\n  - `.Query`\n  - `.DataSet`\n  - `.Reference`\n- `.Values`\n\nThe script simply loops through the values for each of these properties that were loaded from the XML file and then uses `New-Object` to create the corresponding object type needed to populate these new properties with the existing values.\n\nWhen it has finished, it outputs the subscriptions as valid PowerShell objects, which means they can then be consumed by `Set-RsSubscription` via the pipeline.\n\n**Usage Example:**\n\nImport the subscriptions contained in .\\MySubscriptions.xml, recreate any SSRS specific properties  and pipe the results to Set-RsSubscription which will add them to the /Example/Report report:\n```\nImport-RsSubscriptionXml .\\MySubscriptions.xml | Set-RsSubscription -Path /Example/Report\n```\n## New-RsSubscription\n\n[[Source Code]](https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/New-RsSubscription.ps1)\n\nThis cmdlet allows you to create new subscriptions from scratch by defining the settings via it's parameters. \n\nThere are different parameter set requirements depending on whether the destination of the subscription is a file share or email (as defined by the `-Destination` parameter). The function does not currently support setting SharePoint as a destination.\n\n```\nSYNTAX\n    New-RsSubscription [-ReportServerUri <String>] [-Credential <PSCredential>] [-Proxy <Object>] -Path <String> [-Description <String>] [-EventType <String>]\n    -Destination <String> -Schedule <String> -DestinationPath <String> -Filename <String> -RenderFormat <String> [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsSubscription [-ReportServerUri <String>] [-Credential <PSCredential>] [-Proxy <Object>] -Path <String> [-Description <String>] [-EventType <String>]\n    -Destination <String> -Schedule <String> -To <String> [-CC <String>] [-BCC <String>] [-ExcludeReport] -Subject <String> -RenderFormat <String> [-WhatIf] [-Confirm]\n    [<CommonParameters>]\n```\n\nSimilar to Import-RsSubscriptionXml, this is using the proxy to interact with the necessary class in order to build the appropriate object that the SOAP API requires for the [CreateSubscription](https://msdn.microsoft.com/library/reportservice2010.reportingservice2010.createsubscription(v=SQL.130).aspx) method.\n\n**Usage Example:**\n\nCreate a subscription for the report/s in the specified `-Path` that emails them fortnightly on Saturday to the specified address in PDF format:\n\n    New-RsSubscription -ReportServerUri http://yourserver/Reportserver -path /some/path -Description 'Report A Fortnightly by email' -Destination 'Email' -Schedule (New-RsScheduleXML -Weekly -Interval 2 -DaysOfWeek Saturday) -Subject 'Fortnightly Report A' -To 'test@someaddress.com' -RenderFormat 'PDF'\n\n\n## New-RsScheduleXml\n\n[[Source Code]](https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/New-RsScheduleXML.ps1)\n\nThis cmdlet generates the XML that the `-Schedule` property requires in the `New-RsSubscription` cmdlet. The `CreateSubscription` method of the SOAP API requires this particular input be in XML and the XML needs to be structured in a specific way.\n\n> This cmdlet is designed to work in a similar way to `New-ScheduledTaskTrigger` which is a helper cmdlet you use with `New-ScheduledTask` to create a valid input for it's `-Trigger` parameter.\n\nYou can generate a number of different schedule types (these same options are available via the UI when you create a subscription in SSRS): \n\n- Once: Runs once at a specified date/time.\n- Minute: repeats every X minutes.\n- Daily: repeats every X days.\n- Weekly: repeats every X weeks and you an optionaly specity DaysOfWeek to make it repeat only on specified named days of the week.\n- Monthly: repeats every month on the specified days (which is a string where you can provide specific dates or a range, e.g 1,3,5,10-15). You can also optionally specify the months this is valid for (e.g January,June,October) rather than every month.\n- MonthlyDayOfWeek: repeats every month on a specific week of the month (e.g First, Second, Third, Last). You can also optionally specify the Days of the Week.\n\nFor each of these schedules you can also specify a Start and End date/time period during which the schedule is valid.\n\nHere's the full Syntax from the `get-help`:\n\n```\nSYNTAX\n    New-RsScheduleXml [-Once] [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsScheduleXml [-Minute] [[-Interval] <Int32>] [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsScheduleXml [-Daily] [[-Interval] <Int32>] [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsScheduleXml [-Weekly] [[-Interval] <Int32>] [-DaysOfWeek <String[]>] [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsScheduleXml [-Monthly] [-Months <String[]>] -DaysOfMonth <String> [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm] [<CommonParameters>]\n\n    New-RsScheduleXml [-MonthlyDayOfWeek] [-DaysOfWeek <String[]>] [-Months <String[]>] -WeekOfMonth <String> [-Start <DateTime>] [-End <DateTime>] [-WhatIf] [-Confirm]\n    [<CommonParameters>]\n```\n\n**Usage Examples:**\n\nCreate an XML string to schedule a subscription to run once at the date/time specified:\n```\nNew-RsScheduleXml -Once -Start '01/02/2017 12:34'\n```\nCreate an XML string to schedule a subscription to run every 90 minutes starting at 9pm:\n```\nNew-RsScheduleXml -Minute -Interval 90 -Start '21:00'\n```\nCreate an XML string to schedule a subscription to run every 5 days starting at the current date/time:\n```\nNew-RsScheduleXml -Daily -Interval 5\n```\nCreate an XML string to schedule a subscription to run every 5 days starting at the current date/time:\n```\nNew-RsScheduleXml -Weekly -Interval 2 -DaysOfWeek Monday,Tuesday,Wednesday,Thursday,Friday\n```\n---\n## Limitations\n\nThere are a few limitations to the cmdlets at the moment to be aware of:\n\n- `New-RsSubscription` doesn't currently support creating subscriptions within reports that have one or more parameters (defined in the report).\n- `New-RsSubscription` doesn't currently support the creation of data driven subscriptions.\n- `Set-RsSubscription` does not support creating subscriptions using the older API version (you can just retrieve them with the `Get-` cmdlet).\n\nIf you find any other issues/limitations to the cmdlets please raise them as [issues in GitHub](https://github.com/Microsoft/ReportingServicesTools/issues).\n\n## Useful Links\n\nIf you are looking to make modifications to the [ReportingServicesTools](https://github.com/Microsoft/ReportingServicesTools) module or are looking to develop solutions around the SSRS API, these links may be helpful:\n\n- http://www.tek-tips.com/viewthread.cfm?qid=1780331\n- https://sqlblogcasts.com/blogs/sqlandthelike/archive/2013/02/12/deploying-ssrs-artefacts-using-powershell-simply.aspx\n- http://www.techtree.co.uk/windows-server/windows-powershell/powershell-ssrs-function-export-ssrsreports-to-rdl-files/\n- https://msdn.microsoft.com/en-us/library/reportservice2010.reportingservice2010.aspx\n- http://odetocode.com/Articles/114.aspx\n- http://www.sqlservercurry.com/2009/07/programmatically-create-data-driven.html\n- http://jaliyaudagedara.blogspot.co.uk/2012/10/creating-data-driven-subscription.html\n- https://www.codeproject.com/Questions/1034771/SSRS-Programmatically-Create-Subscriptions\n- http://www.sqlservercentral.com/blogs/cl%C3%A1udio-silva/2017/07/27/does-that-copy-subscriptions-too-now-it-does-new-powershell-ssrs-commands/\n- https://www.mssqltips.com/sqlservertip/4738/powershell-commands-for-sql-server-reporting-services/\n- https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/accessing-the-soap-api\n- https://www.codeproject.com/Articles/36009/Programmatically-Playing-With-SSRS-Subscriptions",
                        "html": "<p>I was recently tasked with investigating how we could store the configuration of our SQL Server Reporting Services report subscriptions  in a source control and then automate the process of (later) configuring them in one or more new SSRS servers.</p>\n\n<p>My first instinct was to look for a PowerShell module and I discovered a <a href=\"https://github.com/Microsoft/ReportingServicesTools\">Microsoft maintained module named ReportingServicesTools</a> in Github and published to the Powershell Gallery.</p>\n\n<p>If you have Powershell 5 or newer you can install the module with:</p>\n\n<pre><code>Install-Module ReportingServicesTools\n</code></pre>\n\n<p>For older versions of PowerShell the module has an <code>install.ps1</code> script that you can invoke as follows:</p>\n\n<pre><code>Invoke-Expression (Invoke-WebRequest https://aka.ms/rstools)\n</code></pre>\n\n<p>I was pleased to discover that <a href=\"https://claudioessilva.eu/\">Cláudio Silva</a> had recently contributed two cmdlets to the module that looked to meet my need:</p>\n\n<ul>\n<li><code>Get-RsSubscription</code> -- retrieves subscriptions from a specified report path or folder. </li>\n<li><code>Set-RsSubscription</code> -- creates subscriptions that have been retrieved and piped in via the aforementioned command, in a specified server and folder.</li>\n</ul>\n\n<p>These cmdlets were written to allow someone to clone reports or copy them between systems by simply doing:</p>\n\n<pre><code>Get-RsSubscription -Path '/old/report' | Set-RsSubscription -Path '/new/report'\n</code></pre>\n\n<p>However unfortunately they only work if the action is done in a single session, exporting the result of <code>Get-RsSubscription</code> to disk via <code>Export-CliXML</code> (per my need) and then reimporting it with <code>Import-CliXML</code> made the resultant object invalid for the <code>Set-RsSubscription</code> cmdlet due to the object being deserialized and the SOAP API requiring strongly typed inputs.</p>\n\n<hr />\n\n<p><strong>TL;DR:</strong> As a result, I have <a href=\"https://github.com/Microsoft/ReportingServicesTools/pull/79\">updated the ReportingServicesTools module</a> to add cmdlets which enable subscriptions to be stored on disk:</p>\n\n<ul>\n<li><code>Export-RsSubscriptionXml</code> -- exports subscriptions to disk using Export-CliXML with <code>-Depth</code> set to 3.</li>\n<li><code>Import-RsSubscriptionXml</code> -- imports subscriptions from one or more XML files on disk and recreates a number of the properties to make them valid object types for <code>Set-RsSubscription</code>. </li>\n</ul>\n\n<p>I also had a need to retrieve subscriptions from a SQL Server 2008 SSRS system (which uses an older version of the SOAP API) and as such amended the module to add some backwards compatibility support for this API via an <code>-ApiVersion</code> parameter.</p>\n\n<p>As a result you can now save subscriptions to disk by doing:</p>\n\n<pre><code>Get-RsSubscription -Path '/old/report' -ApiVersion 2005 | Export-RsSubscription SomeFile.xml\n</code></pre>\n\n<p><em>-- <code>-ApiVersion</code> is not needed if you're exporting from a SQL Server 2008 R2 or newer system.</em></p>\n\n<p>And import them in later by doing:</p>\n\n<pre><code>Import-RsSubscription SomeFile.xml | Set-RsSubscription -Path '/new/report'\n</code></pre>\n\n<p>I've also added the following two cmdlets to allow subscriptions to be created from scratch directly with PowerShell:</p>\n\n<ul>\n<li><code>New-RsSubscription</code> -- creates subscriptions in a specified location using settings defined by the cmdlets various parameters.</li>\n<li><code>New-RsScheduleXml</code> -- use this helper function to generate the XML that you need to pass to the <code>New-RsSubscription -Schedule</code> parameter if you want to create a scheduled/recurring subscription.</li>\n</ul>\n\n<p>Read on below for more in depth information about how these work.</p>\n\n<blockquote>\n  <p>Were it not for Cláudio's cmdlets as well as the work of the others in the module this would have been much more difficult to figure out. I'm also really grateful for the help and support he offered me via Twitter, reviewing my code (while on holiday no less!).</p>\n  \n  <p>It's also great that Microsoft are maintaining things like this as open source and are supportive in helping new contributors understand how to meet the standards for the project.</p>\n</blockquote>\n\n<hr />\n\n<p>SSRS uses a SOAP based API for interacting with the system config. It also has a REST API, but that is for interacting with SSRS setup/config. </p>\n\n<p>These new cmdlets (where applicable) interact with the SOAP API.</p>\n\n<h2 id=\"exportrssubscriptionxml\">Export-RsSubscriptionXml</h2>\n\n<p><a href=\"https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/Export-RsSubscriptionXml.ps1\">[Source code]</a></p>\n\n<pre><code>SYNTAX  \n    Export-RsSubscriptionXml [-Path] &lt;String&gt; -Subscription &lt;Object&gt; [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n</code></pre>\n\n<p>This cmdlet really only exists to compliment the more complex and necessary <code>Import-RsSubscriptionXml</code> cmdlet described below. This cmdlet is just a wrapper around <code>Export-CliXML</code> that ensures you also set <code>-Depth 3</code>, as the default depth of 2 doesn't result in fully exporting all properties of the subscription.</p>\n\n<p><strong>Usage Examples</strong>:</p>\n\n<p>Export the current set of subscriptions contained in '/path/to/my/report' to an XML file named MySubscriptions.xml:  </p>\n\n<pre><code>Get-RsSubscription -path '/path/to/my/report' | Export-RsSubscriptionXml .\\MySubscriptions.xml  \n</code></pre>\n\n<h2 id=\"importrssubscriptionxml\">Import-RsSubscriptionXml</h2>\n\n<p><a href=\"https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/Import-RsSubscriptionXml.ps1\">[Source code]</a></p>\n\n<pre><code>SYNTAX  \n    Import-RsSubscriptionXml [-Path] &lt;String&gt; [-ReportServerUri &lt;String&gt;] [-Credential &lt;PSCredential&gt;] [-Proxy &lt;Object&gt;] [&lt;CommonParameters&gt;]\n</code></pre>\n\n<p>This cmdlet uses <code>Import-CliXml</code> to read subscriptions from one or more XML files on disk as saved by the above cmdlet. At this point the subscription object has been recreated, but a number of sub-properties are rendered invalid as input to the Reporting Services SOAP API due to being deserialized. As such, this cmdlet then recreates each of those properties. It does this by first creating a web proxy connection to the SOAP web service which it then uses to access the class and methods of the API, as <a href=\"https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/methods/report-server-web-service-methods\">documented here</a>.</p>\n\n<p>The properties that get recreated are:</p>\n\n<ul>\n<li><code>.DeliverySettings</code>\n<ul><li><code>.ParameterValues</code> : these get recreated as either a <code>.ParameterValue</code> or a <code>.ParameterFieldReference</code> object.</li></ul></li>\n<li><code>.DataRetrievalPlan</code> (for Data Driven subscriptions) and it's sub properties:\n<ul><li><code>.Query</code></li>\n<li><code>.DataSet</code></li>\n<li><code>.Reference</code></li></ul></li>\n<li><code>.Values</code></li>\n</ul>\n\n<p>The script simply loops through the values for each of these properties that were loaded from the XML file and then uses <code>New-Object</code> to create the corresponding object type needed to populate these new properties with the existing values.</p>\n\n<p>When it has finished, it outputs the subscriptions as valid PowerShell objects, which means they can then be consumed by <code>Set-RsSubscription</code> via the pipeline.</p>\n\n<p><strong>Usage Example:</strong></p>\n\n<p>Import the subscriptions contained in .\\MySubscriptions.xml, recreate any SSRS specific properties  and pipe the results to Set-RsSubscription which will add them to the /Example/Report report:  </p>\n\n<pre><code>Import-RsSubscriptionXml .\\MySubscriptions.xml | Set-RsSubscription -Path /Example/Report  \n</code></pre>\n\n<h2 id=\"newrssubscription\">New-RsSubscription</h2>\n\n<p><a href=\"https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/New-RsSubscription.ps1\">[Source Code]</a></p>\n\n<p>This cmdlet allows you to create new subscriptions from scratch by defining the settings via it's parameters. </p>\n\n<p>There are different parameter set requirements depending on whether the destination of the subscription is a file share or email (as defined by the <code>-Destination</code> parameter). The function does not currently support setting SharePoint as a destination.</p>\n\n<pre><code>SYNTAX  \n    New-RsSubscription [-ReportServerUri &lt;String&gt;] [-Credential &lt;PSCredential&gt;] [-Proxy &lt;Object&gt;] -Path &lt;String&gt; [-Description &lt;String&gt;] [-EventType &lt;String&gt;]\n    -Destination &lt;String&gt; -Schedule &lt;String&gt; -DestinationPath &lt;String&gt; -Filename &lt;String&gt; -RenderFormat &lt;String&gt; [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsSubscription [-ReportServerUri &lt;String&gt;] [-Credential &lt;PSCredential&gt;] [-Proxy &lt;Object&gt;] -Path &lt;String&gt; [-Description &lt;String&gt;] [-EventType &lt;String&gt;]\n    -Destination &lt;String&gt; -Schedule &lt;String&gt; -To &lt;String&gt; [-CC &lt;String&gt;] [-BCC &lt;String&gt;] [-ExcludeReport] -Subject &lt;String&gt; -RenderFormat &lt;String&gt; [-WhatIf] [-Confirm]\n    [&lt;CommonParameters&gt;]\n</code></pre>\n\n<p>Similar to Import-RsSubscriptionXml, this is using the proxy to interact with the necessary class in order to build the appropriate object that the SOAP API requires for the <a href=\"https://msdn.microsoft.com/library/reportservice2010.reportingservice2010.createsubscription(v=SQL.130).aspx\">CreateSubscription</a> method.</p>\n\n<p><strong>Usage Example:</strong></p>\n\n<p>Create a subscription for the report/s in the specified <code>-Path</code> that emails them fortnightly on Saturday to the specified address in PDF format:</p>\n\n<pre><code>New-RsSubscription -ReportServerUri http://yourserver/Reportserver -path /some/path -Description 'Report A Fortnightly by email' -Destination 'Email' -Schedule (New-RsScheduleXML -Weekly -Interval 2 -DaysOfWeek Saturday) -Subject 'Fortnightly Report A' -To 'test@someaddress.com' -RenderFormat 'PDF'\n</code></pre>\n\n<h2 id=\"newrsschedulexml\">New-RsScheduleXml</h2>\n\n<p><a href=\"https://github.com/Microsoft/ReportingServicesTools/blob/master/ReportingServicesTools/Functions/CatalogItems/New-RsScheduleXML.ps1\">[Source Code]</a></p>\n\n<p>This cmdlet generates the XML that the <code>-Schedule</code> property requires in the <code>New-RsSubscription</code> cmdlet. The <code>CreateSubscription</code> method of the SOAP API requires this particular input be in XML and the XML needs to be structured in a specific way.</p>\n\n<blockquote>\n  <p>This cmdlet is designed to work in a similar way to <code>New-ScheduledTaskTrigger</code> which is a helper cmdlet you use with <code>New-ScheduledTask</code> to create a valid input for it's <code>-Trigger</code> parameter.</p>\n</blockquote>\n\n<p>You can generate a number of different schedule types (these same options are available via the UI when you create a subscription in SSRS): </p>\n\n<ul>\n<li>Once: Runs once at a specified date/time.</li>\n<li>Minute: repeats every X minutes.</li>\n<li>Daily: repeats every X days.</li>\n<li>Weekly: repeats every X weeks and you an optionaly specity DaysOfWeek to make it repeat only on specified named days of the week.</li>\n<li>Monthly: repeats every month on the specified days (which is a string where you can provide specific dates or a range, e.g 1,3,5,10-15). You can also optionally specify the months this is valid for (e.g January,June,October) rather than every month.</li>\n<li>MonthlyDayOfWeek: repeats every month on a specific week of the month (e.g First, Second, Third, Last). You can also optionally specify the Days of the Week.</li>\n</ul>\n\n<p>For each of these schedules you can also specify a Start and End date/time period during which the schedule is valid.</p>\n\n<p>Here's the full Syntax from the <code>get-help</code>:</p>\n\n<pre><code>SYNTAX  \n    New-RsScheduleXml [-Once] [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsScheduleXml [-Minute] [[-Interval] &lt;Int32&gt;] [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsScheduleXml [-Daily] [[-Interval] &lt;Int32&gt;] [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsScheduleXml [-Weekly] [[-Interval] &lt;Int32&gt;] [-DaysOfWeek &lt;String[]&gt;] [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsScheduleXml [-Monthly] [-Months &lt;String[]&gt;] -DaysOfMonth &lt;String&gt; [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm] [&lt;CommonParameters&gt;]\n\n    New-RsScheduleXml [-MonthlyDayOfWeek] [-DaysOfWeek &lt;String[]&gt;] [-Months &lt;String[]&gt;] -WeekOfMonth &lt;String&gt; [-Start &lt;DateTime&gt;] [-End &lt;DateTime&gt;] [-WhatIf] [-Confirm]\n    [&lt;CommonParameters&gt;]\n</code></pre>\n\n<p><strong>Usage Examples:</strong></p>\n\n<p>Create an XML string to schedule a subscription to run once at the date/time specified:  </p>\n\n<pre><code>New-RsScheduleXml -Once -Start '01/02/2017 12:34'  \n</code></pre>\n\n<p>Create an XML string to schedule a subscription to run every 90 minutes starting at 9pm:  </p>\n\n<pre><code>New-RsScheduleXml -Minute -Interval 90 -Start '21:00'  \n</code></pre>\n\n<p>Create an XML string to schedule a subscription to run every 5 days starting at the current date/time:  </p>\n\n<pre><code>New-RsScheduleXml -Daily -Interval 5  \n</code></pre>\n\n<p>Create an XML string to schedule a subscription to run every 5 days starting at the current date/time:  </p>\n\n<pre><code>New-RsScheduleXml -Weekly -Interval 2 -DaysOfWeek Monday,Tuesday,Wednesday,Thursday,Friday  \n</code></pre>\n\n<hr />\n\n<h2 id=\"limitations\">Limitations</h2>\n\n<p>There are a few limitations to the cmdlets at the moment to be aware of:</p>\n\n<ul>\n<li><code>New-RsSubscription</code> doesn't currently support creating subscriptions within reports that have one or more parameters (defined in the report).</li>\n<li><code>New-RsSubscription</code> doesn't currently support the creation of data driven subscriptions.</li>\n<li><code>Set-RsSubscription</code> does not support creating subscriptions using the older API version (you can just retrieve them with the <code>Get-</code> cmdlet).</li>\n</ul>\n\n<p>If you find any other issues/limitations to the cmdlets please raise them as <a href=\"https://github.com/Microsoft/ReportingServicesTools/issues\">issues in GitHub</a>.</p>\n\n<h2 id=\"usefullinks\">Useful Links</h2>\n\n<p>If you are looking to make modifications to the <a href=\"https://github.com/Microsoft/ReportingServicesTools\">ReportingServicesTools</a> module or are looking to develop solutions around the SSRS API, these links may be helpful:</p>\n\n<ul>\n<li><a href=\"http://www.tek-tips.com/viewthread.cfm?qid=1780331\">http://www.tek-tips.com/viewthread.cfm?qid=1780331</a></li>\n<li><a href=\"https://sqlblogcasts.com/blogs/sqlandthelike/archive/2013/02/12/deploying-ssrs-artefacts-using-powershell-simply.aspx\">https://sqlblogcasts.com/blogs/sqlandthelike/archive/2013/02/12/deploying-ssrs-artefacts-using-powershell-simply.aspx</a></li>\n<li><a href=\"http://www.techtree.co.uk/windows-server/windows-powershell/powershell-ssrs-function-export-ssrsreports-to-rdl-files/\">http://www.techtree.co.uk/windows-server/windows-powershell/powershell-ssrs-function-export-ssrsreports-to-rdl-files/</a></li>\n<li><a href=\"https://msdn.microsoft.com/en-us/library/reportservice2010.reportingservice2010.aspx\">https://msdn.microsoft.com/en-us/library/reportservice2010.reportingservice2010.aspx</a></li>\n<li><a href=\"http://odetocode.com/Articles/114.aspx\">http://odetocode.com/Articles/114.aspx</a></li>\n<li><a href=\"http://www.sqlservercurry.com/2009/07/programmatically-create-data-driven.html\">http://www.sqlservercurry.com/2009/07/programmatically-create-data-driven.html</a></li>\n<li><a href=\"http://jaliyaudagedara.blogspot.co.uk/2012/10/creating-data-driven-subscription.html\">http://jaliyaudagedara.blogspot.co.uk/2012/10/creating-data-driven-subscription.html</a></li>\n<li><a href=\"https://www.codeproject.com/Questions/1034771/SSRS-Programmatically-Create-Subscriptions\">https://www.codeproject.com/Questions/1034771/SSRS-Programmatically-Create-Subscriptions</a></li>\n<li><a href=\"http://www.sqlservercentral.com/blogs/cl%C3%A1udio-silva/2017/07/27/does-that-copy-subscriptions-too-now-it-does-new-powershell-ssrs-commands/\">http://www.sqlservercentral.com/blogs/cl%C3%A1udio-silva/2017/07/27/does-that-copy-subscriptions-too-now-it-does-new-powershell-ssrs-commands/</a></li>\n<li><a href=\"https://www.mssqltips.com/sqlservertip/4738/powershell-commands-for-sql-server-reporting-services/\">https://www.mssqltips.com/sqlservertip/4738/powershell-commands-for-sql-server-reporting-services/</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/accessing-the-soap-api\">https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/accessing-the-soap-api</a></li>\n<li><a href=\"https://www.codeproject.com/Articles/36009/Programmatically-Playing-With-SSRS-Subscriptions\">https://www.codeproject.com/Articles/36009/Programmatically-Playing-With-SSRS-Subscriptions</a></li>\n</ul>",
                        "image": "/content/images/2017/10/BusinessAutomation.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2017-10-04 16:32:31",
                        "created_by": 1,
                        "updated_at": "2017-10-28 07:41:09",
                        "updated_by": 1,
                        "published_at": "2017-10-28 07:41:09",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 61,
                        "uuid": "406ceea5-b377-4fdf-88d4-d7c4c2fb315b",
                        "title": "How to create a Grafana metrics dashboard via Influx and PowerShell",
                        "slug": "windows-based-grafana-analytics-platform-via-influxdb-and-powershell",
                        "markdown": "This blog post describes how you can use the open source tools Influx and Grafana along with a [PowerShell module I've authored](https://github.com/markwragg/PowerShell-Influx) on Windows to create and populate interactive metric and monitoring dashboards like this one:\n\n![](/content/images/2018/02/Grafana-Example-2.png)\n\n*(-- note that all the graph labels and legends from the above screenshot have been removed to anonymize the data.)*\n\nI recently attended a [DevOps meet up](https://www.meetup.com/AHODevOps/) hosted by Shazam in their London office. The talks were held in a breakout area next to their open plan office space, through which were pillars covered in monitors displaying various Grafana dashboards. I hadn't seen Grafana before and this piqued my interest.\n\nIn case you're not aware, Grafana is an open source metrics dashboard and graph editor. It's power and popularity lies in the fact that it lets you put together beautiful looking dashboards with incredible ease that give you instant insight in to any metrics you wish to track (as well as allowing you to aggregate metrics from multiple sources). \n\nWhether you are in development or operations or somewhere in between, you often want or need the ability to quickly and easily monitor something closely, be that a server, network or infrastructure metric or some component or behaviour of your application. Monitoring and measuring is a core tenet of DevOps.\n\n> *\"If it moves, we track it. Sometimes we’ll draw a graph of something that isn’t moving yet, just in case it decides to make a run for it.\"*\n>\n> -- [Etsy, 2011](https://codeascraft.com/2011/02/15/measure-anything-measure-everything/)\n\n[Grafana](https://grafana.com/) is purely a front-end visualisation tool, so to use it you need one or more back-end time series database systems to operate as a data source. There's lots of options to choose from and the list is growing all the time as developers can add new data source providers as plug ins.\n\nI looked for something that I could install on Windows and ultimately decided to try out [InfluxDB](https://www.influxdata.com/).\n\n# InfluxDB\n\nInflux is an open source time series database product. It is actually four components which make up what they call the TICK stack: \n\n- Telegraf -- Time-Series Data Collector\n- InfluxDB -- Time-Series Data Storage\n- Chronograf -- Time-Series Data Visualization\n- Kapacitor. -- Time-Series Data Processing\n\nTo back-end Grafana you only actually need InfluxDB, but all the components are pretty simple to install on Windows as follows:\n\n**Prerequisites**\n\n1. Deploy/locate a Windows server to act as host. I used a Windows 2012 R2 Server.\n- Download the four Influx components from https://portal.influxdata.com/downloads to somewhere on your host machine, e.g C:\\Influx\\<component>\n-  Download [NSSM](https://nssm.cc/) (tool for installing an application as a Windows service) and place NSSM.exe in C:\\Windows\\System32 (or some other PATH directory if you wish).\n\n**Installing InfluxDB**\n\n1. Open an Administrator PowerShell Window and go to your InfluxDB directory (e.g C:\\Influx\\InfluxDB\\).\n- Run `influxd.exe` to check it starts successfully (note that Influx.exe is the CLI tool). If it does, stop it and then install it as a service as follows:\n```\nnssm install InfluxDB \"C:\\Influx\\InfluxDB\\influxd.exe\"\n```\n\nBy default, InfluxDB uses the following network ports:\n\n- TCP port 8086 is used for client-server communication over InfluxDB’s HTTP API\n- TCP port 8088 is used for the RPC service for backup and restore\n\nYou can check that InfluxDB is working correctly by opening a PowerShell/cmd window, changing to the directory you installed it (e.g `C:\\Influx\\InfluxDB\\` if you followed the above) and running `.\\influx.exe` which is the CLI tool. Here are some useful commands:\n\n- `help` -- list commands\n- `show databases` -- list the current databases\n- `create database <name>` -- create a new database called `<name>` (if there isn't a default DB you want to use, create one now).\n\n**Installing Telegraf** (optional)\n\n> Telegraf is the InfluxData plugin-driven server agent for collecting and reporting metrics.\n\nPer the above, you can use Telegraf as an agent for collecting metrics from a server. Later in this blog post I am going to introduce how you can collect/query your metrics directly with PowerShell but if you want a generic server agent for monitoring Telegraf is worth installing.\n\nTo install Telegraf:\n\n1. Go to your Telegraf download directory and open `telegraf.conf`. Change the `logfile` path to `/telegraf.log`.\n- Open an Administrator PowerShell Window. Run Telegraf.exe, if it starts up successfully then stop it and install it as a service as follows:\n```\nC:\\Influx\\Telegraf\\telegraf.exe --service install --config C:\\Influx\\Telegraf\\telegraf.conf\n```\n\n**Installing Chronograf** (optional)\n\n> Chronograf is InfluxData’s open source web application. Use Chronograf with the other components of the TICK stack to visualize your monitoring data and easily create alerting and automation rules.\n\nAs noted earlier, Chronograf provides the same functionality as Grafana. I haven't explored it in detail yet so can't comment on whether it's worth using. However you can install it alongside Grafana (they use different ports by default) and it's simple to install as follows:\n\n1. Open an Administrator PowerShell Window and go to your Chronograf directory.\n- Run `chronograf.exe` to check it starts successfully. If it does, install it as a service as follows:\n```\nnssm install Chronograf \"C:\\Influx\\Chronograf\\chronograf.exe\"\n```\n\nIf Chronograf is working, you should be able to access it via http://localhost:8888/ (although again note that it isn't core to this blog post, you just might find it interesting to explore).\n\n**Installing Kapacitor** (optional)\n\n> Kapacitor is the InfluxData processing framework for creating alerts, running ETL jobs, and detecting anomalies in your data. Kapacitor is responsible for creating and sending alerts in Chronograf.\n\nIf you want to install Kapacitor, you can do so as follows:\n\n1. Open an Administrator PowerShell Window and go to your Kapacitor directory.\n- Run `kapacitord.exe` to check it starts successfully. If it does, install it as a service as follows:\n```\nnssm install Kapacitor \"C:\\Influx\\Kapacitor\\kapacitord.exe\"\n```\n\n**Start the services**\n\nYou can now start all the services you created above. Beware that if you run the services as the local system identity, Influx will store its data by default in: \n```\nC:\\Windows\\System32\\config\\systemprofile.influxdb\\data\n```\nIf you run it as a named user, it will store its data in:\n```\nc:\\Users\\<username>\\.influxdb\\data\\\n```\nYou can specify the data directory by changing `.\\influxdb.conf` in the Influx DB directory, but note that you will then also need to use `nssm` to modify the service so that it starts `influxd.exe` with `-config infludb.conf` to have it use your config file.\n\n# Grafana\n\nYou can download Grafana from [here](https://grafana.com/grafana/download).\n\n**Installing Grafana**\n\nGrafana provides documentation on [installing it on Windows](http://docs.grafana.org/installation/windows/) which I recommend you check in case the below is out of date. That being said, it should be as simple as:\n\n1. Extract the downloaded Grafana files somewhere on your system. E.g C:\\Grafana.\n- Go in to the `\\Conf` directory and copy `sample.ini` to `custom.ini` (you should edit `custom.ini` and never `defaults.ini`).\n- The default Grafana port is 3000 but this requires extra permissions on Windows, so uncomment the `http_port` config section and change it to another port (e.g 80 or 8080). \n- Open an Administrator PowerShell window. Check grafana-server.exe runs OK manually, then stop it and use NSSM to install it as a service as follows:\n```\nnssm install Grafana \"C:\\Grafana\\bin\\grafana-server.exe\"\n```\nThen start the service.\n\nIf Grafana is working, you should now be able to access it via http://localhost:80/ (or whatever port you chose). For more guidance on getting started with Grafana [see here](http://docs.grafana.org/guides/getting_started/).\n\n# Writing metrics to Influx\n\nThere is detailed information on how to [write to Influx in their own user guide](https://docs.influxdata.com/influxdb/v0.9/guides/writing_data/), but below are some examples of how you might transmit metrics to a database in Influx named 'yourdb'.\n\nWhen writing metrics to Influx you provide the name of a metric, some optional key/value pair tags and then a value for the metric. You can also optionally include a timestamp (omitted it just uses the time of write). You can provide multiple metrics in a single submission, you just need each to be on a separate line.\n\n**Writing a metric via CURL:**\n```\ncurl -i -XPOST 'http://localhost:8086/write?db=yourdb' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'\n```\nIn the above example `cpu_load_short` is the name of the metric, it has two tags `host` and `region` with their values, and the metrics own value is 0.64. The `1434055562000000000` part is the (optional) timestamp.\n\n> Note that you don't have to create any existing tables or fields in your Influx database in advance when writing a metric. When you submit a new metric for the first time Influx creates it on the fly.\n\n**Writing a metric via Invoke-WebRequest:**\n\nHere is the above example again, submitted via PowerShell's `Invoke-WebRequest` cmdlet:\n```\nInvoke-WebRequest 'http://localhost:8086/write?db=yourdb' -Method POST -Body 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'\n```\n\n# PowerShell-Influx\n\nIn order to simplify and standardise writing to Influx with PowerShell, i've written a module named [PowerShell-Influx](https://github.com/markwragg/PowerShell-Influx) which you can find on [Github](https://github.com/markwragg/PowerShell-Influx) and in the [PowerShell Gallery](https://www.powershellgallery.com/packages/Influx/). If you have PS5/PackageManagement you can install the module as follows:\n```\nInstall-Module Influx -Scope CurrentUser\n```\n\n**Write-Influx**\n\nThe module provides a `Write-Influx` cmdlet, which essentially wraps the `Invoke-WebRequest` example shown above, but takes the metrics and tags you want to submit as hashtables. \n\nHere's an example of how you could use `Write-Influx` to capture the Memory and CPU stats of a server every 5 seconds:\n```\nWhile ($true){\n    $Metrics = @{\n        Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n        CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    }\n    Write-Influx -Measure Server -Tags @{Server=$env:COMPUTERNAME} -Metrics $Metrics -Database yourdb -Server http://localhost:8086 -Verbose\n    Start-Sleep -Seconds 5\n}\n```\nNote that `Write-Influx` (as with the `Invoke-WebRequest` example) is submitting the metrics via a TCP request. TCP is a connection-oriented protocol. If the InfluxDB server was down, your script would wait until the connection timed out before continuing, or could even throw an exception causing the script to terminate. \n\nWhile this might be fine if you're capturing metrics via some dedicated monitoring script, it might be an issue if you were embedding metric capturing code inside an application that you need to be stable and performant. Fortunately, Influx can be configured to run a UDP listener. UDP is connectionless, so its an ideal candidate for this scenario, where the metrics can be sent to a UDP socket and the code then immediately continue, with no pause to confirm receipt.\n\n**Write-InfluxUDP**\n\nTo write to the Influx UDP listener with the PowerShell Influx module, you can use `Write-InfluxUDP`. Here's an example:\n```\nWhile ($true){\n    $Metrics = @{\n        Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n        CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    }\n    Write-InfluxUDP -IP 1.2.3.4 -Port 8089 -Measure Server -Tags @{Server=$env:COMPUTERNAME} -Metrics $Metrics -Verbose\n    Start-Sleep -Seconds 5\n}\n```\nNote that you have to provide an IP address for the endpoint. DNS names are not supported. Otherwise the cmdlet works in the same way, taking hashtables for the metrics and tags you want to submit.\n\n**Write-StatsD**\n\nAs a third option, the PowerShell Influx module also provides `Write-StatsD` for writing metrics in to Influx via a statsd listener.\n\n> *\"Statsd is a network daemon that runs on the Node.js platform and listens for statistics, like counters and timers, sent over UDP or TCP and sends aggregates to one or more pluggable backend services (e.g., Graphite).\"*\n>\n> -- https://github.com/etsy/statsd\n\nInflux can be configured with to run a built-in StatsD listener, so there's no need to run/install it directly. You do this via the Telegraf component, and need to go to:\n```\nC:\\Influx\\Telegraf\\Telegraf.conf\n```\nAnd enable/configure the listener per the settings in the `# Statsd Server` section. By default StatsD listens on port 8125 and as a UDP endpoint.\n\nStatsD has its own line protocol for submitting metrics which the cmdlet accepts directly as a string. Here's an example:\n```\nWhile ($true){\n    $Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n    $CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    Send-Statsd -IP 1.2.3.4 -Port 8125 -Data \"Server.Memory,Server=$env:COMPUTERNAME`:$Memory|g\" -Verbose\n    Send-Statsd -IP 1.2.3.4 -Port 8125 -Data \"Server.CPU,Server=$env:COMPUTERNAME`:$CPU|g\" -Verbose\n    Start-Sleep -Seconds 5\n}\n```\n> Note that you can't specify a destination database when using the StatsD Influx listener, the metrics are just written to the default database (although I think you can set config in` telegraf.conf` to route different metrics to different databases depending on the metric name).\n\n**Get-*Metric cmdlets**\n\nTo aid data collection, the PowerShell Influx module also contains a number of cmdlets for gathering some standard metrics from various datasources.\n\n![](/content/images/2018/02/Get-VMMetric-Example.png)\n\nAt time of writing the module has cmdlets for gathering metrics from 3Par, VMWare, Isilon and TFS. These return a custom `metric` PowerShell object which have the specified measure name, gathered metrics and tags (as hashtables) and (if specified) timestamp properties. You can then pipe this object in to one of the above `Write-` cmdlets to transmit to Influx (giving you flexibility to choose which method of transmission you want to use).\n\nHere are some examples:\n\n```\nGet-DatastoreMetric | Write-Influx -Database VMWareStats\n```\nThis returns metrics for any VMWare Datastores such as capacity and free/used space (you need to have the [VMWare PowerCLI cmdlets](https://my.vmware.com/web/vmware/details?downloadGroup=PCLI650R1&productId=614) installed and have connected to a server via Connect-VIServer first). It then writes these to Influx via the standard REST API method in to a database named VMWareStats.\n\n```\nGet-3ParSystemMetric -SANIPAddress '3.4.5.6' -SANUserName someuser -SANPasswordFile 'C:\\some3parpasswordfile.txt' | Write-InfluxUDP -Database 3ParData\n```\nThis retrieves 3Par System metrics such as raw free and usable free space (using the [HPES3PARSToolkit module](https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=3PARPSToolkit) and having stored your password via `Set-3parPoshSshConnectionPasswordFile` first). It then writes them to Influx via the Influx UDP listener to a database named 3ParData.\n\n```\nGet-TFSBuildMetric -TFSRootURL 'https://mytfsurl.local/tfs' -TFSCollection somecollection -TFSProject someproject | Write-StatsD -Type g\n```\nThis returns TFS Build data such as build name, result, duration (via [this TFS module](https://github.com/majkinetor/TFS)) and sends these to Influx via StatsD (note that `Write-StatsD` converts the piped metric object to StatsD formatted strings automatically). \n\nHere's how I'm presenting the TFS build data in Grafana:\n\n![](/content/images/2018/02/Grafana-TFS-Build-Dashboard.png)\n\nThere are also `Send-` cmdlets for each of the `Get-` cmdlets which just perform the equivalent `Get-` cmdlet and then submit the metrics immediately to Influx via the standard `Write-Influx` method.\n\n# Visualising your metrics with Grafana\n\nHaving (hopefully) got some metrics in to Influx you probably now want to see them. As mentioned earlier, I suggest looking at the [Getting started with Grafana](http://docs.grafana.org/guides/getting_started/) guidance directly, but here's a brief intro:\n\n1. Open Grafana http://localhost:80/ (or whatever port/URL you're using)\n- Go to Data Sources > Add data source\n- Enter a name for your datasource.\n- Change \"Type\" to InfluxDB. Select the default HTTP settings (e.g URL: http://localhost:8086) if your InfluxDB is running on the same server as Grafana. Leave the access as \"proxy\" (selecting \"direct\" will make Grafana attempt to read the datasource from wherever you currently are accessing it in a browser).\n- Enter the name of the database you want to query.\n- Click \"Add\".\n\nNow go to \"Dashboards\":\n\n1. Click Dashboards > New\n- Add a Graph panel. Click its title and click \"edit\"\n- Change \"Data Source\" to the data source you added.\n- Select the measurement you submitted (the name you provided for your measure).\n- Select the field/s you want (or enter * for all).\n- By default Grafana doesn't join together points (so your graph might look blank). Go to the display tab and change \"Null value\" to \"connected\" to join together the points. Ensure \"Draw mode\" has \"Lines\" ticked also.\n\nYou probably want to customise other things. For example go to \"Axes\" and change the unit to something more appropriate. Another good tip is that you can click on the colour of a measure in the legend of the graph and move it over on to the Right Y axis if you're displaying two metrics that have different units.\n\nHere's the server CPU/Memory stats gathered in the examples above visualised:\n\n![](/content/images/2018/02/Grafana-Server01-Example.png)\n\n# Summary\n\nI hope this has served as a useful introduction to Influx, Grafana and my PowerShell-Influx module. I definitely suggest reading the official pages for [Grafana](http://docs.grafana.org/) and [Influx](https://docs.influxdata.com/) to learn more about how to configure and customise them (for example you might want to enable outbound email for Grafana alerting and you can also Active Directory integrate the login).\n\nIf you have any ideas for improvements to my [PowerShell-Influx](https://github.com/markwragg/PowerShell-Influx) module please feel free to [fork the repo](https://github.com/markwragg/PowerShell-Influx#fork-destination-box) and submit a PR.",
                        "html": "<p>This blog post describes how you can use the open source tools Influx and Grafana along with a <a href=\"https://github.com/markwragg/PowerShell-Influx\">PowerShell module I've authored</a> on Windows to create and populate interactive metric and monitoring dashboards like this one:</p>\n\n<p><img src=\"/content/images/2018/02/Grafana-Example-2.png\" alt=\"\" /></p>\n\n<p><em>(-- note that all the graph labels and legends from the above screenshot have been removed to anonymize the data.)</em></p>\n\n<p>I recently attended a <a href=\"https://www.meetup.com/AHODevOps/\">DevOps meet up</a> hosted by Shazam in their London office. The talks were held in a breakout area next to their open plan office space, through which were pillars covered in monitors displaying various Grafana dashboards. I hadn't seen Grafana before and this piqued my interest.</p>\n\n<p>In case you're not aware, Grafana is an open source metrics dashboard and graph editor. It's power and popularity lies in the fact that it lets you put together beautiful looking dashboards with incredible ease that give you instant insight in to any metrics you wish to track (as well as allowing you to aggregate metrics from multiple sources). </p>\n\n<p>Whether you are in development or operations or somewhere in between, you often want or need the ability to quickly and easily monitor something closely, be that a server, network or infrastructure metric or some component or behaviour of your application. Monitoring and measuring is a core tenet of DevOps.</p>\n\n<blockquote>\n  <p><em>\"If it moves, we track it. Sometimes we’ll draw a graph of something that isn’t moving yet, just in case it decides to make a run for it.\"</em></p>\n  \n  <p>-- <a href=\"https://codeascraft.com/2011/02/15/measure-anything-measure-everything/\">Etsy, 2011</a></p>\n</blockquote>\n\n<p><a href=\"https://grafana.com/\">Grafana</a> is purely a front-end visualisation tool, so to use it you need one or more back-end time series database systems to operate as a data source. There's lots of options to choose from and the list is growing all the time as developers can add new data source providers as plug ins.</p>\n\n<p>I looked for something that I could install on Windows and ultimately decided to try out <a href=\"https://www.influxdata.com/\">InfluxDB</a>.</p>\n\n<h1 id=\"influxdb\">InfluxDB</h1>\n\n<p>Influx is an open source time series database product. It is actually four components which make up what they call the TICK stack: </p>\n\n<ul>\n<li>Telegraf -- Time-Series Data Collector</li>\n<li>InfluxDB -- Time-Series Data Storage</li>\n<li>Chronograf -- Time-Series Data Visualization</li>\n<li>Kapacitor. -- Time-Series Data Processing</li>\n</ul>\n\n<p>To back-end Grafana you only actually need InfluxDB, but all the components are pretty simple to install on Windows as follows:</p>\n\n<p><strong>Prerequisites</strong></p>\n\n<ol>\n<li>Deploy/locate a Windows server to act as host. I used a Windows 2012 R2 Server.  </li>\n<li>Download the four Influx components from <a href=\"https://portal.influxdata.com/downloads\">https://portal.influxdata.com/downloads</a> to somewhere on your host machine, e.g C:\\Influx\\<component></li>\n<li>Download <a href=\"https://nssm.cc/\">NSSM</a> (tool for installing an application as a Windows service) and place NSSM.exe in C:\\Windows\\System32 (or some other PATH directory if you wish).</li>\n</ol>\n\n<p><strong>Installing InfluxDB</strong></p>\n\n<ol>\n<li>Open an Administrator PowerShell Window and go to your InfluxDB directory (e.g C:\\Influx\\InfluxDB).  </li>\n<li>Run <code>influxd.exe</code> to check it starts successfully (note that Influx.exe is the CLI tool). If it does, stop it and then install it as a service as follows:</li>\n</ol>\n\n<pre><code>nssm install InfluxDB \"C:\\Influx\\InfluxDB\\influxd.exe\"  \n</code></pre>\n\n<p>By default, InfluxDB uses the following network ports:</p>\n\n<ul>\n<li>TCP port 8086 is used for client-server communication over InfluxDB’s HTTP API</li>\n<li>TCP port 8088 is used for the RPC service for backup and restore</li>\n</ul>\n\n<p>You can check that InfluxDB is working correctly by opening a PowerShell/cmd window, changing to the directory you installed it (e.g <code>C:\\Influx\\InfluxDB\\</code> if you followed the above) and running <code>.\\influx.exe</code> which is the CLI tool. Here are some useful commands:</p>\n\n<ul>\n<li><code>help</code> -- list commands</li>\n<li><code>show databases</code> -- list the current databases</li>\n<li><code>create database &lt;name&gt;</code> -- create a new database called <code>&lt;name&gt;</code> (if there isn't a default DB you want to use, create one now).</li>\n</ul>\n\n<p><strong>Installing Telegraf</strong> (optional)</p>\n\n<blockquote>\n  <p>Telegraf is the InfluxData plugin-driven server agent for collecting and reporting metrics.</p>\n</blockquote>\n\n<p>Per the above, you can use Telegraf as an agent for collecting metrics from a server. Later in this blog post I am going to introduce how you can collect/query your metrics directly with PowerShell but if you want a generic server agent for monitoring Telegraf is worth installing.</p>\n\n<p>To install Telegraf:</p>\n\n<ol>\n<li>Go to your Telegraf download directory and open <code>telegraf.conf</code>. Change the <code>logfile</code> path to <code>/telegraf.log</code>.  </li>\n<li>Open an Administrator PowerShell Window. Run Telegraf.exe, if it starts up successfully then stop it and install it as a service as follows:</li>\n</ol>\n\n<pre><code>C:\\Influx\\Telegraf\\telegraf.exe --service install --config C:\\Influx\\Telegraf\\telegraf.conf  \n</code></pre>\n\n<p><strong>Installing Chronograf</strong> (optional)</p>\n\n<blockquote>\n  <p>Chronograf is InfluxData’s open source web application. Use Chronograf with the other components of the TICK stack to visualize your monitoring data and easily create alerting and automation rules.</p>\n</blockquote>\n\n<p>As noted earlier, Chronograf provides the same functionality as Grafana. I haven't explored it in detail yet so can't comment on whether it's worth using. However you can install it alongside Grafana (they use different ports by default) and it's simple to install as follows:</p>\n\n<ol>\n<li>Open an Administrator PowerShell Window and go to your Chronograf directory.  </li>\n<li>Run <code>chronograf.exe</code> to check it starts successfully. If it does, install it as a service as follows:</li>\n</ol>\n\n<pre><code>nssm install Chronograf \"C:\\Influx\\Chronograf\\chronograf.exe\"  \n</code></pre>\n\n<p>If Chronograf is working, you should be able to access it via <a href=\"http://localhost:8888/\">http://localhost:8888/</a> (although again note that it isn't core to this blog post, you just might find it interesting to explore).</p>\n\n<p><strong>Installing Kapacitor</strong> (optional)</p>\n\n<blockquote>\n  <p>Kapacitor is the InfluxData processing framework for creating alerts, running ETL jobs, and detecting anomalies in your data. Kapacitor is responsible for creating and sending alerts in Chronograf.</p>\n</blockquote>\n\n<p>If you want to install Kapacitor, you can do so as follows:</p>\n\n<ol>\n<li>Open an Administrator PowerShell Window and go to your Kapacitor directory.  </li>\n<li>Run <code>kapacitord.exe</code> to check it starts successfully. If it does, install it as a service as follows:</li>\n</ol>\n\n<pre><code>nssm install Kapacitor \"C:\\Influx\\Kapacitor\\kapacitord.exe\"  \n</code></pre>\n\n<p><strong>Start the services</strong></p>\n\n<p>You can now start all the services you created above. Beware that if you run the services as the local system identity, Influx will store its data by default in:  </p>\n\n<pre><code>C:\\Windows\\System32\\config\\systemprofile.influxdb\\data  \n</code></pre>\n\n<p>If you run it as a named user, it will store its data in:  </p>\n\n<pre><code>c:\\Users\\&lt;username&gt;\\.influxdb\\data\\  \n</code></pre>\n\n<p>You can specify the data directory by changing <code>.\\influxdb.conf</code> in the Influx DB directory, but note that you will then also need to use <code>nssm</code> to modify the service so that it starts <code>influxd.exe</code> with <code>-config infludb.conf</code> to have it use your config file.</p>\n\n<h1 id=\"grafana\">Grafana</h1>\n\n<p>You can download Grafana from <a href=\"https://grafana.com/grafana/download\">here</a>.</p>\n\n<p><strong>Installing Grafana</strong></p>\n\n<p>Grafana provides documentation on <a href=\"http://docs.grafana.org/installation/windows/\">installing it on Windows</a> which I recommend you check in case the below is out of date. That being said, it should be as simple as:</p>\n\n<ol>\n<li>Extract the downloaded Grafana files somewhere on your system. E.g C:\\Grafana.  </li>\n<li>Go in to the <code>\\Conf</code> directory and copy <code>sample.ini</code> to <code>custom.ini</code> (you should edit <code>custom.ini</code> and never <code>defaults.ini</code>).</li>\n<li>The default Grafana port is 3000 but this requires extra permissions on Windows, so uncomment the <code>http_port</code> config section and change it to another port (e.g 80 or 8080). </li>\n<li>Open an Administrator PowerShell window. Check grafana-server.exe runs OK manually, then stop it and use NSSM to install it as a service as follows:</li>\n</ol>\n\n<pre><code>nssm install Grafana \"C:\\Grafana\\bin\\grafana-server.exe\"  \n</code></pre>\n\n<p>Then start the service.</p>\n\n<p>If Grafana is working, you should now be able to access it via <a href=\"http://localhost:80/\">http://localhost:80/</a> (or whatever port you chose). For more guidance on getting started with Grafana <a href=\"http://docs.grafana.org/guides/getting_started/\">see here</a>.</p>\n\n<h1 id=\"writingmetricstoinflux\">Writing metrics to Influx</h1>\n\n<p>There is detailed information on how to <a href=\"https://docs.influxdata.com/influxdb/v0.9/guides/writing_data/\">write to Influx in their own user guide</a>, but below are some examples of how you might transmit metrics to a database in Influx named 'yourdb'.</p>\n\n<p>When writing metrics to Influx you provide the name of a metric, some optional key/value pair tags and then a value for the metric. You can also optionally include a timestamp (omitted it just uses the time of write). You can provide multiple metrics in a single submission, you just need each to be on a separate line.</p>\n\n<p><strong>Writing a metric via CURL:</strong></p>\n\n<pre><code>curl -i -XPOST 'http://localhost:8086/write?db=yourdb' --data-binary 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'  \n</code></pre>\n\n<p>In the above example <code>cpu_load_short</code> is the name of the metric, it has two tags <code>host</code> and <code>region</code> with their values, and the metrics own value is 0.64. The <code>1434055562000000000</code> part is the (optional) timestamp.</p>\n\n<blockquote>\n  <p>Note that you don't have to create any existing tables or fields in your Influx database in advance when writing a metric. When you submit a new metric for the first time Influx creates it on the fly.</p>\n</blockquote>\n\n<p><strong>Writing a metric via Invoke-WebRequest:</strong></p>\n\n<p>Here is the above example again, submitted via PowerShell's <code>Invoke-WebRequest</code> cmdlet:  </p>\n\n<pre><code>Invoke-WebRequest 'http://localhost:8086/write?db=yourdb' -Method POST -Body 'cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000'  \n</code></pre>\n\n<h1 id=\"powershellinflux\">PowerShell-Influx</h1>\n\n<p>In order to simplify and standardise writing to Influx with PowerShell, i've written a module named <a href=\"https://github.com/markwragg/PowerShell-Influx\">PowerShell-Influx</a> which you can find on <a href=\"https://github.com/markwragg/PowerShell-Influx\">Github</a> and in the <a href=\"https://www.powershellgallery.com/packages/Influx/\">PowerShell Gallery</a>. If you have PS5/PackageManagement you can install the module as follows:  </p>\n\n<pre><code>Install-Module Influx -Scope CurrentUser  \n</code></pre>\n\n<p><strong>Write-Influx</strong></p>\n\n<p>The module provides a <code>Write-Influx</code> cmdlet, which essentially wraps the <code>Invoke-WebRequest</code> example shown above, but takes the metrics and tags you want to submit as hashtables. </p>\n\n<p>Here's an example of how you could use <code>Write-Influx</code> to capture the Memory and CPU stats of a server every 5 seconds:  </p>\n\n<pre><code>While ($true){  \n    $Metrics = @{\n        Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n        CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    }\n    Write-Influx -Measure Server -Tags @{Server=$env:COMPUTERNAME} -Metrics $Metrics -Database yourdb -Server http://localhost:8086 -Verbose\n    Start-Sleep -Seconds 5\n}\n</code></pre>\n\n<p>Note that <code>Write-Influx</code> (as with the <code>Invoke-WebRequest</code> example) is submitting the metrics via a TCP request. TCP is a connection-oriented protocol. If the InfluxDB server was down, your script would wait until the connection timed out before continuing, or could even throw an exception causing the script to terminate. </p>\n\n<p>While this might be fine if you're capturing metrics via some dedicated monitoring script, it might be an issue if you were embedding metric capturing code inside an application that you need to be stable and performant. Fortunately, Influx can be configured to run a UDP listener. UDP is connectionless, so its an ideal candidate for this scenario, where the metrics can be sent to a UDP socket and the code then immediately continue, with no pause to confirm receipt.</p>\n\n<p><strong>Write-InfluxUDP</strong></p>\n\n<p>To write to the Influx UDP listener with the PowerShell Influx module, you can use <code>Write-InfluxUDP</code>. Here's an example:  </p>\n\n<pre><code>While ($true){  \n    $Metrics = @{\n        Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n        CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    }\n    Write-InfluxUDP -IP 1.2.3.4 -Port 8089 -Measure Server -Tags @{Server=$env:COMPUTERNAME} -Metrics $Metrics -Verbose\n    Start-Sleep -Seconds 5\n}\n</code></pre>\n\n<p>Note that you have to provide an IP address for the endpoint. DNS names are not supported. Otherwise the cmdlet works in the same way, taking hashtables for the metrics and tags you want to submit.</p>\n\n<p><strong>Write-StatsD</strong></p>\n\n<p>As a third option, the PowerShell Influx module also provides <code>Write-StatsD</code> for writing metrics in to Influx via a statsd listener.</p>\n\n<blockquote>\n  <p><em>\"Statsd is a network daemon that runs on the Node.js platform and listens for statistics, like counters and timers, sent over UDP or TCP and sends aggregates to one or more pluggable backend services (e.g., Graphite).\"</em></p>\n  \n  <p>-- <a href=\"https://github.com/etsy/statsd\">https://github.com/etsy/statsd</a></p>\n</blockquote>\n\n<p>Influx can be configured with to run a built-in StatsD listener, so there's no need to run/install it directly. You do this via the Telegraf component, and need to go to:  </p>\n\n<pre><code>C:\\Influx\\Telegraf\\Telegraf.conf  \n</code></pre>\n\n<p>And enable/configure the listener per the settings in the <code># Statsd Server</code> section. By default StatsD listens on port 8125 and as a UDP endpoint.</p>\n\n<p>StatsD has its own line protocol for submitting metrics which the cmdlet accepts directly as a string. Here's an example:  </p>\n\n<pre><code>While ($true){  \n    $Memory = (Get-Counter '\\Memory\\Available MBytes').CounterSamples.CookedValue\n    $CPU = (Get-Counter '\\Processor(_Total)\\% Processor Time').CounterSamples.CookedValue\n    Send-Statsd -IP 1.2.3.4 -Port 8125 -Data \"Server.Memory,Server=$env:COMPUTERNAME`:$Memory|g\" -Verbose\n    Send-Statsd -IP 1.2.3.4 -Port 8125 -Data \"Server.CPU,Server=$env:COMPUTERNAME`:$CPU|g\" -Verbose\n    Start-Sleep -Seconds 5\n}\n</code></pre>\n\n<blockquote>\n  <p>Note that you can't specify a destination database when using the StatsD Influx listener, the metrics are just written to the default database (although I think you can set config in<code>telegraf.conf</code> to route different metrics to different databases depending on the metric name).</p>\n</blockquote>\n\n<p><strong>Get-*Metric cmdlets</strong></p>\n\n<p>To aid data collection, the PowerShell Influx module also contains a number of cmdlets for gathering some standard metrics from various datasources.</p>\n\n<p><img src=\"/content/images/2018/02/Get-VMMetric-Example.png\" alt=\"\" /></p>\n\n<p>At time of writing the module has cmdlets for gathering metrics from 3Par, VMWare, Isilon and TFS. These return a custom <code>metric</code> PowerShell object which have the specified measure name, gathered metrics and tags (as hashtables) and (if specified) timestamp properties. You can then pipe this object in to one of the above <code>Write-</code> cmdlets to transmit to Influx (giving you flexibility to choose which method of transmission you want to use).</p>\n\n<p>Here are some examples:</p>\n\n<pre><code>Get-DatastoreMetric | Write-Influx -Database VMWareStats  \n</code></pre>\n\n<p>This returns metrics for any VMWare Datastores such as capacity and free/used space (you need to have the <a href=\"https://my.vmware.com/web/vmware/details?downloadGroup=PCLI650R1&amp;productId=614\">VMWare PowerCLI cmdlets</a> installed and have connected to a server via Connect-VIServer first). It then writes these to Influx via the standard REST API method in to a database named VMWareStats.</p>\n\n<pre><code>Get-3ParSystemMetric -SANIPAddress '3.4.5.6' -SANUserName someuser -SANPasswordFile 'C:\\some3parpasswordfile.txt' | Write-InfluxUDP -Database 3ParData  \n</code></pre>\n\n<p>This retrieves 3Par System metrics such as raw free and usable free space (using the <a href=\"https://h20392.www2.hpe.com/portal/swdepot/displayProductInfo.do?productNumber=3PARPSToolkit\">HPES3PARSToolkit module</a> and having stored your password via <code>Set-3parPoshSshConnectionPasswordFile</code> first). It then writes them to Influx via the Influx UDP listener to a database named 3ParData.</p>\n\n<pre><code>Get-TFSBuildMetric -TFSRootURL 'https://mytfsurl.local/tfs' -TFSCollection somecollection -TFSProject someproject | Write-StatsD -Type g  \n</code></pre>\n\n<p>This returns TFS Build data such as build name, result, duration (via <a href=\"https://github.com/majkinetor/TFS\">this TFS module</a>) and sends these to Influx via StatsD (note that <code>Write-StatsD</code> converts the piped metric object to StatsD formatted strings automatically). </p>\n\n<p>Here's how I'm presenting the TFS build data in Grafana:</p>\n\n<p><img src=\"/content/images/2018/02/Grafana-TFS-Build-Dashboard.png\" alt=\"\" /></p>\n\n<p>There are also <code>Send-</code> cmdlets for each of the <code>Get-</code> cmdlets which just perform the equivalent <code>Get-</code> cmdlet and then submit the metrics immediately to Influx via the standard <code>Write-Influx</code> method.</p>\n\n<h1 id=\"visualisingyourmetricswithgrafana\">Visualising your metrics with Grafana</h1>\n\n<p>Having (hopefully) got some metrics in to Influx you probably now want to see them. As mentioned earlier, I suggest looking at the <a href=\"http://docs.grafana.org/guides/getting_started/\">Getting started with Grafana</a> guidance directly, but here's a brief intro:</p>\n\n<ol>\n<li>Open Grafana <a href=\"http://localhost:80/\">http://localhost:80/</a> (or whatever port/URL you're using)  </li>\n<li>Go to Data Sources > Add data source</li>\n<li>Enter a name for your datasource.</li>\n<li>Change \"Type\" to InfluxDB. Select the default HTTP settings (e.g URL: <a href=\"http://localhost:8086\">http://localhost:8086</a>) if your InfluxDB is running on the same server as Grafana. Leave the access as \"proxy\" (selecting \"direct\" will make Grafana attempt to read the datasource from wherever you currently are accessing it in a browser).</li>\n<li>Enter the name of the database you want to query.</li>\n<li>Click \"Add\".</li>\n</ol>\n\n<p>Now go to \"Dashboards\":</p>\n\n<ol>\n<li>Click Dashboards > New  </li>\n<li>Add a Graph panel. Click its title and click \"edit\"</li>\n<li>Change \"Data Source\" to the data source you added.</li>\n<li>Select the measurement you submitted (the name you provided for your measure).</li>\n<li>Select the field/s you want (or enter * for all).</li>\n<li>By default Grafana doesn't join together points (so your graph might look blank). Go to the display tab and change \"Null value\" to \"connected\" to join together the points. Ensure \"Draw mode\" has \"Lines\" ticked also.</li>\n</ol>\n\n<p>You probably want to customise other things. For example go to \"Axes\" and change the unit to something more appropriate. Another good tip is that you can click on the colour of a measure in the legend of the graph and move it over on to the Right Y axis if you're displaying two metrics that have different units.</p>\n\n<p>Here's the server CPU/Memory stats gathered in the examples above visualised:</p>\n\n<p><img src=\"/content/images/2018/02/Grafana-Server01-Example.png\" alt=\"\" /></p>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>I hope this has served as a useful introduction to Influx, Grafana and my PowerShell-Influx module. I definitely suggest reading the official pages for <a href=\"http://docs.grafana.org/\">Grafana</a> and <a href=\"https://docs.influxdata.com/\">Influx</a> to learn more about how to configure and customise them (for example you might want to enable outbound email for Grafana alerting and you can also Active Directory integrate the login).</p>\n\n<p>If you have any ideas for improvements to my <a href=\"https://github.com/markwragg/PowerShell-Influx\">PowerShell-Influx</a> module please feel free to <a href=\"https://github.com/markwragg/PowerShell-Influx#fork-destination-box\">fork the repo</a> and submit a PR.</p>",
                        "image": "/content/images/2018/02/Control-Center-L.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "",
                        "meta_description": "This blog post describes how you can use Influx, Grafana and my PowerShell-Influx module to create and populate interactive metric and monitoring dashboards.",
                        "author_id": 1,
                        "created_at": "2017-12-01 22:15:16",
                        "created_by": 1,
                        "updated_at": "2018-02-21 14:02:43",
                        "updated_by": 1,
                        "published_at": "2018-02-21 14:01:56",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 62,
                        "uuid": "ea4fc134-b2b1-48cb-b928-8630a48cf31f",
                        "title": "Intermittent Isilon write failures due to SMB3 Multichannel setting",
                        "slug": "intermittent-isilon-file-operation-interruptions",
                        "markdown": "I recently resolved an issue with our Isilon storage cluster that was causing file writes to be interrupted and fail. While diagnosing the issue I discovered that (intriguingly) the disruption was occurring at (almost) exact 10 minute intervals. Any in progress write operation occurring at that interval would fail.\n\n*TL;DR: In case you want to skip straight to the resolution, the cause of my issue turned out to be the `support-multichannel` setting on the Isilon, which when enabled caused the issue to occur.*\n\n*I resolved the problem by logging on to the Isilon via SSH and entering the following command to verify the setting was enabled:*\n\n```\nisi smb settings global view\n```\n\n*And then entered this command to set `support-multichannel` to be off:*\n\n```\nisi smb settings global modify --support-multichannel=no\n```\n*Immediately after which the disruptions no longer occurred.*\n\n---\nI wanted to Blog this issue for two reasons:\n\n1. When googling, I found nothing like this on the internet, at least related specifically to an Isilon. It took me several days of solid work to resolve, so I hope to save someone else at least some of that time.\n2. In a job interview recently I was asked to describe a complex problem I solved and how I went about solving it. I completely blanked on the question (which is annoying because as a former manager I used to ask this classic question all the time so really should have expected it). It occurred to me that what I describe here would (perhaps) now be a good example to use in the future. However I found even just a few weeks later I was already struggling to recall it. This blog post will hopefully rectify that.\n\n### Symptoms\n\nThe issue was reported to me by our database team, who were finding that their SQL database backups were sometimes failing at random on a Windows 2012 R2 SQL server. Those backups were being written to a 5 node Isilon cluster. The issue was apparently particularly likely to occur with large (50GB+) databases, but could also occur for a database of any size.\n\n### Diagnosis\n\nThe first step I took to diagnose the issue was to perform a few backups of one of the large databases to the local server storage to see if they failed. They did not, which seemed to rule out anything being wrong with the SQL side of things.\n\nThe SQL server was a VMWare VM, so I performed a vMotion of the server to another host. The issue still occurred, which seemed to rule out anything with the underlying host hardware.\n\nNext I found some suggestions online that the network card setting \"Allow the computer to turn off this device to save power\" could cause intermittent interruptions. This setting was enabled on our NIC so I disabled it. This made no difference to the issue.\n\nFollowing this I wanted to see how frequently the failures/interruptions were occurring, so I used `fsutil file createnew 100mb.txt 104857600` to create a 100mb test file and then wrote a very dirty PowerShell script to copy this file repeatedly to the Isilon and log any errors that occurred along with a timestamp:\n\n```\n$file = '100mb.txt'\n \nWhile ($True) {\n    Try{\n        Copy-Item $file \"\\\\myisilonserver\\somefolder\\\"\n        Remove-Item \"\\\\myisilonserver\\somefolder\\$file\"\n    }Catch{\n        Get-Date -f s\n        Write-Error $_\n    }\n}\n```\nRunning this revealed the very intriguing fact that the interruptions were occurring at almost 10 minute intervals. I say almost as it was 10 minutes and perhaps a fraction of a second, so it would sort of slowly drift. Here's some example timestamps I logged:\n\n```\n2017-12-15T08:27:42\n2017-12-15T08:37:42\n2017-12-15T08:47:43\n```\n\nAfter discovering this I ran a constant ping to www.google.com to see if that was interrupted at the same time as the copies were. It was not, suggesting the NIC was not the cause. I was able to get permission to reboot the Isilon, but this made no difference to the issue. \n\nI also rebooted the SQL server VM, and this didn't resolve the issue, but I discovered that it changed the time it was occurring. It was still at 10 minute intervals, but it seemed to correlate as being 10 minutes from when the server had completed booting. A while later the SQL team informed me of another server where the issue was occurring. I ran the PowerShell copy test on that server and found that it also was having 10 minute interval interruptions starting from the time that the server booted. \n\nI also tried the copy test from some other servers. I found a VM running Windows 7 was not impacted by the issue, my copy test was never interrupted. This gave me a hunch that it related to SMBv3, as this was introduced in Windows 8 and Server 2012. I did more testing across more servers that backed this up. It was only Win 8 / 2012 or newer OS systems that were affected by the issue. This discovery was frustrating, because it seemed to suggest the Isilon was not at fault.\n\nAfter this my frustration sort of peaked, and I went down several rabbit holes where I tried changing the NIC type of the VM, disabling SMBv3, disabling SMBv1/2, changing jumbo frames settings and performing network packet captures and analysis with Microsoft Message Analyzer. None of my changes helped and the packet capture showed the issue occurring but did not help me to understand why.\n\n### Resolution\nI'd like this story to end with me using some incredible feat of technical prowess to solve this issue, but actually I got lucky. I was in the fortunate position of having a second (identical model) Isilon cluster in which the issue did not occur. This had me convinced there was something specific to the Isilon causing the issue, so I painstakingly compared every setting between the two clusters by logging in to their Web UIs.\n\nWhat I discovered was that **they were identical.**\n\n![](/content/images/2018/01/dafuq.jpg)\n\nEventually (after much screaming in to the void) I logged on to each Isilon via SSH and used the Isilon CLI to compare their settings. In particular I ran:\n\n```\nisi smb settings global view\n```\nwhich (among other settings) returned `Support Multichannel: Yes` on the problem server and `Support Multichannel: No` on the problem free server.\n\nA short Google later, I discovered I could change this setting by running:\n```\nisi smb settings global modify --support-multichannel=no\n```\nAfter which my copy test showed no further interruptions!\n\n### Root Cause\n\nI'm not actually completely sure why having this setting enabled caused my problem. However I did do some research to understand the feature and I did find a potential explanation for why it was occurring at exact 10 minute intervals.\n\nMultichannel is an SMB feature that can be used to increase network performance and fault tolerance. There's more info about it here: \n\n- https://blogs.technet.microsoft.com/josebda/2012/06/28/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0/\n\nBut to summarize:\n\n> Windows Server 2012 includes a new feature called SMB Multichannel, part of the SMB 3.0 protocol, which increases the network performance and availability for File Servers.\n\n> SMB Multichannel allows file servers to use multiple network connections simultaneously and provides the following capabilities:\n\n> - Increased throughput. The file server can simultaneously transmit more data using multiple connections for high speed network adapters or multiple network adapters.\n> - Network Fault Tolerance. When using multiple network connections at the same time, the clients can continue to work uninterrupted despite the loss of a network connection.\n> - Automatic Configuration: SMB Multichannel automatically discovers the existence of multiple available network paths and dynamically adds connections as required.\n\nThis article goes on to say that the requirements are that the server has multiple NICs, NIC Teaming or a NIC that supports RSS or RDMA. It seems [there might be an issue with RSS for the VMWare vmxnet3 driver at the moment](https://communities.vmware.com/thread/545782), so maybe that was a contributing factor / the root cause.\n\nI found another Microsoft Blog post from 2012 that I think explains the 10 minute interval:\n\n- https://blogs.technet.microsoft.com/josebda/2012/10/08/windows-server-2012-file-servers-and-smb-3-0-simpler-and-easier-by-design/\n\n> Beyond surviving the failure of an interface, SMB Multichannel can also re-establish the connections when a NIC “comes back”. Again, this happens automatically and typically within seconds, due to the fact that the SMB client will listen to the networking activity of the machine, being notified whenever a new network interface arrives on the system.\n\n> Using the example on the item above, if the cable for the second 10GbE interface is plugged back in, the SMB client will get a notification and re-evaluate its policy. This will lead to the second set of channel being brought back and the throughput going up to 20Gbps again, automatically.\n\n> If a new interface arrives on the server side, the behavior is slightly different. If you lost one of the server NICs and it comes back, the server will immediately accept connections on the new interface. However, clients with existing connections might take up to 10 minutes the readjust their policies. This is because **they will poll the server for configuration adjustments in 10-minute intervals.** After 10 minutes, all clients will be back to full throughput automatically.\n\nThis also explains why it correlated with the server startup as (I assume) the interval starts from the point SMBv3 is initialised.\n\n---\n\nIf you've suffered with this issue or anything similar and/or know any more detail about the root cause, please let me know in the comments below.",
                        "html": "<p>I recently resolved an issue with our Isilon storage cluster that was causing file writes to be interrupted and fail. While diagnosing the issue I discovered that (intriguingly) the disruption was occurring at (almost) exact 10 minute intervals. Any in progress write operation occurring at that interval would fail.</p>\n\n<p><em>TL;DR: In case you want to skip straight to the resolution, the cause of my issue turned out to be the <code>support-multichannel</code> setting on the Isilon, which when enabled caused the issue to occur.</em></p>\n\n<p><em>I resolved the problem by logging on to the Isilon via SSH and entering the following command to verify the setting was enabled:</em></p>\n\n<pre><code>isi smb settings global view  \n</code></pre>\n\n<p><em>And then entered this command to set <code>support-multichannel</code> to be off:</em></p>\n\n<pre><code>isi smb settings global modify --support-multichannel=no  \n</code></pre>\n\n<p><em>Immediately after which the disruptions no longer occurred.</em></p>\n\n<hr />\n\n<p>I wanted to Blog this issue for two reasons:</p>\n\n<ol>\n<li>When googling, I found nothing like this on the internet, at least related specifically to an Isilon. It took me several days of solid work to resolve, so I hope to save someone else at least some of that time.  </li>\n<li>In a job interview recently I was asked to describe a complex problem I solved and how I went about solving it. I completely blanked on the question (which is annoying because as a former manager I used to ask this classic question all the time so really should have expected it). It occurred to me that what I describe here would (perhaps) now be a good example to use in the future. However I found even just a few weeks later I was already struggling to recall it. This blog post will hopefully rectify that.</li>\n</ol>\n\n<h3 id=\"symptoms\">Symptoms</h3>\n\n<p>The issue was reported to me by our database team, who were finding that their SQL database backups were sometimes failing at random on a Windows 2012 R2 SQL server. Those backups were being written to a 5 node Isilon cluster. The issue was apparently particularly likely to occur with large (50GB+) databases, but could also occur for a database of any size.</p>\n\n<h3 id=\"diagnosis\">Diagnosis</h3>\n\n<p>The first step I took to diagnose the issue was to perform a few backups of one of the large databases to the local server storage to see if they failed. They did not, which seemed to rule out anything being wrong with the SQL side of things.</p>\n\n<p>The SQL server was a VMWare VM, so I performed a vMotion of the server to another host. The issue still occurred, which seemed to rule out anything with the underlying host hardware.</p>\n\n<p>Next I found some suggestions online that the network card setting \"Allow the computer to turn off this device to save power\" could cause intermittent interruptions. This setting was enabled on our NIC so I disabled it. This made no difference to the issue.</p>\n\n<p>Following this I wanted to see how frequently the failures/interruptions were occurring, so I used <code>fsutil file createnew 100mb.txt 104857600</code> to create a 100mb test file and then wrote a very dirty PowerShell script to copy this file repeatedly to the Isilon and log any errors that occurred along with a timestamp:</p>\n\n<pre><code>$file = '100mb.txt'\n\nWhile ($True) {  \n    Try{\n        Copy-Item $file \"\\\\myisilonserver\\somefolder\\\"\n        Remove-Item \"\\\\myisilonserver\\somefolder\\$file\"\n    }Catch{\n        Get-Date -f s\n        Write-Error $_\n    }\n}\n</code></pre>\n\n<p>Running this revealed the very intriguing fact that the interruptions were occurring at almost 10 minute intervals. I say almost as it was 10 minutes and perhaps a fraction of a second, so it would sort of slowly drift. Here's some example timestamps I logged:</p>\n\n<pre><code>2017-12-15T08:27:42  \n2017-12-15T08:37:42  \n2017-12-15T08:47:43  \n</code></pre>\n\n<p>After discovering this I ran a constant ping to www.google.com to see if that was interrupted at the same time as the copies were. It was not, suggesting the NIC was not the cause. I was able to get permission to reboot the Isilon, but this made no difference to the issue. </p>\n\n<p>I also rebooted the SQL server VM, and this didn't resolve the issue, but I discovered that it changed the time it was occurring. It was still at 10 minute intervals, but it seemed to correlate as being 10 minutes from when the server had completed booting. A while later the SQL team informed me of another server where the issue was occurring. I ran the PowerShell copy test on that server and found that it also was having 10 minute interval interruptions starting from the time that the server booted. </p>\n\n<p>I also tried the copy test from some other servers. I found a VM running Windows 7 was not impacted by the issue, my copy test was never interrupted. This gave me a hunch that it related to SMBv3, as this was introduced in Windows 8 and Server 2012. I did more testing across more servers that backed this up. It was only Win 8 / 2012 or newer OS systems that were affected by the issue. This discovery was frustrating, because it seemed to suggest the Isilon was not at fault.</p>\n\n<p>After this my frustration sort of peaked, and I went down several rabbit holes where I tried changing the NIC type of the VM, disabling SMBv3, disabling SMBv1/2, changing jumbo frames settings and performing network packet captures and analysis with Microsoft Message Analyzer. None of my changes helped and the packet capture showed the issue occurring but did not help me to understand why.</p>\n\n<h3 id=\"resolution\">Resolution</h3>\n\n<p>I'd like this story to end with me using some incredible feat of technical prowess to solve this issue, but actually I got lucky. I was in the fortunate position of having a second (identical model) Isilon cluster in which the issue did not occur. This had me convinced there was something specific to the Isilon causing the issue, so I painstakingly compared every setting between the two clusters by logging in to their Web UIs.</p>\n\n<p>What I discovered was that <strong>they were identical.</strong></p>\n\n<p><img src=\"/content/images/2018/01/dafuq.jpg\" alt=\"\" /></p>\n\n<p>Eventually (after much screaming in to the void) I logged on to each Isilon via SSH and used the Isilon CLI to compare their settings. In particular I ran:</p>\n\n<pre><code>isi smb settings global view  \n</code></pre>\n\n<p>which (among other settings) returned <code>Support Multichannel: Yes</code> on the problem server and <code>Support Multichannel: No</code> on the problem free server.</p>\n\n<p>A short Google later, I discovered I could change this setting by running:  </p>\n\n<pre><code>isi smb settings global modify --support-multichannel=no  \n</code></pre>\n\n<p>After which my copy test showed no further interruptions!</p>\n\n<h3 id=\"rootcause\">Root Cause</h3>\n\n<p>I'm not actually completely sure why having this setting enabled caused my problem. However I did do some research to understand the feature and I did find a potential explanation for why it was occurring at exact 10 minute intervals.</p>\n\n<p>Multichannel is an SMB feature that can be used to increase network performance and fault tolerance. There's more info about it here: </p>\n\n<ul>\n<li><a href=\"https://blogs.technet.microsoft.com/josebda/2012/06/28/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0/\">https://blogs.technet.microsoft.com/josebda/2012/06/28/the-basics-of-smb-multichannel-a-feature-of-windows-server-2012-and-smb-3-0/</a></li>\n</ul>\n\n<p>But to summarize:</p>\n\n<blockquote>\n  <p>Windows Server 2012 includes a new feature called SMB Multichannel, part of the SMB 3.0 protocol, which increases the network performance and availability for File Servers.</p>\n  \n  <p>SMB Multichannel allows file servers to use multiple network connections simultaneously and provides the following capabilities:</p>\n  \n  <ul>\n  <li>Increased throughput. The file server can simultaneously transmit more data using multiple connections for high speed network adapters or multiple network adapters.</li>\n  <li>Network Fault Tolerance. When using multiple network connections at the same time, the clients can continue to work uninterrupted despite the loss of a network connection.</li>\n  <li>Automatic Configuration: SMB Multichannel automatically discovers the existence of multiple available network paths and dynamically adds connections as required.</li>\n  </ul>\n</blockquote>\n\n<p>This article goes on to say that the requirements are that the server has multiple NICs, NIC Teaming or a NIC that supports RSS or RDMA. It seems <a href=\"https://communities.vmware.com/thread/545782\">there might be an issue with RSS for the VMWare vmxnet3 driver at the moment</a>, so maybe that was a contributing factor / the root cause.</p>\n\n<p>I found another Microsoft Blog post from 2012 that I think explains the 10 minute interval:</p>\n\n<ul>\n<li><a href=\"https://blogs.technet.microsoft.com/josebda/2012/10/08/windows-server-2012-file-servers-and-smb-3-0-simpler-and-easier-by-design/\">https://blogs.technet.microsoft.com/josebda/2012/10/08/windows-server-2012-file-servers-and-smb-3-0-simpler-and-easier-by-design/</a></li>\n</ul>\n\n<blockquote>\n  <p>Beyond surviving the failure of an interface, SMB Multichannel can also re-establish the connections when a NIC “comes back”. Again, this happens automatically and typically within seconds, due to the fact that the SMB client will listen to the networking activity of the machine, being notified whenever a new network interface arrives on the system.</p>\n  \n  <p>Using the example on the item above, if the cable for the second 10GbE interface is plugged back in, the SMB client will get a notification and re-evaluate its policy. This will lead to the second set of channel being brought back and the throughput going up to 20Gbps again, automatically.</p>\n  \n  <p>If a new interface arrives on the server side, the behavior is slightly different. If you lost one of the server NICs and it comes back, the server will immediately accept connections on the new interface. However, clients with existing connections might take up to 10 minutes the readjust their policies. This is because <strong>they will poll the server for configuration adjustments in 10-minute intervals.</strong> After 10 minutes, all clients will be back to full throughput automatically.</p>\n</blockquote>\n\n<p>This also explains why it correlated with the server startup as (I assume) the interval starts from the point SMBv3 is initialised.</p>\n\n<hr />\n\n<p>If you've suffered with this issue or anything similar and/or know any more detail about the root cause, please let me know in the comments below.</p>",
                        "image": "/content/images/2018/01/dell-emc-isilon.png",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-01-05 20:53:54",
                        "created_by": 1,
                        "updated_at": "2018-01-05 23:54:10",
                        "updated_by": 1,
                        "published_at": "2018-01-05 22:08:15",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 63,
                        "uuid": "011027c8-3938-4089-83b1-bf908ca0784b",
                        "title": "PowerShell Core 6.0",
                        "slug": "powershell-core",
                        "markdown": "PowerShell now comes in two flavours, (Vanilla) Windows PowerShell and PowerShell Core (..Rocky Road? ice cream flavour TBD). PowerShell Core is a version of PowerShell built on top of [.NET Core](https://github.com/dotnet/core). The GA version of PowerShell Core is due to be released on the 10th January and Release Candidate versions have been available for some time.\n\nDespite being a new version of PowerShell, its release continues the existing versioning history and also marks the next major release of PowerShell. As such the release will be numbered 6.0 (even though it's effectively 1.0 for Core). The [PowerShell dev team have decided to continue the existing version numbering](https://github.com/PowerShell/PowerShell/issues/5165) on the basis that:\n\n> *\"..as an engine and platform, PSCore6 is a superset of Windows PowerShell 5.1.\"* -- [Steve Lee](https://twitter.com/steve_msft?lang=en)\n\nWhile there are two versions today, it is important to note that PowerShell Core is being touted by Microsoft as the future of PowerShell, with the expectation that only bug fixes will be added to the (still delicious) Vanilla-flavoured PowerShell going forward. \n\nThey've been pretty clear that **all new feature development will go in to PowerShell Core.** And as such..\n\n>  *\"..there are no plans for a Windows PowerShell 6.0.\"* -- [Joey Aiello](https://twitter.com/joeyaiello?lang=en)\n\n![](/content/images/2018/01/say-what-meme.jpg)\n\n### So.. why are there two versions?\n\nWindows PowerShell was written on top of the .NET Framework. As a result, it can only be installed on Windows. PowerShell Core (due to being built on top of .NET Core) is cross platform, and available for install on Windows, macOS and various distributions of Linux.\n\n**In an increasingly cloudified cross-platform world, this makes a lot of sense.**\n\nHowever, while the PowerShell team want Core to ultimately have as much backwards compatibility as possible, right now there are some things that won't work in Core that do work in Vanilla. As such the existing version of PowerShell remains available as a fallback/alternative.\n\n### What's different?\n\nAside from the cross-platform compatibility, here are some key differences:\n\n- The PowerShell Core shell and desktop icon are back in black..\n\n![](/content/images/2018/01/PowerShell-Core-6.png)\n\n*(-- This makes sense because PSCore is a slimmed down version of PowerShell and everyone knows black is slimming.)*\n\n- ..and the executable has changed to `pwsh.exe`. Both choices are likely to avoid confusion with Vanilla PowerShell, as (on Windows) they can be installed side by side.\n- It's (for some things) fast(er). Not necessarily for every script/operation, but for example: I have a set of Pester tests for a module that take 53 seconds to run under Windows PowerShell. Under PowerShell Core they complete in a blistering 15 seconds.\n- The install directory for PowerShell Core is `C:\\Program Files\\PowerShell` (where as it's `\\WindowsPowerShell` for Vanilla) and the user directory is `C:\\users\\<username>\\documents\\PowerShell`. This is where you'll store your profile.ps1 and any user specific Modules/Scripts. As a result Windows PowerShell and PowerShell Core do not share the same `$env:PSModulePath`, which means if you run them together you will have separate versions of modules installed for each.\n- As already mentioned, the underlying .NET version is different and so some existing things won't work (see: downsides).\n\n### What's new?\n\nThe release of PowerShell Core also sees the release of the new 6.0 version of PowerShell, so while this is a significant change from an underlying engine perspective there are also a number of new features and enhancements as you'd expect with any major release. \n\nHere are some of my favourites:\n\n- PowerShell remoting over SSH. This obviously makes sense with the new support for non-Windows OSes and should mean you can PowerShell remote anywhere PowerShell Core is supported/installed. SSH support is being built in to the existing `*-PSSession` cmdlets.\n- `Invoke-WebRequest` now has a `-SkipCertificateCheck` switch. If you've ever struggled with the hacky workarounds of connecting to a site that is using a self-signed certificate, this switch should (hopefully) make that a problem of the past.\n- It's now possible to get an array of characters via the `..` operator. For example: `'a'..'z'` (note you need to include the quote marks around the letters).\n- Putting `&` at the end of a pipeline will cause the pipeline to be run as a PowerShell job (this probably won't confuse people in the future..):\n\n![](/content/images/2018/01/PowerShell-Core-6-Ampersand.jpg)\n\n- $OutputEncoding default has been changed to be UTF8 without BOM rather than ASCII. The default encoding output of PowerShell has been a sticking point in the [past](https://stackoverflow.com/questions/40098771/changing-powershells-default-output-encoding-to-utf-8).\n- They've added `-AsHashtable` to `ConvertFrom-Json` to return a Hashtable instead. I can see that being useful.\n- They've made `Import-Csv` support CR, LF and CRLF as line delimiters. Again, can see lots of future use cases for this that will avoid hacky `-split` type workarounds.\n\nIf you'd like to see all of the changes you can review the [Change Log](https://github.com/PowerShell/PowerShell/blob/master/CHANGELOG.md) on Git.\n\n### What are the downsides?\n\nDue to the underlying .NET engine change there are a number of things that (at present) are not supported in PowerShell Core. \n\n- There is no WMI support. For example there is no `Get-WMIObject` cmdlet at present in PowerShell Core. However the newer `Get-CIM*` cmdlets are present:\n\n![](/content/images/2018/01/PowerShell-Core-6-Get-CimInstance.jpg)\n\n- The ActiveDirectory module/cmdlets are not yet supported.\n- There's no support for the Windows Presentation Foundation (WPF) or Windows Forms.\n- The Workflows functionality is missing, although I think this was a rarely used feature of PowerShell for most.\n- Many other first and third party modules may not be compatible and will need to be identified (and potentially updated) on a case by case basis.\n\nThere is apparently a Windows Compatibility Pack for .NET core in the works that adds back in some of the missing functionality. [Richard Siddaway blogged about this recently](https://richardspowershellblog.wordpress.com/2018/01/04/windows-compatibility-pack/).\n\n- There is no PowerShell ISE for PowerShell Core. ISE has been sidelined for a while and Microsoft now recommend Visual Studio Code with the PowerShell plugin as the defacto PowerShell IDE.\n\nIf you're using AppVeyor and would like to test PowerShell Core support as part of your pipeline, I suggest watching [this issue on the AppVeyor site](https://help.appveyor.com/discussions/questions/16107-different-images) as [Oliver Lipkau](https://github.com/lipkau) has posed the question of the best way to implement this. I suspect when Core is GA AppVeyor will provide an official image fairly quickly.\n\n### Final thoughts\n\nHaving browsed around various blogs and articles to source information for this post I can see that this change is going to cause some people some anguish. I'm sure it will frustrate me in the near future, but my feeling at this point is that we should get on board, the earlier the better. If you are currently maintaining any public modules or scripts you should check them for compatibility and identify the outcome in your documentation/readme.md. \n\nIf PowerShell Core is a success as a cross-platform scripting language then that will add tremendous career value to those of us who are (traditionally lesser-paid than our Unix counterpart) Microsoft-stack professionals. It could also greatly reduce the complexity of the tools/scripts/technologies that we have to support in the future.",
                        "html": "<p>PowerShell now comes in two flavours, (Vanilla) Windows PowerShell and PowerShell Core (..Rocky Road? ice cream flavour TBD). PowerShell Core is a version of PowerShell built on top of <a href=\"https://github.com/dotnet/core\">.NET Core</a>. The GA version of PowerShell Core is due to be released on the 10th January and Release Candidate versions have been available for some time.</p>\n\n<p>Despite being a new version of PowerShell, its release continues the existing versioning history and also marks the next major release of PowerShell. As such the release will be numbered 6.0 (even though it's effectively 1.0 for Core). The <a href=\"https://github.com/PowerShell/PowerShell/issues/5165\">PowerShell dev team have decided to continue the existing version numbering</a> on the basis that:</p>\n\n<blockquote>\n  <p><em>\"..as an engine and platform, PSCore6 is a superset of Windows PowerShell 5.1.\"</em> -- <a href=\"https://twitter.com/steve_msft?lang=en\">Steve Lee</a></p>\n</blockquote>\n\n<p>While there are two versions today, it is important to note that PowerShell Core is being touted by Microsoft as the future of PowerShell, with the expectation that only bug fixes will be added to the (still delicious) Vanilla-flavoured PowerShell going forward. </p>\n\n<p>They've been pretty clear that <strong>all new feature development will go in to PowerShell Core.</strong> And as such..</p>\n\n<blockquote>\n  <p><em>\"..there are no plans for a Windows PowerShell 6.0.\"</em> -- <a href=\"https://twitter.com/joeyaiello?lang=en\">Joey Aiello</a></p>\n</blockquote>\n\n<p><img src=\"/content/images/2018/01/say-what-meme.jpg\" alt=\"\" /></p>\n\n<h3 id=\"sowhyaretheretwoversions\">So.. why are there two versions?</h3>\n\n<p>Windows PowerShell was written on top of the .NET Framework. As a result, it can only be installed on Windows. PowerShell Core (due to being built on top of .NET Core) is cross platform, and available for install on Windows, macOS and various distributions of Linux.</p>\n\n<p><strong>In an increasingly cloudified cross-platform world, this makes a lot of sense.</strong></p>\n\n<p>However, while the PowerShell team want Core to ultimately have as much backwards compatibility as possible, right now there are some things that won't work in Core that do work in Vanilla. As such the existing version of PowerShell remains available as a fallback/alternative.</p>\n\n<h3 id=\"whatsdifferent\">What's different?</h3>\n\n<p>Aside from the cross-platform compatibility, here are some key differences:</p>\n\n<ul>\n<li>The PowerShell Core shell and desktop icon are back in black..</li>\n</ul>\n\n<p><img src=\"/content/images/2018/01/PowerShell-Core-6.png\" alt=\"\" /></p>\n\n<p><em>(-- This makes sense because PSCore is a slimmed down version of PowerShell and everyone knows black is slimming.)</em></p>\n\n<ul>\n<li>..and the executable has changed to <code>pwsh.exe</code>. Both choices are likely to avoid confusion with Vanilla PowerShell, as (on Windows) they can be installed side by side.</li>\n<li>It's (for some things) fast(er). Not necessarily for every script/operation, but for example: I have a set of Pester tests for a module that take 53 seconds to run under Windows PowerShell. Under PowerShell Core they complete in a blistering 15 seconds.</li>\n<li>The install directory for PowerShell Core is <code>C:\\Program Files\\PowerShell</code> (where as it's <code>\\WindowsPowerShell</code> for Vanilla) and the user directory is <code>C:\\users\\&lt;username&gt;\\documents\\PowerShell</code>. This is where you'll store your profile.ps1 and any user specific Modules/Scripts. As a result Windows PowerShell and PowerShell Core do not share the same <code>$env:PSModulePath</code>, which means if you run them together you will have separate versions of modules installed for each.</li>\n<li>As already mentioned, the underlying .NET version is different and so some existing things won't work (see: downsides).</li>\n</ul>\n\n<h3 id=\"whatsnew\">What's new?</h3>\n\n<p>The release of PowerShell Core also sees the release of the new 6.0 version of PowerShell, so while this is a significant change from an underlying engine perspective there are also a number of new features and enhancements as you'd expect with any major release. </p>\n\n<p>Here are some of my favourites:</p>\n\n<ul>\n<li>PowerShell remoting over SSH. This obviously makes sense with the new support for non-Windows OSes and should mean you can PowerShell remote anywhere PowerShell Core is supported/installed. SSH support is being built in to the existing <code>*-PSSession</code> cmdlets.</li>\n<li><code>Invoke-WebRequest</code> now has a <code>-SkipCertificateCheck</code> switch. If you've ever struggled with the hacky workarounds of connecting to a site that is using a self-signed certificate, this switch should (hopefully) make that a problem of the past.</li>\n<li>It's now possible to get an array of characters via the <code>..</code> operator. For example: <code>'a'..'z'</code> (note you need to include the quote marks around the letters).</li>\n<li>Putting <code>&amp;</code> at the end of a pipeline will cause the pipeline to be run as a PowerShell job (this probably won't confuse people in the future..):</li>\n</ul>\n\n<p><img src=\"/content/images/2018/01/PowerShell-Core-6-Ampersand.jpg\" alt=\"\" /></p>\n\n<ul>\n<li>$OutputEncoding default has been changed to be UTF8 without BOM rather than ASCII. The default encoding output of PowerShell has been a sticking point in the <a href=\"https://stackoverflow.com/questions/40098771/changing-powershells-default-output-encoding-to-utf-8\">past</a>.</li>\n<li>They've added <code>-AsHashtable</code> to <code>ConvertFrom-Json</code> to return a Hashtable instead. I can see that being useful.</li>\n<li>They've made <code>Import-Csv</code> support CR, LF and CRLF as line delimiters. Again, can see lots of future use cases for this that will avoid hacky <code>-split</code> type workarounds.</li>\n</ul>\n\n<p>If you'd like to see all of the changes you can review the <a href=\"https://github.com/PowerShell/PowerShell/blob/master/CHANGELOG.md\">Change Log</a> on Git.</p>\n\n<h3 id=\"whatarethedownsides\">What are the downsides?</h3>\n\n<p>Due to the underlying .NET engine change there are a number of things that (at present) are not supported in PowerShell Core. </p>\n\n<ul>\n<li>There is no WMI support. For example there is no <code>Get-WMIObject</code> cmdlet at present in PowerShell Core. However the newer <code>Get-CIM*</code> cmdlets are present:</li>\n</ul>\n\n<p><img src=\"/content/images/2018/01/PowerShell-Core-6-Get-CimInstance.jpg\" alt=\"\" /></p>\n\n<ul>\n<li>The ActiveDirectory module/cmdlets are not yet supported.</li>\n<li>There's no support for the Windows Presentation Foundation (WPF) or Windows Forms.</li>\n<li>The Workflows functionality is missing, although I think this was a rarely used feature of PowerShell for most.</li>\n<li>Many other first and third party modules may not be compatible and will need to be identified (and potentially updated) on a case by case basis.</li>\n</ul>\n\n<p>There is apparently a Windows Compatibility Pack for .NET core in the works that adds back in some of the missing functionality. <a href=\"https://richardspowershellblog.wordpress.com/2018/01/04/windows-compatibility-pack/\">Richard Siddaway blogged about this recently</a>.</p>\n\n<ul>\n<li>There is no PowerShell ISE for PowerShell Core. ISE has been sidelined for a while and Microsoft now recommend Visual Studio Code with the PowerShell plugin as the defacto PowerShell IDE.</li>\n</ul>\n\n<p>If you're using AppVeyor and would like to test PowerShell Core support as part of your pipeline, I suggest watching <a href=\"https://help.appveyor.com/discussions/questions/16107-different-images\">this issue on the AppVeyor site</a> as <a href=\"https://github.com/lipkau\">Oliver Lipkau</a> has posed the question of the best way to implement this. I suspect when Core is GA AppVeyor will provide an official image fairly quickly.</p>\n\n<h3 id=\"finalthoughts\">Final thoughts</h3>\n\n<p>Having browsed around various blogs and articles to source information for this post I can see that this change is going to cause some people some anguish. I'm sure it will frustrate me in the near future, but my feeling at this point is that we should get on board, the earlier the better. If you are currently maintaining any public modules or scripts you should check them for compatibility and identify the outcome in your documentation/readme.md. </p>\n\n<p>If PowerShell Core is a success as a cross-platform scripting language then that will add tremendous career value to those of us who are (traditionally lesser-paid than our Unix counterpart) Microsoft-stack professionals. It could also greatly reduce the complexity of the tools/scripts/technologies that we have to support in the future.</p>",
                        "image": "/content/images/2018/01/PowerShell-6-Core-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "PowerShell Core 6.0",
                        "meta_description": "A blog post about the new PowerShell Core 6.0 version and how it compares to the existing Windows PowerShell. PSCore is due for GA on the 10th January 2018.",
                        "author_id": 1,
                        "created_at": "2018-01-06 13:54:37",
                        "created_by": 1,
                        "updated_at": "2018-01-09 00:01:03",
                        "updated_by": 1,
                        "published_at": "2018-01-09 00:01:03",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 64,
                        "uuid": "5195b5a5-347d-423e-a102-90f35908ef8b",
                        "title": "Getting started with Puppet on Windows",
                        "slug": "getting-started-with-puppet-on-windows",
                        "markdown": "This blog post describes some initial steps you can take to learn Puppet, particularly to control Windows machines. I'm currently learning Puppet via the [Puppet 5 Beginners Guide (Third Edition)](https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&tag=exsite0a-21&camp=1634&creative=6738&linkCode=as2&creativeASIN=178847290X&linkId=17c0eba83d12b8e7b213b6899d3b5207) book by John Arundel. \n\nThe book provides you with a Vagrant Ubuntu VM in order to experiment with Puppet. Below I have recreated some of the examples from the book but with minor changes necessary for them to work under Windows. It should therefore (hopefully) serve as a good initial guide to getting started with Puppet for controlling Windows machines. That being said I strongly recommend you also get the book, which is excellent and goes much further than I will cover below.\n\n# Installing the agent on Windows \n\nBefore you can start using Puppet, you need to install the Puppet agent on Windows. Puppet supports master/slave and master-less architectures, the latter of which is simpler for testing purposes.\n\n> It should probably go without saying that Puppet is a tool with the power to make significant changes to your machine (both constructive and destructive). As such I strongly recommend you use an isolated testing machine for playing with Puppet.\n\nTo install the Puppet agent on a Windows machine you simply need to:\n\n1. Download the [MSI installer](https://puppet.com/docs/pe/2017.3/installing/installing_agents.html#install-windows-agents-with-the-msi-package).\n- Run/install using all defaults.\n\nTo check it's installed and working you can:\n\n1. Open a PowerShell window (as administrator).\n- run `puppet config print config` to output puppet's configuration file path.\n- run `puppet config print modulepath` to output the default module directory paths.\n\n# Hello, Puppet\n\nAs used in the Puppet 5 Beginners book, a good first example of the general syntax for Puppet is the following, which simply sets a text file to \"hello, world\":\n\n```\nfile { 'c:\\temp\\hello.txt':\n  ensure  => file,\n  content => \"hello, world\\n\",\n}\n```\nIf you copy this in to a text file and save is as `hello.pp` you will have written your first puppet manifest (congratulations!).\n\nTo execute it, open a PowerShell window, change to the directory you saved the .pp file in and then run the following: \n```\npuppet apply hello.pp\n```\nYou will see Puppet start and compile a catalog for your manifest. Puppet will then check if `hello.txt` exists under `c:\\temp`. If it doesn't exist, it will create it. If the content of the file is not the text \"hello, world\" followed by a new line, it will modify it to be as such (note, any existing contents will be replaced entirely).\n\n> In case you are not aware, Puppet is an example of an [idempotent](https://puppet.com/blog/idempotence-not-just-a-big-and-scary-word) configuration management tool. As such it will only make the minimum changes necessary to bring the system in to the state declared in your manifests. Once hello.txt exists with the content we defined Puppet will not attempt to change/rewrite it unless a subsequent run finds it to be out of compliance with your declarations.\n\n# Resources\n\nThe syntax shown above is a *resource declaration* and the syntax for these is always this:\n```\nRESOURCE_TYPE { TITLE:\n  ATTRIBUTE => VALUE\n}\n```\nThe `RESOURCE_TYPE` is how you tell Puppet the type of resource you're declaring. There are a large number of built-in resource types and the Puppet 5.3 ones are listed in a single page [here](https://puppet.com/docs/puppet/5.3/type.html).\n\nResource Types include things like:\n\n- exec\n- file\n- group\n- host\n- interface\n- notify\n- package\n- scheduled_task\n- service\n- user\n- ..and many more (including the ability to make custom ones).\n\nThe `TITLE` is the name that puppet uses to identify the resource internally. **Every resource must have a unique title.** With file resources this is typically the full path to the file (which is therefore unique by nature).\n\nThe `attributes` describe how the resource should be configured. These vary by resource (and there are more for `file` than were shown above). All resources support `ensure` but the possible values for it vary by resource.\n\n# Performing a dry run\n\nYou can get Puppet to show you what it would modify without actually performing those changes by adding the `--noop` flag:\n```\npuppet apply --noop your_file.pp\n```\nIf the file existed but would be modified by Puppet the above would only show you that the MD5 hashes of the files were different than what it expected it to be. \n\nTo see what actual changes it would make to files, you can add the `--show_diff` switch. However while this simply works on a Unix system, [for Windows you need to have a 3rd party diff tool and the Puppet diff setting to be properly set](https://puppet.com/docs/pe/2017.2/trouble_windows.html#diffs).\n\n# Packages\n\nWhile managing files is cool, managing packages is cooler. To do this you use the puppet `Package` resource. On Windows by default this can be used to execute `.msi` or `.exe` installers like so:\n```\npackage { 'Atom':\n  ensure   => installed,\n  source   => 'C:/Installers/AtomSetup-x64.exe',\n}\n```\n> It's generally best practice to use forward slashes in paths as shown here because it's easy with a backslash to accidentally end up escaping a character by mistake. Another option is to use double quoted strings and then double each of the back slashes.\n\nAlternatively if you install the [Puppet Chocolatey](https://github.com/chocolatey/puppet-chocolatey) module you can use [Chocolatey](https://chocolatey.org) as a package provider.\n\nYou can manually install the Chocolatey Package provider as follows:\n```\npuppet module install puppetlabs-chocolatey --version 3.0.0\n```\nThen add it to your puppet manifest by including the following line:\n```\ninclude chocolatey\n```\nat the top. You can then use chocolatey to install a named package by referring to it in the `title` field, and ensuring you specify chocolatey as the provider:\n```\npackage { 'Atom':\n  ensure   => '1.23.3',\n  provider => chocolatey,\n}\n```\n![](/content/images/2018/01/Puppet-Chocolatey-3.png)\n\n> Up to now you might have been writing your puppet manifests with something like notepad. I recommend using [Atom](https://atom.io/) as it's a much more powerful editor. We could of course download it's installer and install it manually, or if you've already followed the above you've just installed it with Puppet.\n\nIf we want to use `Puppet` to remove some software, we can change `ensure` to be `ensure => absent`. You can use `ensure => latest` to always get the latest available version, but it's generally considered best practice to specify a specific version so that your packages aren't being automatically updated whenever the source version changes.\n\n> When you want to remove something with Puppet it's important to remember to change it's configuration to be `ensure => absent` and then to apply that via Puppet. Simply removing the configuration for something from the file is not going to uninstall/remove it, it just removes Puppet from evaluating/controlling it. This is true for all resources.\n\nIf you want to see what version of a package Puppet thinks is installed, you can use the `Puppet Resource` tool:\n```\npuppet resource package atom\n```\nThis will output a Puppet manifest with the current state of the named resource. In fact if you don't specifcy a specific `title` you get a manifest for all resources of that type. Try `puppet resource package` for example.\n\n> Beware the `title` field (and other fields) in your manifest can be case sensitive. I was originally referring to Atom as 'atom' in my `Package` declaration, but doing so stopped `ensure => 'absent'` from working. When I ran `puppet resource package atom` I could see that Puppet was identifying it as 'Atom'.\n\n# Services\n\nThe next kind of resource you might want to manage is services. Predictably, this is done with the `service` resource type:\n```\nservice { 'wuauserv':\n  ensure => running,\n  enable => true,\n}\n```\nThe above will ensure the Windows Update service is always running and enabled.\n\n> On Windows you need to use the short name for the service, not the display name (e.g `wuauserv` not `Windows Updates`).\n\n`Service` is an example of a fairly generic Puppet resource. The declarations are very similar on Unix or Windows and Puppet knows what underlying provider to call in the OS to manage the services. On Windows, that is `net.exe` which should always be present by default.\n\n# Puppet Describe\n\nPuppet has a built-in tool to help you discover all of the different attributes associated with a resource called `Describe`. For example:\n```\npuppet describe service\n```\nReturns a description of the service resource and a complete list of all the attributes and their associated valid values. \n\nTo see a list of all available resource types you can enter:\n```\npuppet describe --list\n```\n\n# Package-file-service Pattern\n\nNow that you know how to manage files, packages and services you have the three tools that in general allow you to manage software. This is referred to as the 'package-file-service pattern'.\n\nHere's an example:\n\n```\npackage { 'mysql':\n  ensure   => installed,\n  provider => chocolatey,\n  notify   => Service['mysql']\n}\n\nfile { 'C:/mysql-5.5/mysql.cnf':\n  source => 'c:/source/mysql.cnf',\n  notify => Service['mysql']\n}\n\nservice { 'mysql':\n  ensure => running,\n  enable => true,\n}\n```\nThis uses `package` to install MySQL, `file` to make changes to it's configuration file and `service` to ensure it's running.\n\nThe example above also introduces the `notify` attribute. This is a way to define dependencies between different parts of your configuration. It ensure that if the package or configuration file change that the service is also restarted. Puppet wouldn't do this by default because the service would already be running so (due to idempotency) it would see no reason to modify it's state. Using `notify` in this instance ensures that config or install changes take an immediate effect by also restarting the service. It performs a restart because that is the default action performed on a service when it is notified.\n\n# Users and Groups\n\nYou can use the puppet `user` resource type to manage local users and groups (whether you should though is worth [considering](https://jumpcloud.com/blog/why-user-management-in-chef-and-puppet-is-a-mistake/)). You can't use this to manage domain users, but you can define domain users to be members of local groups.\n\nThe following will create a group named 'Avengers' and a user named 'TonyStark':\n```\ngroup { 'Avengers':\n  ensure => present,\n}\n\nuser { 'TonyStark':\n  ensure => present,\n  password => 'password123',\n  groups => ['Avengers'],\n}\n```\nDepending on your local security policy you will need to specify a password of a certain complexity.\n\nUnder Unix you can specifiy `gid` and `uid` properties respectively to ensure that the unique ID for users and groups are consistent across systems. You can't do this on Windows, these properties are visible via `puppet resource user <name>` but are read-only.\n\n# Scheduled Tasks\n\nThe `scheduled_task` resource can be used to create Windows Scheduled Tasks. This is the equivalent of `cron` in Unix. The book suggests using `cron`  to set up a routine task to apply your puppet manifests. This is how you might use Puppet in production to ensure your systems remain compliant on a routine basis. This isn't strictly necessary in a Windows environment as by default Puppet Agent runs as a service and applies the manifests every 30 minutes.\n\n> The Puppet 5 Beginners book also suggests that (for a master-less setup) you store your puppet configuration in a source control system like Github and then as part of this task pull the latest changes from the repository  with Git before running `puppet apply`.\n\nIf you didn't want to utilise the Windows service, below is how you can use Puppet to create a scheduled task to routinely apply any puppet configuration files that exist in the folder `C:\\Puppet`:\n\n```\nscheduled_task { 'Run Puppet Every 5 Minutes':\n  ensure    => present,\n  enabled   => true,\n  command   => 'C:/Program Files/Puppet Labs/Puppet/bin/puppet.bat',\n  arguments => 'apply C:/puppet',\n  working_dir => 'C:/Program Files/Puppet Labs/Puppet/bin/',\n  trigger   => {\n    schedule   => daily,\n\tstart_time => '08:00',\n    minutes_interval => 5,\n    minutes_duration => 60,\n  }\n}\n```\nYou may also need to configure this scheduled task to run as a user with elevated permissions, particularly if your Puppet manifests are managing packages.\n\n# Executing Scripts\n\nThe `Exec` resource allows you to execute a command or script. Here is an example:\n```\nexec { 'do-something':\n  command => 'C:/something.exe'\n}\n```\nThis is a little different to the other resources which model some concrete state of the system, while `exec` can run any arbitrary command or script. This might change the state of something or it might not. \n\nAs a result Puppet (ideally) needs a way to know whether it should trigger the `exec`. This is done via either a `creates`, `onlyif` or `unless` attribute.\n\n- `creates =>` specifies a path or file that should exist after the `exec` has completed successfully. If this exists then it acts as a flag to Puppet that it doesn't need to be run again unless it does not exist.\n- `onlyif =>` specifies a command for puppet to run and the exec is executed if it returns a success (zero) exit status.\n- `unless =>` is the opposite of `onlyif`. The command specified causes the `exec` to be triggered unless the command specified returns a success (zero) exit status.\n\nWithout one of these conditions, the `exec` will run on every run of the puppet file which is generally undesirable.\n\nAnother way to control an `exec` resource is to add a `refreshonly => true` attribute. When this is present the `exec` will only execute if another resource triggers it via the `notify` attribute.\n \nBecause it is difficult for Puppet to know the state of things changed by `exec` you should use it with care and consider whether creating a custom resource type might be better.\n\n# Summary\n\nHopefully this guide has given you a useful Windows-centric starting point for Puppet. Beware the above is only a partial summary of what is covered in just the first four chapters of the [Puppet 5 Beginners Guide (Third Edition)](https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&tag=exsite0a-21&camp=1634&creative=6738&linkCode=as2&creativeASIN=178847290X&linkId=17c0eba83d12b8e7b213b6899d3b5207) book, so again I strongly recommend you pick it up, even if your focus is on Windows.\n \nHere are some further links specific to Puppet on Windows that you might find useful:\n\n- [Troubleshooting Puppet on Windows](https://puppet.com/docs/pe/2017.2/trouble_windows.html)\n- [Avoiding Common Windows Gotchas with Puppet](http://codebetter.com/robreynolds/2014/12/03/how-to-avoid-common-windows-gotchas-with-puppet/)\n- [Developing Puppet Modules on Windows](http://cavaliercoder.com/blog/developing-puppet-modules-on-windows.html)\n- [The Puppet on Windows Module Pack](https://github.com/puppetlabs/puppetlabs-windows)",
                        "html": "<p>This blog post describes some initial steps you can take to learn Puppet, particularly to control Windows machines. I'm currently learning Puppet via the <a href=\"https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&amp;tag=exsite0a-21&amp;camp=1634&amp;creative=6738&amp;linkCode=as2&amp;creativeASIN=178847290X&amp;linkId=17c0eba83d12b8e7b213b6899d3b5207\">Puppet 5 Beginners Guide (Third Edition)</a> book by John Arundel. </p>\n\n<p>The book provides you with a Vagrant Ubuntu VM in order to experiment with Puppet. Below I have recreated some of the examples from the book but with minor changes necessary for them to work under Windows. It should therefore (hopefully) serve as a good initial guide to getting started with Puppet for controlling Windows machines. That being said I strongly recommend you also get the book, which is excellent and goes much further than I will cover below.</p>\n\n<h1 id=\"installingtheagentonwindows\">Installing the agent on Windows</h1>\n\n<p>Before you can start using Puppet, you need to install the Puppet agent on Windows. Puppet supports master/slave and master-less architectures, the latter of which is simpler for testing purposes.</p>\n\n<blockquote>\n  <p>It should probably go without saying that Puppet is a tool with the power to make significant changes to your machine (both constructive and destructive). As such I strongly recommend you use an isolated testing machine for playing with Puppet.</p>\n</blockquote>\n\n<p>To install the Puppet agent on a Windows machine you simply need to:</p>\n\n<ol>\n<li>Download the <a href=\"https://puppet.com/docs/pe/2017.3/installing/installing_agents.html#install-windows-agents-with-the-msi-package\">MSI installer</a>.  </li>\n<li>Run/install using all defaults.</li>\n</ol>\n\n<p>To check it's installed and working you can:</p>\n\n<ol>\n<li>Open a PowerShell window (as administrator).  </li>\n<li>run <code>puppet config print config</code> to output puppet's configuration file path.</li>\n<li>run <code>puppet config print modulepath</code> to output the default module directory paths.</li>\n</ol>\n\n<h1 id=\"hellopuppet\">Hello, Puppet</h1>\n\n<p>As used in the Puppet 5 Beginners book, a good first example of the general syntax for Puppet is the following, which simply sets a text file to \"hello, world\":</p>\n\n<pre><code>file { 'c:\\temp\\hello.txt':  \n  ensure  =&gt; file,\n  content =&gt; \"hello, world\\n\",\n}\n</code></pre>\n\n<p>If you copy this in to a text file and save is as <code>hello.pp</code> you will have written your first puppet manifest (congratulations!).</p>\n\n<p>To execute it, open a PowerShell window, change to the directory you saved the .pp file in and then run the following:  </p>\n\n<pre><code>puppet apply hello.pp  \n</code></pre>\n\n<p>You will see Puppet start and compile a catalog for your manifest. Puppet will then check if <code>hello.txt</code> exists under <code>c:\\temp</code>. If it doesn't exist, it will create it. If the content of the file is not the text \"hello, world\" followed by a new line, it will modify it to be as such (note, any existing contents will be replaced entirely).</p>\n\n<blockquote>\n  <p>In case you are not aware, Puppet is an example of an <a href=\"https://puppet.com/blog/idempotence-not-just-a-big-and-scary-word\">idempotent</a> configuration management tool. As such it will only make the minimum changes necessary to bring the system in to the state declared in your manifests. Once hello.txt exists with the content we defined Puppet will not attempt to change/rewrite it unless a subsequent run finds it to be out of compliance with your declarations.</p>\n</blockquote>\n\n<h1 id=\"resources\">Resources</h1>\n\n<p>The syntax shown above is a <em>resource declaration</em> and the syntax for these is always this:  </p>\n\n<pre><code>RESOURCE_TYPE { TITLE:  \n  ATTRIBUTE =&gt; VALUE\n}\n</code></pre>\n\n<p>The <code>RESOURCE_TYPE</code> is how you tell Puppet the type of resource you're declaring. There are a large number of built-in resource types and the Puppet 5.3 ones are listed in a single page <a href=\"https://puppet.com/docs/puppet/5.3/type.html\">here</a>.</p>\n\n<p>Resource Types include things like:</p>\n\n<ul>\n<li>exec</li>\n<li>file</li>\n<li>group</li>\n<li>host</li>\n<li>interface</li>\n<li>notify</li>\n<li>package</li>\n<li>scheduled_task</li>\n<li>service</li>\n<li>user</li>\n<li>..and many more (including the ability to make custom ones).</li>\n</ul>\n\n<p>The <code>TITLE</code> is the name that puppet uses to identify the resource internally. <strong>Every resource must have a unique title.</strong> With file resources this is typically the full path to the file (which is therefore unique by nature).</p>\n\n<p>The <code>attributes</code> describe how the resource should be configured. These vary by resource (and there are more for <code>file</code> than were shown above). All resources support <code>ensure</code> but the possible values for it vary by resource.</p>\n\n<h1 id=\"performingadryrun\">Performing a dry run</h1>\n\n<p>You can get Puppet to show you what it would modify without actually performing those changes by adding the <code>--noop</code> flag:  </p>\n\n<pre><code>puppet apply --noop your_file.pp  \n</code></pre>\n\n<p>If the file existed but would be modified by Puppet the above would only show you that the MD5 hashes of the files were different than what it expected it to be. </p>\n\n<p>To see what actual changes it would make to files, you can add the <code>--show_diff</code> switch. However while this simply works on a Unix system, <a href=\"https://puppet.com/docs/pe/2017.2/trouble_windows.html#diffs\">for Windows you need to have a 3rd party diff tool and the Puppet diff setting to be properly set</a>.</p>\n\n<h1 id=\"packages\">Packages</h1>\n\n<p>While managing files is cool, managing packages is cooler. To do this you use the puppet <code>Package</code> resource. On Windows by default this can be used to execute <code>.msi</code> or <code>.exe</code> installers like so:  </p>\n\n<pre><code>package { 'Atom':  \n  ensure   =&gt; installed,\n  source   =&gt; 'C:/Installers/AtomSetup-x64.exe',\n}\n</code></pre>\n\n<blockquote>\n  <p>It's generally best practice to use forward slashes in paths as shown here because it's easy with a backslash to accidentally end up escaping a character by mistake. Another option is to use double quoted strings and then double each of the back slashes.</p>\n</blockquote>\n\n<p>Alternatively if you install the <a href=\"https://github.com/chocolatey/puppet-chocolatey\">Puppet Chocolatey</a> module you can use <a href=\"https://chocolatey.org\">Chocolatey</a> as a package provider.</p>\n\n<p>You can manually install the Chocolatey Package provider as follows:  </p>\n\n<pre><code>puppet module install puppetlabs-chocolatey --version 3.0.0  \n</code></pre>\n\n<p>Then add it to your puppet manifest by including the following line:  </p>\n\n<pre><code>include chocolatey  \n</code></pre>\n\n<p>at the top. You can then use chocolatey to install a named package by referring to it in the <code>title</code> field, and ensuring you specify chocolatey as the provider:  </p>\n\n<pre><code>package { 'Atom':  \n  ensure   =&gt; '1.23.3',\n  provider =&gt; chocolatey,\n}\n</code></pre>\n\n<p><img src=\"/content/images/2018/01/Puppet-Chocolatey-3.png\" alt=\"\" /></p>\n\n<blockquote>\n  <p>Up to now you might have been writing your puppet manifests with something like notepad. I recommend using <a href=\"https://atom.io/\">Atom</a> as it's a much more powerful editor. We could of course download it's installer and install it manually, or if you've already followed the above you've just installed it with Puppet.</p>\n</blockquote>\n\n<p>If we want to use <code>Puppet</code> to remove some software, we can change <code>ensure</code> to be <code>ensure =&gt; absent</code>. You can use <code>ensure =&gt; latest</code> to always get the latest available version, but it's generally considered best practice to specify a specific version so that your packages aren't being automatically updated whenever the source version changes.</p>\n\n<blockquote>\n  <p>When you want to remove something with Puppet it's important to remember to change it's configuration to be <code>ensure =&gt; absent</code> and then to apply that via Puppet. Simply removing the configuration for something from the file is not going to uninstall/remove it, it just removes Puppet from evaluating/controlling it. This is true for all resources.</p>\n</blockquote>\n\n<p>If you want to see what version of a package Puppet thinks is installed, you can use the <code>Puppet Resource</code> tool:  </p>\n\n<pre><code>puppet resource package atom  \n</code></pre>\n\n<p>This will output a Puppet manifest with the current state of the named resource. In fact if you don't specifcy a specific <code>title</code> you get a manifest for all resources of that type. Try <code>puppet resource package</code> for example.</p>\n\n<blockquote>\n  <p>Beware the <code>title</code> field (and other fields) in your manifest can be case sensitive. I was originally referring to Atom as 'atom' in my <code>Package</code> declaration, but doing so stopped <code>ensure =&gt; 'absent'</code> from working. When I ran <code>puppet resource package atom</code> I could see that Puppet was identifying it as 'Atom'.</p>\n</blockquote>\n\n<h1 id=\"services\">Services</h1>\n\n<p>The next kind of resource you might want to manage is services. Predictably, this is done with the <code>service</code> resource type:  </p>\n\n<pre><code>service { 'wuauserv':  \n  ensure =&gt; running,\n  enable =&gt; true,\n}\n</code></pre>\n\n<p>The above will ensure the Windows Update service is always running and enabled.</p>\n\n<blockquote>\n  <p>On Windows you need to use the short name for the service, not the display name (e.g <code>wuauserv</code> not <code>Windows Updates</code>).</p>\n</blockquote>\n\n<p><code>Service</code> is an example of a fairly generic Puppet resource. The declarations are very similar on Unix or Windows and Puppet knows what underlying provider to call in the OS to manage the services. On Windows, that is <code>net.exe</code> which should always be present by default.</p>\n\n<h1 id=\"puppetdescribe\">Puppet Describe</h1>\n\n<p>Puppet has a built-in tool to help you discover all of the different attributes associated with a resource called <code>Describe</code>. For example:  </p>\n\n<pre><code>puppet describe service  \n</code></pre>\n\n<p>Returns a description of the service resource and a complete list of all the attributes and their associated valid values. </p>\n\n<p>To see a list of all available resource types you can enter:  </p>\n\n<pre><code>puppet describe --list  \n</code></pre>\n\n<h1 id=\"packagefileservicepattern\">Package-file-service Pattern</h1>\n\n<p>Now that you know how to manage files, packages and services you have the three tools that in general allow you to manage software. This is referred to as the 'package-file-service pattern'.</p>\n\n<p>Here's an example:</p>\n\n<pre><code>package { 'mysql':  \n  ensure   =&gt; installed,\n  provider =&gt; chocolatey,\n  notify   =&gt; Service['mysql']\n}\n\nfile { 'C:/mysql-5.5/mysql.cnf':  \n  source =&gt; 'c:/source/mysql.cnf',\n  notify =&gt; Service['mysql']\n}\n\nservice { 'mysql':  \n  ensure =&gt; running,\n  enable =&gt; true,\n}\n</code></pre>\n\n<p>This uses <code>package</code> to install MySQL, <code>file</code> to make changes to it's configuration file and <code>service</code> to ensure it's running.</p>\n\n<p>The example above also introduces the <code>notify</code> attribute. This is a way to define dependencies between different parts of your configuration. It ensure that if the package or configuration file change that the service is also restarted. Puppet wouldn't do this by default because the service would already be running so (due to idempotency) it would see no reason to modify it's state. Using <code>notify</code> in this instance ensures that config or install changes take an immediate effect by also restarting the service. It performs a restart because that is the default action performed on a service when it is notified.</p>\n\n<h1 id=\"usersandgroups\">Users and Groups</h1>\n\n<p>You can use the puppet <code>user</code> resource type to manage local users and groups (whether you should though is worth <a href=\"https://jumpcloud.com/blog/why-user-management-in-chef-and-puppet-is-a-mistake/\">considering</a>). You can't use this to manage domain users, but you can define domain users to be members of local groups.</p>\n\n<p>The following will create a group named 'Avengers' and a user named 'TonyStark':  </p>\n\n<pre><code>group { 'Avengers':  \n  ensure =&gt; present,\n}\n\nuser { 'TonyStark':  \n  ensure =&gt; present,\n  password =&gt; 'password123',\n  groups =&gt; ['Avengers'],\n}\n</code></pre>\n\n<p>Depending on your local security policy you will need to specify a password of a certain complexity.</p>\n\n<p>Under Unix you can specifiy <code>gid</code> and <code>uid</code> properties respectively to ensure that the unique ID for users and groups are consistent across systems. You can't do this on Windows, these properties are visible via <code>puppet resource user &lt;name&gt;</code> but are read-only.</p>\n\n<h1 id=\"scheduledtasks\">Scheduled Tasks</h1>\n\n<p>The <code>scheduled_task</code> resource can be used to create Windows Scheduled Tasks. This is the equivalent of <code>cron</code> in Unix. The book suggests using <code>cron</code>  to set up a routine task to apply your puppet manifests. This is how you might use Puppet in production to ensure your systems remain compliant on a routine basis. This isn't strictly necessary in a Windows environment as by default Puppet Agent runs as a service and applies the manifests every 30 minutes.</p>\n\n<blockquote>\n  <p>The Puppet 5 Beginners book also suggests that (for a master-less setup) you store your puppet configuration in a source control system like Github and then as part of this task pull the latest changes from the repository  with Git before running <code>puppet apply</code>.</p>\n</blockquote>\n\n<p>If you didn't want to utilise the Windows service, below is how you can use Puppet to create a scheduled task to routinely apply any puppet configuration files that exist in the folder <code>C:\\Puppet</code>:</p>\n\n<pre><code>scheduled_task { 'Run Puppet Every 5 Minutes':  \n  ensure    =&gt; present,\n  enabled   =&gt; true,\n  command   =&gt; 'C:/Program Files/Puppet Labs/Puppet/bin/puppet.bat',\n  arguments =&gt; 'apply C:/puppet',\n  working_dir =&gt; 'C:/Program Files/Puppet Labs/Puppet/bin/',\n  trigger   =&gt; {\n    schedule   =&gt; daily,\n    start_time =&gt; '08:00',\n    minutes_interval =&gt; 5,\n    minutes_duration =&gt; 60,\n  }\n}\n</code></pre>\n\n<p>You may also need to configure this scheduled task to run as a user with elevated permissions, particularly if your Puppet manifests are managing packages.</p>\n\n<h1 id=\"executingscripts\">Executing Scripts</h1>\n\n<p>The <code>Exec</code> resource allows you to execute a command or script. Here is an example:  </p>\n\n<pre><code>exec { 'do-something':  \n  command =&gt; 'C:/something.exe'\n}\n</code></pre>\n\n<p>This is a little different to the other resources which model some concrete state of the system, while <code>exec</code> can run any arbitrary command or script. This might change the state of something or it might not. </p>\n\n<p>As a result Puppet (ideally) needs a way to know whether it should trigger the <code>exec</code>. This is done via either a <code>creates</code>, <code>onlyif</code> or <code>unless</code> attribute.</p>\n\n<ul>\n<li><code>creates =&gt;</code> specifies a path or file that should exist after the <code>exec</code> has completed successfully. If this exists then it acts as a flag to Puppet that it doesn't need to be run again unless it does not exist.</li>\n<li><code>onlyif =&gt;</code> specifies a command for puppet to run and the exec is executed if it returns a success (zero) exit status.</li>\n<li><code>unless =&gt;</code> is the opposite of <code>onlyif</code>. The command specified causes the <code>exec</code> to be triggered unless the command specified returns a success (zero) exit status.</li>\n</ul>\n\n<p>Without one of these conditions, the <code>exec</code> will run on every run of the puppet file which is generally undesirable.</p>\n\n<p>Another way to control an <code>exec</code> resource is to add a <code>refreshonly =&gt; true</code> attribute. When this is present the <code>exec</code> will only execute if another resource triggers it via the <code>notify</code> attribute.</p>\n\n<p>Because it is difficult for Puppet to know the state of things changed by <code>exec</code> you should use it with care and consider whether creating a custom resource type might be better.</p>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>Hopefully this guide has given you a useful Windows-centric starting point for Puppet. Beware the above is only a partial summary of what is covered in just the first four chapters of the <a href=\"https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&amp;tag=exsite0a-21&amp;camp=1634&amp;creative=6738&amp;linkCode=as2&amp;creativeASIN=178847290X&amp;linkId=17c0eba83d12b8e7b213b6899d3b5207\">Puppet 5 Beginners Guide (Third Edition)</a> book, so again I strongly recommend you pick it up, even if your focus is on Windows.</p>\n\n<p>Here are some further links specific to Puppet on Windows that you might find useful:</p>\n\n<ul>\n<li><a href=\"https://puppet.com/docs/pe/2017.2/trouble_windows.html\">Troubleshooting Puppet on Windows</a></li>\n<li><a href=\"http://codebetter.com/robreynolds/2014/12/03/how-to-avoid-common-windows-gotchas-with-puppet/\">Avoiding Common Windows Gotchas with Puppet</a></li>\n<li><a href=\"http://cavaliercoder.com/blog/developing-puppet-modules-on-windows.html\">Developing Puppet Modules on Windows</a></li>\n<li><a href=\"https://github.com/puppetlabs/puppetlabs-windows\">The Puppet on Windows Module Pack</a></li>\n</ul>",
                        "image": "/content/images/2018/01/wallpaper-puppet-in-the-rain-2.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-01-18 13:45:36",
                        "created_by": 1,
                        "updated_at": "2018-01-29 15:55:45",
                        "updated_by": 1,
                        "published_at": "2018-01-24 12:37:35",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 65,
                        "uuid": "9585b9b4-2cb3-48da-aab8-a85d68d28359",
                        "title": "Puppet Variables, Expressions, Facts and Hiera on Windows",
                        "slug": "puppet-variables-expressions-facts-and-hiera-on-windows",
                        "markdown": "This post is a continuation of my earlier [Getting Started with Puppet on Windows](http://wragg.io/getting-started-with-puppet-on-windows/) post (although most of the information in this post is OS agnostic). This post explores how you can make your Puppet manifests more dynamic via the typical programming constructs of variables, expressions, conditions and iteration. It also covers Puppet's `Facter` tool (for simplifying the interrogation of system info) and `Hiera` mechanism (for separating configuration data from your code).\n\n> As with my previous post, my primary source of information was the excellent [Puppet 5 Beginners Guide (Third Edition)](https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&tag=exsite0a-21&camp=1634&creative=6738&linkCode=as2&creativeASIN=178847290X&linkId=17c0eba83d12b8e7b213b6899d3b5207) book by [John Arundel](https://twitter.com/bitfield) which I highly recommend.\n\n# Variables, Arrays and Hashtables\n\nIn Puppet variables start with a `$` followed by a name that **must begin with a lowercase letter or an underscore**. The rest of the name can contain uppercase or numbers.\n\nBoolean values `true` and `false` are set by providing them as bare values (e.g not quoted). Note that they are not interchangable with their string equivalents (`$something = true` is not exactly the same as `$something = 'true'`).\n\nYou can include a variable within a string via string interpolation. It works like this:\n```\n$my_name = 'Mark'\nnotice(\"Hello, ${my_name}! Nice to meet you.\")\n```\n> The `notice` function in a puppet manifest outputs an information message to the console. It can be a useful way to troubleshoot manifests as you can use it to see the contents of a variable at a given point.\n\nArrays are defined and accessed via square brackets, and the first item index starts at `0`:\n```\n$array = [1,2,3,4,5]\n$firsttime = $array[0]\n```\nYou can provide array input to titles as an easy way to trigger iteration. For example:\n```\n$software = [\n  'mysql',\n  'atom',\n  'sysinternals',\n]\npackage { $software:\n  ensure   => installed,\n  provider => chocolatey,\n}\n```\n\nHashtables are defined and accessed like this:\n\n```\n$scores = {\n  'mark'  => 220,\n  'steve' => 180,\n  'bob'   => 30,\n}\nnotice(\"Mark's score is ${scores['mark']}.\")\n```\n\nYou can do splatting of attributes via the **attribute splat operator** which is a `*`:\n```\n$attributes = {\n  'owner' => 'administrator',\n  'group' => 'Administrators',\n  'mode'  => '0644',\n}\n\nfile { 'c:/temp/hello.txt':\n  ensure => present,\n  *      => $attributes,\n}\n```\n\n#Expressions\n\nPuppet supports all the usual arithmetic operators, e.g `+ - * and /` and you can use brackets to specify order of operations:\n```\n$value = (10 * 4) + (12 / 3) - 1\nnotice($value)\n```\nComparison operators are as follows:\n\n- `<` less than, e.g `9 < 10`\n- `>` greater than, e.g `11 > 10`\n- `>=` greater than or equal to\n- `<=` less than or equal to\n- `==` is equal to e.g `'foo' == 'foo'`\n- `in` which can be used in several ways e.g\n  - `'foo' in 'foobar'` substring within a string\n  - `'foo' in ['foo','bar']` item within an array\n  - `'foo' in { 'foo' => 'bar' }` key within a hashtable\n- `=~` matches a regular expression, e.g `'foo' =~ /oo/` or a specified type, e.g `'foo' =~ String`\n- `1 != 2` not equal\n\n# If and Case statements\nThe structure for an `if..else` statement is as follows:\n```\nif $something {\n   ..do something..\n} else {\n   ..do something else..\n}\n```\nIf you need to decide between more than two options, you can use `case`:\n```\ncase $something {\n  'thing1': {\n     ..do thing 1..\n  }\n  'thing2': {\n     ..do thing 2..\n  }\n  'thing3': {\n     ..do thing 3..\n  }\n  default: {\n     ..do default thing..\n  }\n}\n```\n\n# Iteration\n\nPuppet include an `each` function for performing iteration. This takes an array input and applies a block of code to each element of the array. For example:\n```\n$files = ['file1','file2','file3']\n\n$files.each | $file | {\n  file { \"c:/temp/${file}.txt\":\n    ensure  => present,\n    content => \"I am ${file}\",\n  }\n}\n```\nYou can see that each item in the array is sent to the `$file` variable when we can then use within the code block to represent the current item. Similarly you can iterate over hashtables by providing two parameters. For more information on iteration [see here](https://puppet.com/docs/puppet/5.3/lang_iteration.html).\n\n# Facts\n\nYou often need to know something about the system Puppet is running on to make a decision or to appropriately configure an attribute. Puppet gives you easy access to this sort of information via the `$facts` hashtable variable. For example you can access the 'kernel' fact via \n```\n$facts['kernel']\n```\nWhich will return `windows` on a windows machine.\n\nFrom the command-line you can use the `facter` command to explore the facts. Running it on its own will return a series of hashtables of all the facts on the system. Note that there are often nested hashtables:\n\n![](/content/images/2018/01/Puppet-facter.png)\n\nYou can provide a specific fact name to return just that config, e.g `facter os` or if you want to access sub properties, use a dot as a separator. e.g:\n```\nfacter memory.system.available\n```\nTo access sub properties within your manifest via the $facts variable you specify multiple key names like this:\n```\n$facts['os']['release']['major']\n```\nYou can extend the `$facts` hashtable by adding [external facts](https://puppet.com/docs/facter/3.9/custom_facts.html). The simplest way to do this is by adding one or more text files to the `\\facts.d` directory. On Windows that can be found here:\n```\nC:\\ProgramData\\PuppetLabs\\facter\\facts.d\n```\nFor example, if we create a text file named `datacenter.txt` in that directory with the following content:\n```\ndatacenter=NewYork\n```\n> You may need admin rights to save to the `facts.d` folder by default.\n\nThen your fact will be immediately available to `facter` and via the `$facts` variable:\n\n![](/content/images/2018/01/Puppet-Facter-Custom-Fact.png)\n\nYou can put multiple facts in a single text file and/or have multiple files. Puppet will simply read all the files in that directory and extract all the `key=value` pairs. You can also create more complex facts using the YAML or JSON structured data formats.\n\nYou can also create **executable facts** where Puppet executes a script to retrieve the fact. This can be via a cmd or PowerShell script, or by using Ruby. Beware that per [this article](- https://puppet.com/blog/starting-out-writing-custom-facts-windows), Ruby is generally the best option from a performance perspective as there is less overheard vs using PowerShell. Another consideration is whether the executable fact needs to be regenerated every time Puppet runs. If it can instead be generated less frequently, consider having a separate scheduled task run the script and write the result to a text file as a custom fact.\n\n# Hiera\n\n[Hiera](https://puppet.com/docs/puppet/5.3/hiera_intro.html) is Puppet's mechanism for separating your configuration data from your code. This is desirable because once your Puppet manifests get moderately complex maintaining your settings as they evolve over time can be time consuming and difficult. Hiera aims to solve this by acting as a centralised database where Puppet can instead look up and retrieve your configuration settings. \n\nHiera allows you to store your configuration data sources as [YAML](https://stackoverflow.com/questions/6968366/if-yaml-aint-markup-language-what-is-it), JSON or HOCON text files. You then query the database from your manifest using the `lookup()` function, which you pass the name of the key you want to retrieve and the data type you expect the result to be. For example:\n```\nfile { lookup('temp_dir', String):\n  ensure => directory,\n}\n```\n> Specifying the data type in the lookup is optional, but is good practice as it can help catch mistakes, such as if you had looked up the wrong key which then returned a different data type.\n\nHiera looks up data by following a heirarchy - an ordered list of your data sources. You configure hierarchies in `hiera.yaml`. Here is an example:\n```\n---\nversion: 5\n\ndefaults:\n  datadir: data\n  data_hash: yaml_data\n\nhierarchy:\n  - name: \"Host data\"\n    path: \"nodes/%{facts.hostname}.yaml\"\n  - name: \"Common defaults\"\n    path: \"common.yaml\"\n```\nPuppet will check for these (under the folder defined by `datadir:` which is `\\data` by default) in order to find a match for the item being looked up. You can also see in the example above that you can interpolate variables such as those in $facts in to these paths to make this list more dynamic.\n\nA YAML hiera data file might look like this:\n```\n---\nmonitoring_server: '1.2.3.4'\nmonitoring_ips:\n  - '10.20.30.40'\n  - '10.20.30.41'\n  - '10.20.30.42'\n  - '10.20.30.43'\nserver_config:\n  updates_enabled: false\n  dhcp_enabled: true\n```\nThis example demonstrates three different data structures as defined in YAML: \n\n- `monitoring_server` is an example of a single value\n- `monitoring_ips` is an example of an array of values\n- `server_config` is an example of a hash with two key/value pairs. \n\nWhen these are retrieved via `lookup()` these will be converted to the equivalent puppet types.\n\nYou can also interpolate Hiera data within the Hiera data itself. For example:\n```\nips:\n  home: '1.2.3.4'\n  office: '10.20.30.40'\nrdp_allowed_ips:\n  - \"{lookup('ips.home')}\"\n  - \"{lookup('ips.office')}\"\n```\nDoing this reduces the number of places an item of data needs to be declared within the Hiera file, reducing the number of places it might need to be updated in the future. Beware however that when doing this via lookup the result is always a string. If you need to interpolate a hash, array or boolean value you need to use the `alias()` function instead, like this:\n```\nping _allowed_ips: \"%{alias('rdp_allowed_ips')}\"\n``` \n\nFinally because `%` is a special character, you need to use the `literal()` function if you want to include it literally. For example to write `%{HTTP_HOST}` as Hiera datta you need to do:\n```\n%{literal('%')}{HTTP_HOST}\n```\n\n# Summary\n\nThis was just a brief introduction to these topics to show how you can include programmatic features in your Manifests to make them dynamic as well as how you can start to utilise system data via Facts and separate and centralise your system specific settings away from your resource declarations via Hiera. \n\nIf you'd like to learn more about these topics, have a look at these official documentation pages:\n\n- [Puppet Variables](https://puppet.com/docs/puppet/5.3/lang_variables.html)\n- [Puppet Expressions](https://puppet.com/docs/puppet/5.3/lang_expressions.html)\n- [Puppet Facts](https://puppet.com/docs/puppet/5.3/lang_facts_and_builtin_vars.html)\n- [Puppet Hiera](https://puppet.com/docs/puppet/5.3/hiera_intro.html)\n\n",
                        "html": "<p>This post is a continuation of my earlier <a href=\"http://wragg.io/getting-started-with-puppet-on-windows/\">Getting Started with Puppet on Windows</a> post (although most of the information in this post is OS agnostic). This post explores how you can make your Puppet manifests more dynamic via the typical programming constructs of variables, expressions, conditions and iteration. It also covers Puppet's <code>Facter</code> tool (for simplifying the interrogation of system info) and <code>Hiera</code> mechanism (for separating configuration data from your code).</p>\n\n<blockquote>\n  <p>As with my previous post, my primary source of information was the excellent <a href=\"https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&amp;tag=exsite0a-21&amp;camp=1634&amp;creative=6738&amp;linkCode=as2&amp;creativeASIN=178847290X&amp;linkId=17c0eba83d12b8e7b213b6899d3b5207\">Puppet 5 Beginners Guide (Third Edition)</a> book by <a href=\"https://twitter.com/bitfield\">John Arundel</a> which I highly recommend.</p>\n</blockquote>\n\n<h1 id=\"variablesarraysandhashtables\">Variables, Arrays and Hashtables</h1>\n\n<p>In Puppet variables start with a <code>$</code> followed by a name that <strong>must begin with a lowercase letter or an underscore</strong>. The rest of the name can contain uppercase or numbers.</p>\n\n<p>Boolean values <code>true</code> and <code>false</code> are set by providing them as bare values (e.g not quoted). Note that they are not interchangable with their string equivalents (<code>$something = true</code> is not exactly the same as <code>$something = 'true'</code>).</p>\n\n<p>You can include a variable within a string via string interpolation. It works like this:  </p>\n\n<pre><code>$my_name = 'Mark'\nnotice(\"Hello, ${my_name}! Nice to meet you.\")  \n</code></pre>\n\n<blockquote>\n  <p>The <code>notice</code> function in a puppet manifest outputs an information message to the console. It can be a useful way to troubleshoot manifests as you can use it to see the contents of a variable at a given point.</p>\n</blockquote>\n\n<p>Arrays are defined and accessed via square brackets, and the first item index starts at <code>0</code>:  </p>\n\n<pre><code>$array = [1,2,3,4,5]\n$firsttime = $array[0]\n</code></pre>\n\n<p>You can provide array input to titles as an easy way to trigger iteration. For example:  </p>\n\n<pre><code>$software = [\n  'mysql',\n  'atom',\n  'sysinternals',\n]\npackage { $software:  \n  ensure   =&gt; installed,\n  provider =&gt; chocolatey,\n}\n</code></pre>\n\n<p>Hashtables are defined and accessed like this:</p>\n\n<pre><code>$scores = {\n  'mark'  =&gt; 220,\n  'steve' =&gt; 180,\n  'bob'   =&gt; 30,\n}\nnotice(\"Mark's score is ${scores['mark']}.\")  \n</code></pre>\n\n<p>You can do splatting of attributes via the <strong>attribute splat operator</strong> which is a <code>*</code>:  </p>\n\n<pre><code>$attributes = {\n  'owner' =&gt; 'administrator',\n  'group' =&gt; 'Administrators',\n  'mode'  =&gt; '0644',\n}\n\nfile { 'c:/temp/hello.txt':  \n  ensure =&gt; present,\n  *      =&gt; $attributes,\n}\n</code></pre>\n\n<h1 id=\"expressions\">Expressions</h1>\n\n<p>Puppet supports all the usual arithmetic operators, e.g <code>+ - * and /</code> and you can use brackets to specify order of operations:  </p>\n\n<pre><code>$value = (10 * 4) + (12 / 3) - 1\nnotice($value)  \n</code></pre>\n\n<p>Comparison operators are as follows:</p>\n\n<ul>\n<li><code>&lt;</code> less than, e.g <code>9 &lt; 10</code></li>\n<li><code>&gt;</code> greater than, e.g <code>11 &gt; 10</code></li>\n<li><code>&gt;=</code> greater than or equal to</li>\n<li><code>&lt;=</code> less than or equal to</li>\n<li><code>==</code> is equal to e.g <code>'foo' == 'foo'</code></li>\n<li><code>in</code> which can be used in several ways e.g\n<ul><li><code>'foo' in 'foobar'</code> substring within a string</li>\n<li><code>'foo' in ['foo','bar']</code> item within an array</li>\n<li><code>'foo' in { 'foo' =&gt; 'bar' }</code> key within a hashtable</li></ul></li>\n<li><code>=~</code> matches a regular expression, e.g <code>'foo' =~ /oo/</code> or a specified type, e.g <code>'foo' =~ String</code></li>\n<li><code>1 != 2</code> not equal</li>\n</ul>\n\n<h1 id=\"ifandcasestatements\">If and Case statements</h1>\n\n<p>The structure for an <code>if..else</code> statement is as follows:  </p>\n\n<pre><code>if $something {  \n   ..do something..\n} else {\n   ..do something else..\n}\n</code></pre>\n\n<p>If you need to decide between more than two options, you can use <code>case</code>:  </p>\n\n<pre><code>case $something {  \n  'thing1': {\n     ..do thing 1..\n  }\n  'thing2': {\n     ..do thing 2..\n  }\n  'thing3': {\n     ..do thing 3..\n  }\n  default: {\n     ..do default thing..\n  }\n}\n</code></pre>\n\n<h1 id=\"iteration\">Iteration</h1>\n\n<p>Puppet include an <code>each</code> function for performing iteration. This takes an array input and applies a block of code to each element of the array. For example:  </p>\n\n<pre><code>$files = ['file1','file2','file3']\n\n$files.each | $file | {\n  file { \"c:/temp/${file}.txt\":\n    ensure  =&gt; present,\n    content =&gt; \"I am ${file}\",\n  }\n}\n</code></pre>\n\n<p>You can see that each item in the array is sent to the <code>$file</code> variable when we can then use within the code block to represent the current item. Similarly you can iterate over hashtables by providing two parameters. For more information on iteration <a href=\"https://puppet.com/docs/puppet/5.3/lang_iteration.html\">see here</a>.</p>\n\n<h1 id=\"facts\">Facts</h1>\n\n<p>You often need to know something about the system Puppet is running on to make a decision or to appropriately configure an attribute. Puppet gives you easy access to this sort of information via the <code>$facts</code> hashtable variable. For example you can access the 'kernel' fact via  </p>\n\n<pre><code>$facts['kernel']\n</code></pre>\n\n<p>Which will return <code>windows</code> on a windows machine.</p>\n\n<p>From the command-line you can use the <code>facter</code> command to explore the facts. Running it on its own will return a series of hashtables of all the facts on the system. Note that there are often nested hashtables:</p>\n\n<p><img src=\"/content/images/2018/01/Puppet-facter.png\" alt=\"\" /></p>\n\n<p>You can provide a specific fact name to return just that config, e.g <code>facter os</code> or if you want to access sub properties, use a dot as a separator. e.g:  </p>\n\n<pre><code>facter memory.system.available  \n</code></pre>\n\n<p>To access sub properties within your manifest via the $facts variable you specify multiple key names like this:  </p>\n\n<pre><code>$facts['os']['release']['major']\n</code></pre>\n\n<p>You can extend the <code>$facts</code> hashtable by adding <a href=\"https://puppet.com/docs/facter/3.9/custom_facts.html\">external facts</a>. The simplest way to do this is by adding one or more text files to the <code>\\facts.d</code> directory. On Windows that can be found here:  </p>\n\n<pre><code>C:\\ProgramData\\PuppetLabs\\facter\\facts.d  \n</code></pre>\n\n<p>For example, if we create a text file named <code>datacenter.txt</code> in that directory with the following content:  </p>\n\n<pre><code>datacenter=NewYork  \n</code></pre>\n\n<blockquote>\n  <p>You may need admin rights to save to the <code>facts.d</code> folder by default.</p>\n</blockquote>\n\n<p>Then your fact will be immediately available to <code>facter</code> and via the <code>$facts</code> variable:</p>\n\n<p><img src=\"/content/images/2018/01/Puppet-Facter-Custom-Fact.png\" alt=\"\" /></p>\n\n<p>You can put multiple facts in a single text file and/or have multiple files. Puppet will simply read all the files in that directory and extract all the <code>key=value</code> pairs. You can also create more complex facts using the YAML or JSON structured data formats.</p>\n\n<p>You can also create <strong>executable facts</strong> where Puppet executes a script to retrieve the fact. This can be via a cmd or PowerShell script, or by using Ruby. Beware that per <a href=\"- https://puppet.com/blog/starting-out-writing-custom-facts-windows\">this article</a>, Ruby is generally the best option from a performance perspective as there is less overheard vs using PowerShell. Another consideration is whether the executable fact needs to be regenerated every time Puppet runs. If it can instead be generated less frequently, consider having a separate scheduled task run the script and write the result to a text file as a custom fact.</p>\n\n<h1 id=\"hiera\">Hiera</h1>\n\n<p><a href=\"https://puppet.com/docs/puppet/5.3/hiera_intro.html\">Hiera</a> is Puppet's mechanism for separating your configuration data from your code. This is desirable because once your Puppet manifests get moderately complex maintaining your settings as they evolve over time can be time consuming and difficult. Hiera aims to solve this by acting as a centralised database where Puppet can instead look up and retrieve your configuration settings. </p>\n\n<p>Hiera allows you to store your configuration data sources as <a href=\"https://stackoverflow.com/questions/6968366/if-yaml-aint-markup-language-what-is-it\">YAML</a>, JSON or HOCON text files. You then query the database from your manifest using the <code>lookup()</code> function, which you pass the name of the key you want to retrieve and the data type you expect the result to be. For example:  </p>\n\n<pre><code>file { lookup('temp_dir', String):  \n  ensure =&gt; directory,\n}\n</code></pre>\n\n<blockquote>\n  <p>Specifying the data type in the lookup is optional, but is good practice as it can help catch mistakes, such as if you had looked up the wrong key which then returned a different data type.</p>\n</blockquote>\n\n<p>Hiera looks up data by following a heirarchy - an ordered list of your data sources. You configure hierarchies in <code>hiera.yaml</code>. Here is an example:  </p>\n\n<pre><code>---\nversion: 5\n\ndefaults:  \n  datadir: data\n  data_hash: yaml_data\n\nhierarchy:  \n  - name: \"Host data\"\n    path: \"nodes/%{facts.hostname}.yaml\"\n  - name: \"Common defaults\"\n    path: \"common.yaml\"\n</code></pre>\n\n<p>Puppet will check for these (under the folder defined by <code>datadir:</code> which is <code>\\data</code> by default) in order to find a match for the item being looked up. You can also see in the example above that you can interpolate variables such as those in $facts in to these paths to make this list more dynamic.</p>\n\n<p>A YAML hiera data file might look like this:  </p>\n\n<pre><code>---\nmonitoring_server: '1.2.3.4'  \nmonitoring_ips:  \n  - '10.20.30.40'\n  - '10.20.30.41'\n  - '10.20.30.42'\n  - '10.20.30.43'\nserver_config:  \n  updates_enabled: false\n  dhcp_enabled: true\n</code></pre>\n\n<p>This example demonstrates three different data structures as defined in YAML: </p>\n\n<ul>\n<li><code>monitoring_server</code> is an example of a single value</li>\n<li><code>monitoring_ips</code> is an example of an array of values</li>\n<li><code>server_config</code> is an example of a hash with two key/value pairs. </li>\n</ul>\n\n<p>When these are retrieved via <code>lookup()</code> these will be converted to the equivalent puppet types.</p>\n\n<p>You can also interpolate Hiera data within the Hiera data itself. For example:  </p>\n\n<pre><code>ips:  \n  home: '1.2.3.4'\n  office: '10.20.30.40'\nrdp_allowed_ips:  \n  - \"{lookup('ips.home')}\"\n  - \"{lookup('ips.office')}\"\n</code></pre>\n\n<p>Doing this reduces the number of places an item of data needs to be declared within the Hiera file, reducing the number of places it might need to be updated in the future. Beware however that when doing this via lookup the result is always a string. If you need to interpolate a hash, array or boolean value you need to use the <code>alias()</code> function instead, like this:  </p>\n\n<pre><code>ping _allowed_ips: \"%{alias('rdp_allowed_ips')}\"  \n</code></pre>\n\n<p>Finally because <code>%</code> is a special character, you need to use the <code>literal()</code> function if you want to include it literally. For example to write <code>%{HTTP_HOST}</code> as Hiera datta you need to do:  </p>\n\n<pre><code>%{literal('%')}{HTTP_HOST}\n</code></pre>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>This was just a brief introduction to these topics to show how you can include programmatic features in your Manifests to make them dynamic as well as how you can start to utilise system data via Facts and separate and centralise your system specific settings away from your resource declarations via Hiera. </p>\n\n<p>If you'd like to learn more about these topics, have a look at these official documentation pages:</p>\n\n<ul>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/lang_variables.html\">Puppet Variables</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/lang_expressions.html\">Puppet Expressions</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/lang_facts_and_builtin_vars.html\">Puppet Facts</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/hiera_intro.html\">Puppet Hiera</a></li>\n</ul>",
                        "image": "/content/images/2018/01/Cat-Expressions.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": "A blog post about how to make Puppet manifests more dynamic via variables, expressions, conditions and iteration as well as utilising Facter and Hiera.",
                        "author_id": 1,
                        "created_at": "2018-01-22 15:39:18",
                        "created_by": 1,
                        "updated_at": "2018-01-31 23:27:23",
                        "updated_by": 1,
                        "published_at": "2018-01-31 12:27:49",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 66,
                        "uuid": "5b9ecef4-92fd-404b-8b26-7310d96f8d4b",
                        "title": "Using Puppet Modules, Forge and r10k on Windows",
                        "slug": "using-puppet-modules-forge-and-r10k",
                        "markdown": "This blog post explores the topic of Puppet Modules for packaging and sharing code. It also looks at how you can use Puppet's public module repository [Forge](https://forge.puppet.com/) to find existing modules and how you can use the [r10k](https://github.com/puppetlabs/r10k) module management tool to update, manage and maintain them on your machines.\n\n>This is a continuation of a series of blog posts on Puppet inspired by the [Puppet 5 Beginner's Guide](https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&tag=exsite0a-21&camp=1634&creative=6738&linkCode=as2&creativeASIN=178847290X&linkId=17c0eba83d12b8e7b213b6899d3b5207) by [John Arundel](http://bitfieldconsulting.com/). If you are new to Puppet you might be interested in the previous two posts:\n>\n>- [Getting Started with Puppet on Windows](http://wragg.io/getting-started-with-puppet-on-windows/)\n>- [Puppet Variables, Expressions, Facts and Hiera](http://wragg.io/puppet-variables-expressions-facts-and-hiera-on-windows/)\n\n# What are Modules?\n\nPut simply a module is a way to package a portion of Puppet code to make it shareable and reusable as well as (ultimately) more maintainable.\n\n> *\"Modules are self-contained bundles of code and data. These reusable, shareable units of Puppet code are a basic building block for Puppet.\"*\n\n# Puppet Forge\n\nWhile you could write all of your own Puppet code from scratch, you can save significant time and effort by using public Puppet modules from the [Forge](https://forge.puppet.com/). \n\nThere are modules covering a large number of common software packages. Many of these modules are officially supported and maintained by Puppet. A significant number of others are \"Puppet Approved\" which means they've been checked to ensure they meet certain quality standards.\n\n![Puppet Forge Windows modules, January 2018](/content/images/2018/01/Puppet-Forge-3.png)\n\nIf you previously followed my [Getting Started with Puppet](http://wragg.io/getting-started-with-puppet-on-windows/) post, we already made use of one public module: chocolatey. We did this by downloading the module via the following command:\n```\npuppet module install chocolatey\n```\nAnd then utilised it in our manifests by adding `include chocolatey`. You can also download modules from the Forge as an archive and extract them manually.\n\n# r10k\n\nWhile you can download and install modules from the Forge via the above method, Puppet has a tool named `r10k` which provides a better solution.\n\nManually downloading modules has several disadvantages:\n\n- Your code becomes cluttered with external code\n- You fail to benefit from future bugfixes/features without manually updating the modules\n- Any modifications you might make to the modules diverges them from their origins, making them difficult to maintain in the future\n\n[r10k](https://puppet.com/docs/pe/2017.3/code_management/r10k.html) is a module management tool that eliminates these problems. Instead of downloading modules directly, you create a special text file called a [puppetfile](https://puppet.com/docs/pe/2017.3/code_management/puppetfile.html) with metadata that tells r10k how to manage the contents of your `\\modules` directory.\n\n> **Why is it called r10k?**\n>\n> *\"It’s called r10k because I’m terrible at names. When I started working on r10k I couldn’t come up with a decent name. While trying to come up with something clever, I recalled that Randall Munroe wrote a bot for controlling IRC chatter, and gave it the most generic name he could think of - [Robot 9000](https://en.wikipedia.org/wiki/Robot9000). Since I just needed a name, any name, I decided to go with an equally generic name by incrementing the robot index.\"* - [Adrien Thebo](http://adrienthebo.github.io/resume/) (creator of r10k)\n\n# Installing r10k on Windows\n\nBefore you can use r10k on Windows you need to install it. This involves adding it as a Ruby Gem to the Puppet agents version of Ruby (which is distinct from the system version of Ruby if you happen to have Ruby installed). You can do this with Puppet by using the `Puppet-Gem` provider as follows:\n\n```\npackage { 'r10k':\n  ensure   => installed,\n  provider => puppet_gem,\n}\n```\n\nThis installs r10k in to the `C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin` directory. You can also do a manual install by switching to this directory and running:\n\n```\ngem install r10k\n```\n\nThis directory is not part of your `PATH` environment variable by default, so if you want to be able to access r10k from any directory, you need to also add `C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin` to your system `PATH` variable and then close and reopen your console window for this to take effect. You should now be able to run r10k:\n\n![Puppet r10k on Windows](/content/images/2018/01/r10k-windows.png)\n\nFinally, (on Windows 2012 R2 at least) I found that when using r10k it would throw an SSL error:\n\n```\nERROR    -> SSL_connect returned=1 errno=0 state=error: certificate verify failed\n```\n\nThis seems to be a [known issue](https://github.com/puppetlabs/r10k/issues/454) (you might want to check you're affected by it first before applying this fix). [Glenn Sarti](https://twitter.com/GlennSarti) (a Windows Dev at Puppet) provides a [workaround](https://github.com/glennsarti/dev-tools/blob/master/RubyCerts.ps1) for this (which I've duplicated but slightly modified below) which involves executing the following PowerShell to download a complete SSL bundle from the CURL website (my version permanently adds the `SSL_CERT_FILE` to the system environment variables):\n\n```\n$CACertFile = Join-Path -Path $ENV:AppData -ChildPath 'RubyCACert.pem'\n\nIf (-Not (Test-Path -Path $CACertFile)) {\n  \"Downloading CA Cert bundle..\"\n  [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n  Invoke-WebRequest -Uri 'https://curl.haxx.se/ca/cacert.pem' -UseBasicParsing -OutFile $CACertFile | Out-Null\n}\n\n\"Setting CA Certificate store set to $CACertFile..\"\n$ENV:SSL_CERT_FILE = $CACertFile\n[System.Environment]::SetEnvironmentVariable('SSL_CERT_FILE',$CACertFile, [System.EnvironmentVariableTarget]::Machine)\n```\n\n# Using r10k\n\nYou can now make use of r10k to manage your modules. As an example, lets install [stdlib](https://forge.puppet.com/puppetlabs/stdlib) which is one of the oldest modules in the Forge. \n\n> Stdlib extends Puppet with a a collection of [useful functions, facts, types and providers](https://forge.puppet.com/puppetlabs/stdlib#module-description). For example:\n>\n>- `count` - If called with only an array, counts the number of elements that are not nil or undef. If called with a second argument, counts the number of elements in an array that matches the second argument.\n>- `dirname` - Returns the directory part of a path.\n>- `ensure_packages` is a better alternative to using the `package` resource directly as it stops errors occurring where the same package might be declared in your code more than once. This might particularly might occur where packages are declared within a module.\n>- `file_line` - Ensures that a given line is contained within a file, or can be used to modify or or more lines in place by matching them with a regular expression.\n>- `pry()` is a useful debugging tool. It allows you to pause execution of your manifest at a specified place and enter an interactive debugger where you can run `catalog` to inspect Puppet's catalog, which contains all the resources currently declared in your manifest. When you're finished you type `exit` to resume execution.\n>- .. and [many more](https://forge.puppet.com/puppetlabs/stdlib#module-description).\n\n>Many Puppet modules make heavy use of this standard library and you should always check if a function exists in stdlib before considering rolling your own.\n\nTo use r10k to install stdlib create a new text file named `puppetfile` (with no extension) in your `\\production` environment directory (e.g `C:\\ProgramData\\PuppetLabs\\code\\environments\\production`) and then give it the following content:\n```\nforge 'http://forge.puppetlabs.com'\n\nmod 'puppetlabs/stdlib', '4.24.0'\n```\nThe `forge` line defines the repository that contains the modules (because you can have private/internal ones). The `mod` line defines the module to be installed, with it's name and specific version as two parameters.\n\nChange to this directory if you're not in it and then execute r10k as follows (this uses the optional `--verbose` flag to give more detailed output):\n```\ncd C:\\ProgramData\\PuppetLabs\\code\\environments\\production\\\nr10k puppetfile install --verbose\n```\n\nYou should now see a `stlib` directory under `\\modules`.\n\n# Module dependencies\n\nMany modules are dependent on the presence of other modules and r10k does not manage these dependencies by default. To assist with generating a puppetfile that includes all the dependent modules you can use the `generate-puppetfile` tool.\n\nTo install `generate-puppetfile`, execute the following:\n\n```\ngem install generate-puppetfile\n```\n*-- If you haven't modified your `PATH` variable as suggested earlier, you will need to specify the full path `C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin` to use `gem` and `generate-puppetfile`.*\n\nAlternatively you can use the `puppet_gem` provider to have Puppet install the gem:\n```\npackage { 'generate-puppetfile':\n  ensure   => installed,\n  provider => puppet_gem,\n}\n```\nOnce `generate-puppetfile` is installed you can use it follows:\n\n*-- You can also specify multiple modules in a single command by separating their names with spaces.*\n\n```\ngenerate-puppetfile puppet/archive\n```\n![](/content/images/2018/01/generate-puppetfile-example.png)\n\nHere we can see that the `puppet/archive` module is dependent on `puppetlabs/stdlib` and as such it has output the required puppetfile text to ensure we install both modules.\n\nOnce you have a pre-existing puppetfile, you can also use this tool to generate an updated version of the file, by having it return a version with all the latest available versions for the modules. You do this as follows (assumes you're in the same directory as your puppetfile):\n```\ngenerate-puppetfile -p .\\puppetfile\n```\n\n# Using modules in your manifests\n\nThe capabilities of a module vary dependent on its purpose. Some modules might just add new resource types or providers. Others might also install software that can then be managed by its resource types or providers.\n\nIf you followed along earlier you will now have the `puppet\\archive` module installed along with its dependent `stdlib`. As described earlier, `stdlib` is providing us with various functions. `Archive` provides us with the ability to manage compressed files. As such it doesn't install or modify any software, so having add it to our `\\Modules` directory we can utilise it in our manifests simply by invoking it's resource declaration. For example:\n```\narchive { 'c:/temp/some-archive.zip':\n  ensure => present,\n  extract => true,\n  extract_path => 'c:/temp',\n  creates => 'c:/temp/some-archive',\n  cleanup => false,\n}\n```\n\nThis would extract `c:\\temp\\some-archive.zip`, to `c:\\temp\\` only if the `c:\\temp\\some-archive` folder didn't already exist. The `cleanup => false` setting tells it not to remove the original archive after performing the extract.\n\nIn contrast, the [Puppetlabs/Chocolatey](https://forge.puppet.com/puppetlabs/chocolatey) module has the capability to install Chocolatey. To trigger this installation to occur (once you have added the module to your `\\modules` directory), we add an `include` statement to our manifest:\n```\nInclude Chocolatey\n```\nAfter which we can then manage packages using the `package` resource type with the newly added attribute of `provider => chocolatey`.\n\nTo best understand what capabilities a module adds, have a look at its associated readme section on the Forge. I also recommend you look at the examples section to see whether or not you might need or want to use `Include`.\n\n# Creating your own modules\n\nIn this section we will author a module that could be used to manage the Windows time service (this is just to act as an example, in reality you're likely better off using [this Windows time module](https://forge.puppet.com/ncorrare/windowstime/) that already exists).\n\n>  *\"A good module for some software should not define how you want the software [configured] but provide an API so that the software can be used on multiple platforms without needing to know the intricacies of that platform.\"* - [Getting Started with Puppet Development](http://fullstack-puppet-docs.readthedocs.io/en/latest/puppet_modules.html)\n\nModules have a [standard directory structure](https://puppet.com/docs/puppet/5.3/modules_fundamentals.html#module-layout) to allow Puppet to find the manifest files, templates and any other components. While you can create the directories your module needs manually, you can also use `puppet module generate` to bootstrap the process. This has the added advantage of creating the `metadata.json` file for you via a series of prompts, which is used to describe the module (and is essential if you plan to publish the module to the Forge).\n\nTo create our Windows Time module, perform the following:\n\n- Navigate to your module directory:\n```\ncd C:\\ProgramData\\PuppetLabs\\code\\environments\\production\\modules\n```\n- Generate the module as follows (obviously change my name to your name -- this identifies the author of the module):\n```\npuppet module generate markwragg/windows_ntp\n```\n- Answer the question prompts (you can hit enter to accept some of them with default values).\n- Navigate to `windows_ntp\\manifests` and modify the `init.pp` template to have the following content:\n```\nclass windows_ntp { \n  registry::value { 'NtpServer':\n    key  => 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data => 'time.windows.com,0x9',\n  }\n  service { 'w32time':\n    ensure => running,\n    enable => true,\n  }\n}\n```\nWe can now make use of our module by adding `include windows_ntp` to a manifest file to execute the class. If you'd like to test the module works without adding it to a manifest, you can do the following:\n```\npuppet apply -e 'include windows_ntp'\n```\nYou could now publish  this module to the Puppet Forge. You could also/alternatively publish the module to a Git repository. If you do so (and ensure you also add a git release tag via `git tag`) you can then use `r10k` to install your module by adding the following to your `puppetfile`:\n```\nmod 'windows_ntp',\n  :git => 'https://github.com/yourusername/windows_ntp.git',\n  :tag => '0.1.1'\n```\nChanging the above values as appropriate, and ensuring that you don't forget to include the other modules that this depends on: \n```\nmod 'puppetlabs/stdlib', '4.24.0'\nmod 'puppetlabs-registry', '2.0.1'\n```\n\n# Summary\n\nThis post has been an introduction to Puppet modules and how to find, use, manage and author them. If you'd like to learn more, I suggest the following links:\n\n- [Managing Code with r10k](https://puppet.com/docs/pe/2017.3/code_management/r10k.html)\n- [Module Fundamentals](https://puppet.com/docs/puppet/5.3/modules_fundamentals.html)\n- [Getting Started: Writing modules for Windows](https://puppet.com/docs/pe/2017.3/quick_start_guides/module_writing_windows_getting_started.html)\n- [The Puppet Language Style Guide](https://puppet.com/docs/puppet/5.3/style_guide.html) -- You should refer to this to ensure any modules you author follow the recommended best practices.",
                        "html": "<p>This blog post explores the topic of Puppet Modules for packaging and sharing code. It also looks at how you can use Puppet's public module repository <a href=\"https://forge.puppet.com/\">Forge</a> to find existing modules and how you can use the <a href=\"https://github.com/puppetlabs/r10k\">r10k</a> module management tool to update, manage and maintain them on your machines.</p>\n\n<blockquote>\n  <p>This is a continuation of a series of blog posts on Puppet inspired by the <a href=\"https://www.amazon.co.uk/gp/product/178847290X/ref=as_li_tl?ie=UTF8&amp;tag=exsite0a-21&amp;camp=1634&amp;creative=6738&amp;linkCode=as2&amp;creativeASIN=178847290X&amp;linkId=17c0eba83d12b8e7b213b6899d3b5207\">Puppet 5 Beginner's Guide</a> by <a href=\"http://bitfieldconsulting.com/\">John Arundel</a>. If you are new to Puppet you might be interested in the previous two posts:</p>\n  \n  <ul>\n  <li><a href=\"http://wragg.io/getting-started-with-puppet-on-windows/\">Getting Started with Puppet on Windows</a></li>\n  <li><a href=\"http://wragg.io/puppet-variables-expressions-facts-and-hiera-on-windows/\">Puppet Variables, Expressions, Facts and Hiera</a></li>\n  </ul>\n</blockquote>\n\n<h1 id=\"whataremodules\">What are Modules?</h1>\n\n<p>Put simply a module is a way to package a portion of Puppet code to make it shareable and reusable as well as (ultimately) more maintainable.</p>\n\n<blockquote>\n  <p><em>\"Modules are self-contained bundles of code and data. These reusable, shareable units of Puppet code are a basic building block for Puppet.\"</em></p>\n</blockquote>\n\n<h1 id=\"puppetforge\">Puppet Forge</h1>\n\n<p>While you could write all of your own Puppet code from scratch, you can save significant time and effort by using public Puppet modules from the <a href=\"https://forge.puppet.com/\">Forge</a>. </p>\n\n<p>There are modules covering a large number of common software packages. Many of these modules are officially supported and maintained by Puppet. A significant number of others are \"Puppet Approved\" which means they've been checked to ensure they meet certain quality standards.</p>\n\n<p><img src=\"/content/images/2018/01/Puppet-Forge-3.png\" alt=\"Puppet Forge Windows modules, January 2018\" /></p>\n\n<p>If you previously followed my <a href=\"http://wragg.io/getting-started-with-puppet-on-windows/\">Getting Started with Puppet</a> post, we already made use of one public module: chocolatey. We did this by downloading the module via the following command:  </p>\n\n<pre><code>puppet module install chocolatey  \n</code></pre>\n\n<p>And then utilised it in our manifests by adding <code>include chocolatey</code>. You can also download modules from the Forge as an archive and extract them manually.</p>\n\n<h1 id=\"r10k\">r10k</h1>\n\n<p>While you can download and install modules from the Forge via the above method, Puppet has a tool named <code>r10k</code> which provides a better solution.</p>\n\n<p>Manually downloading modules has several disadvantages:</p>\n\n<ul>\n<li>Your code becomes cluttered with external code</li>\n<li>You fail to benefit from future bugfixes/features without manually updating the modules</li>\n<li>Any modifications you might make to the modules diverges them from their origins, making them difficult to maintain in the future</li>\n</ul>\n\n<p><a href=\"https://puppet.com/docs/pe/2017.3/code_management/r10k.html\">r10k</a> is a module management tool that eliminates these problems. Instead of downloading modules directly, you create a special text file called a <a href=\"https://puppet.com/docs/pe/2017.3/code_management/puppetfile.html\">puppetfile</a> with metadata that tells r10k how to manage the contents of your <code>\\modules</code> directory.</p>\n\n<blockquote>\n  <p><strong>Why is it called r10k?</strong></p>\n  \n  <p><em>\"It’s called r10k because I’m terrible at names. When I started working on r10k I couldn’t come up with a decent name. While trying to come up with something clever, I recalled that Randall Munroe wrote a bot for controlling IRC chatter, and gave it the most generic name he could think of - <a href=\"https://en.wikipedia.org/wiki/Robot9000\">Robot 9000</a>. Since I just needed a name, any name, I decided to go with an equally generic name by incrementing the robot index.\"</em> - <a href=\"http://adrienthebo.github.io/resume/\">Adrien Thebo</a> (creator of r10k)</p>\n</blockquote>\n\n<h1 id=\"installingr10konwindows\">Installing r10k on Windows</h1>\n\n<p>Before you can use r10k on Windows you need to install it. This involves adding it as a Ruby Gem to the Puppet agents version of Ruby (which is distinct from the system version of Ruby if you happen to have Ruby installed). You can do this with Puppet by using the <code>Puppet-Gem</code> provider as follows:</p>\n\n<pre><code>package { 'r10k':  \n  ensure   =&gt; installed,\n  provider =&gt; puppet_gem,\n}\n</code></pre>\n\n<p>This installs r10k in to the <code>C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin</code> directory. You can also do a manual install by switching to this directory and running:</p>\n\n<pre><code>gem install r10k  \n</code></pre>\n\n<p>This directory is not part of your <code>PATH</code> environment variable by default, so if you want to be able to access r10k from any directory, you need to also add <code>C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin</code> to your system <code>PATH</code> variable and then close and reopen your console window for this to take effect. You should now be able to run r10k:</p>\n\n<p><img src=\"/content/images/2018/01/r10k-windows.png\" alt=\"Puppet r10k on Windows\" /></p>\n\n<p>Finally, (on Windows 2012 R2 at least) I found that when using r10k it would throw an SSL error:</p>\n\n<pre><code>ERROR    -&gt; SSL_connect returned=1 errno=0 state=error: certificate verify failed  \n</code></pre>\n\n<p>This seems to be a <a href=\"https://github.com/puppetlabs/r10k/issues/454\">known issue</a> (you might want to check you're affected by it first before applying this fix). <a href=\"https://twitter.com/GlennSarti\">Glenn Sarti</a> (a Windows Dev at Puppet) provides a <a href=\"https://github.com/glennsarti/dev-tools/blob/master/RubyCerts.ps1\">workaround</a> for this (which I've duplicated but slightly modified below) which involves executing the following PowerShell to download a complete SSL bundle from the CURL website (my version permanently adds the <code>SSL_CERT_FILE</code> to the system environment variables):</p>\n\n<pre><code>$CACertFile = Join-Path -Path $ENV:AppData -ChildPath 'RubyCACert.pem'\n\nIf (-Not (Test-Path -Path $CACertFile)) {  \n  \"Downloading CA Cert bundle..\"\n  [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n  Invoke-WebRequest -Uri 'https://curl.haxx.se/ca/cacert.pem' -UseBasicParsing -OutFile $CACertFile | Out-Null\n}\n\n\"Setting CA Certificate store set to $CACertFile..\"\n$ENV:SSL_CERT_FILE = $CACertFile\n[System.Environment]::SetEnvironmentVariable('SSL_CERT_FILE',$CACertFile, [System.EnvironmentVariableTarget]::Machine)\n</code></pre>\n\n<h1 id=\"usingr10k\">Using r10k</h1>\n\n<p>You can now make use of r10k to manage your modules. As an example, lets install <a href=\"https://forge.puppet.com/puppetlabs/stdlib\">stdlib</a> which is one of the oldest modules in the Forge. </p>\n\n<blockquote>\n  <p>Stdlib extends Puppet with a a collection of <a href=\"https://forge.puppet.com/puppetlabs/stdlib#module-description\">useful functions, facts, types and providers</a>. For example:</p>\n  \n  <ul>\n  <li><code>count</code> - If called with only an array, counts the number of elements that are not nil or undef. If called with a second argument, counts the number of elements in an array that matches the second argument.</li>\n  <li><code>dirname</code> - Returns the directory part of a path.</li>\n  <li><code>ensure_packages</code> is a better alternative to using the <code>package</code> resource directly as it stops errors occurring where the same package might be declared in your code more than once. This might particularly might occur where packages are declared within a module.</li>\n  <li><code>file_line</code> - Ensures that a given line is contained within a file, or can be used to modify or or more lines in place by matching them with a regular expression.</li>\n  <li><code>pry()</code> is a useful debugging tool. It allows you to pause execution of your manifest at a specified place and enter an interactive debugger where you can run <code>catalog</code> to inspect Puppet's catalog, which contains all the resources currently declared in your manifest. When you're finished you type <code>exit</code> to resume execution.</li>\n  <li>.. and <a href=\"https://forge.puppet.com/puppetlabs/stdlib#module-description\">many more</a>.</li>\n  </ul>\n  \n  <p>Many Puppet modules make heavy use of this standard library and you should always check if a function exists in stdlib before considering rolling your own.</p>\n</blockquote>\n\n<p>To use r10k to install stdlib create a new text file named <code>puppetfile</code> (with no extension) in your <code>\\production</code> environment directory (e.g <code>C:\\ProgramData\\PuppetLabs\\code\\environments\\production</code>) and then give it the following content:  </p>\n\n<pre><code>forge 'http://forge.puppetlabs.com'\n\nmod 'puppetlabs/stdlib', '4.24.0'  \n</code></pre>\n\n<p>The <code>forge</code> line defines the repository that contains the modules (because you can have private/internal ones). The <code>mod</code> line defines the module to be installed, with it's name and specific version as two parameters.</p>\n\n<p>Change to this directory if you're not in it and then execute r10k as follows (this uses the optional <code>--verbose</code> flag to give more detailed output):  </p>\n\n<pre><code>cd C:\\ProgramData\\PuppetLabs\\code\\environments\\production\\  \nr10k puppetfile install --verbose  \n</code></pre>\n\n<p>You should now see a <code>stlib</code> directory under <code>\\modules</code>.</p>\n\n<h1 id=\"moduledependencies\">Module dependencies</h1>\n\n<p>Many modules are dependent on the presence of other modules and r10k does not manage these dependencies by default. To assist with generating a puppetfile that includes all the dependent modules you can use the <code>generate-puppetfile</code> tool.</p>\n\n<p>To install <code>generate-puppetfile</code>, execute the following:</p>\n\n<pre><code>gem install generate-puppetfile  \n</code></pre>\n\n<p><em>-- If you haven't modified your <code>PATH</code> variable as suggested earlier, you will need to specify the full path <code>C:\\Program Files\\Puppet Labs\\Puppet\\sys\\ruby\\bin</code> to use <code>gem</code> and <code>generate-puppetfile</code>.</em></p>\n\n<p>Alternatively you can use the <code>puppet_gem</code> provider to have Puppet install the gem:  </p>\n\n<pre><code>package { 'generate-puppetfile':  \n  ensure   =&gt; installed,\n  provider =&gt; puppet_gem,\n}\n</code></pre>\n\n<p>Once <code>generate-puppetfile</code> is installed you can use it follows:</p>\n\n<p><em>-- You can also specify multiple modules in a single command by separating their names with spaces.</em></p>\n\n<pre><code>generate-puppetfile puppet/archive  \n</code></pre>\n\n<p><img src=\"/content/images/2018/01/generate-puppetfile-example.png\" alt=\"\" /></p>\n\n<p>Here we can see that the <code>puppet/archive</code> module is dependent on <code>puppetlabs/stdlib</code> and as such it has output the required puppetfile text to ensure we install both modules.</p>\n\n<p>Once you have a pre-existing puppetfile, you can also use this tool to generate an updated version of the file, by having it return a version with all the latest available versions for the modules. You do this as follows (assumes you're in the same directory as your puppetfile):  </p>\n\n<pre><code>generate-puppetfile -p .\\puppetfile  \n</code></pre>\n\n<h1 id=\"usingmodulesinyourmanifests\">Using modules in your manifests</h1>\n\n<p>The capabilities of a module vary dependent on its purpose. Some modules might just add new resource types or providers. Others might also install software that can then be managed by its resource types or providers.</p>\n\n<p>If you followed along earlier you will now have the <code>puppet\\archive</code> module installed along with its dependent <code>stdlib</code>. As described earlier, <code>stdlib</code> is providing us with various functions. <code>Archive</code> provides us with the ability to manage compressed files. As such it doesn't install or modify any software, so having add it to our <code>\\Modules</code> directory we can utilise it in our manifests simply by invoking it's resource declaration. For example:  </p>\n\n<pre><code>archive { 'c:/temp/some-archive.zip':  \n  ensure =&gt; present,\n  extract =&gt; true,\n  extract_path =&gt; 'c:/temp',\n  creates =&gt; 'c:/temp/some-archive',\n  cleanup =&gt; false,\n}\n</code></pre>\n\n<p>This would extract <code>c:\\temp\\some-archive.zip</code>, to <code>c:\\temp\\</code> only if the <code>c:\\temp\\some-archive</code> folder didn't already exist. The <code>cleanup =&gt; false</code> setting tells it not to remove the original archive after performing the extract.</p>\n\n<p>In contrast, the <a href=\"https://forge.puppet.com/puppetlabs/chocolatey\">Puppetlabs/Chocolatey</a> module has the capability to install Chocolatey. To trigger this installation to occur (once you have added the module to your <code>\\modules</code> directory), we add an <code>include</code> statement to our manifest:  </p>\n\n<pre><code>Include Chocolatey  \n</code></pre>\n\n<p>After which we can then manage packages using the <code>package</code> resource type with the newly added attribute of <code>provider =&gt; chocolatey</code>.</p>\n\n<p>To best understand what capabilities a module adds, have a look at its associated readme section on the Forge. I also recommend you look at the examples section to see whether or not you might need or want to use <code>Include</code>.</p>\n\n<h1 id=\"creatingyourownmodules\">Creating your own modules</h1>\n\n<p>In this section we will author a module that could be used to manage the Windows time service (this is just to act as an example, in reality you're likely better off using <a href=\"https://forge.puppet.com/ncorrare/windowstime/\">this Windows time module</a> that already exists).</p>\n\n<blockquote>\n  <p><em>\"A good module for some software should not define how you want the software [configured] but provide an API so that the software can be used on multiple platforms without needing to know the intricacies of that platform.\"</em> - <a href=\"http://fullstack-puppet-docs.readthedocs.io/en/latest/puppet_modules.html\">Getting Started with Puppet Development</a></p>\n</blockquote>\n\n<p>Modules have a <a href=\"https://puppet.com/docs/puppet/5.3/modules_fundamentals.html#module-layout\">standard directory structure</a> to allow Puppet to find the manifest files, templates and any other components. While you can create the directories your module needs manually, you can also use <code>puppet module generate</code> to bootstrap the process. This has the added advantage of creating the <code>metadata.json</code> file for you via a series of prompts, which is used to describe the module (and is essential if you plan to publish the module to the Forge).</p>\n\n<p>To create our Windows Time module, perform the following:</p>\n\n<ul>\n<li>Navigate to your module directory:</li>\n</ul>\n\n<pre><code>cd C:\\ProgramData\\PuppetLabs\\code\\environments\\production\\modules  \n</code></pre>\n\n<ul>\n<li>Generate the module as follows (obviously change my name to your name -- this identifies the author of the module):</li>\n</ul>\n\n<pre><code>puppet module generate markwragg/windows_ntp  \n</code></pre>\n\n<ul>\n<li>Answer the question prompts (you can hit enter to accept some of them with default values).</li>\n<li>Navigate to <code>windows_ntp\\manifests</code> and modify the <code>init.pp</code> template to have the following content:</li>\n</ul>\n\n<pre><code>class windows_ntp {  \n  registry::value { 'NtpServer':\n    key  =&gt; 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data =&gt; 'time.windows.com,0x9',\n  }\n  service { 'w32time':\n    ensure =&gt; running,\n    enable =&gt; true,\n  }\n}\n</code></pre>\n\n<p>We can now make use of our module by adding <code>include windows_ntp</code> to a manifest file to execute the class. If you'd like to test the module works without adding it to a manifest, you can do the following:  </p>\n\n<pre><code>puppet apply -e 'include windows_ntp'  \n</code></pre>\n\n<p>You could now publish  this module to the Puppet Forge. You could also/alternatively publish the module to a Git repository. If you do so (and ensure you also add a git release tag via <code>git tag</code>) you can then use <code>r10k</code> to install your module by adding the following to your <code>puppetfile</code>:  </p>\n\n<pre><code>mod 'windows_ntp',  \n  :git =&gt; 'https://github.com/yourusername/windows_ntp.git',\n  :tag =&gt; '0.1.1'\n</code></pre>\n\n<p>Changing the above values as appropriate, and ensuring that you don't forget to include the other modules that this depends on:  </p>\n\n<pre><code>mod 'puppetlabs/stdlib', '4.24.0'  \nmod 'puppetlabs-registry', '2.0.1'  \n</code></pre>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>This post has been an introduction to Puppet modules and how to find, use, manage and author them. If you'd like to learn more, I suggest the following links:</p>\n\n<ul>\n<li><a href=\"https://puppet.com/docs/pe/2017.3/code_management/r10k.html\">Managing Code with r10k</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/modules_fundamentals.html\">Module Fundamentals</a></li>\n<li><a href=\"https://puppet.com/docs/pe/2017.3/quick_start_guides/module_writing_windows_getting_started.html\">Getting Started: Writing modules for Windows</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/style_guide.html\">The Puppet Language Style Guide</a> -- You should refer to this to ensure any modules you author follow the recommended best practices.</li>\n</ul>",
                        "image": "/content/images/2018/01/Lego-Brick-Yellow-L.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Using Puppet Modules, Forge and r10k on Windows",
                        "meta_description": "A blog post about using Puppet Modules on Windows. How to find and install them from the Forge. How to install r10k on Windows to perform module management.",
                        "author_id": 1,
                        "created_at": "2018-01-24 10:59:25",
                        "created_by": 1,
                        "updated_at": "2018-02-07 12:11:58",
                        "updated_by": 1,
                        "published_at": "2018-02-07 12:11:58",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 67,
                        "uuid": "2984cd6a-3a7e-4cda-91b8-823c890695bf",
                        "title": "Using Puppet Classes, Roles, Profiles and Templates",
                        "slug": "puppet-classes-roles-profiles-and-templates-on-windows",
                        "markdown": "This post explores the Puppet topics of Classes, Roles and Profiles as well as how to utilise Puppet Template files. The `class` keyword is a key Puppet building block. There are various different ways to utilise the `class` keyword and these are explored below.\n\nThis is a continuation of a series of posts on Puppet, with a focus on its use with Windows. If you are new Puppet, I suggest reviewing the following posts first:\n\n- [Getting started with Puppet on Windows](http://wragg.io/getting-started-with-puppet-on-windows/)\n- [Puppet variables, expressions, facts and hiera](http://wragg.io/puppet-variables-expressions-facts-and-hiera-on-windows/)\n- [Using Puppet modules, forge and r10k](http://wragg.io/using-puppet-modules-forge-and-r10k/)\n\n# Classes\n\n> *\"Classes are named blocks of Puppet code that are stored in modules for later use and are not applied until they are invoked by name. Classes generally configure large or medium-sized chunks of functionality, such as all of the packages, config files, and services needed to run an application.\"* - [Classes](https://puppet.com/docs/puppet/5.3/lang_classes.html)\n\nWhen you use the `class` keyword you're informing Puppet that a specified collection of resources should be grouped together and given a name, but that these resources shouldn't be applied (yet). You can then use the `include` keyword in a manifest to declare the class and have the resources executed. In my Puppet Modules post, we create a `windows_ntp` class, which grouped together a `registry` resource and a `service` resource to configure the Windows Time service:\n\n```\nclass windows_ntp { \n  registry::value { 'NtpServer':\n    key  => 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data => 'time.windows.com,0x9',\n  }\n  service { 'w32time':\n    ensure => running,\n    enable => true,\n  }\n}\n```\n\nWe could then execute this class from a manifest by declaring `include windows_ntp`.\n\n# Class Parameters\n\nWhile the above is already useful, we can make this class more powerful by including parameters, which allow us to pass input data to change how it's applied. For example, we could change our `windows_ntp` class to accept the time server address as a parameter named `$server`:\n\n```\nclass windows_ntp (\n  String $server = 'time.windows.com',\n) { \n  registry::value { 'NtpServer':\n    key  => 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data => \"${server},0x9\",\n  }\n  service { 'w32time':\n    ensure => running,\n    enable => true,\n  }\n}\n```\nWe declare this class via `include windows_ntp` Puppet will automatically check the Hiera data for any matching settings for the class. For example we could have the following line in Hiera:\n```\nwindows_ntp::server: '0.europe.pool.ntp.org'\n```\nAnd this setting would override the default. If no setting was found, the `time.windows.com` default would apply.\n\n> You can probably already see how this is pretty powerful behaviour. We can have a standard class with sensible defaults, but create (for example) regional settings via Hiera that would be applied to different sets of servers as required.\n\n# Parameter Types\n\nIn the above example we added a single `string` type parameter to our class. We could of course have multiple parameters and there are various different parameter [data types](https://puppet.com/docs/puppet/5.3/lang_data.html) available for use:\n\n- `String` - You can also specify one or two additional parameters to specify the minimum and maximum length of the string, e.g `String[1,5]`.\n- `Boolean` - `true` or `false`.\n- `Integer` - Any whole number.\n- `Float` - Any floating-point number with optional decimal fractions.\n- `Numeric` - Any integer or float.\n- `Array` - An array of values. You can also specify the type of value accepted, e.g `Array[Integer]` would only accept an array of integers.\n- `Hash` - A hash of values. Again, you can specify the accepted types, e.g `Hash[String, Integer]` would only accept a hashtable with string keys and integer values.\n- `Regexp` - Any or a specified regular expression. Beware confusing this with the `Pattern` abstract type.\n- `Undef` - Any variable or parameter that hasn't been assigned a value.\n- `Type` - One or more literal Puppet data types, per these lists.\n\nThere are also [abstract data types](https://puppet.com/docs/puppet/5.3/lang_data_abstract.html). Abstract data types let you do more sophisticated or permissive type checking:\n\n- `Optional` - Wraps one other data type, and results in a data type that matches anything that type would match plus undef. E.g `Optional[String]`.\n- `Pattern` - Matches strings against one or more regular expressions.\n- `Enum` - A specific list of valid values, e.g `Enum['red','yellow','green']`.\n- `Scalar` - Matches any valid values that are `integer`, `float`, `string`, `boolean` or `regexp`. Does **not** match `undef`, `array` or `hash`.\n- `Data` - Matches any value that would match `scalar` as well as `undef`, `array` or `hash`.\n- `Collection` - Any array or hash, regardless of values/keys they contain.\n- `Variant` - Matches a collection of other data types. E.g `Variant[Enum['true', 'false'], Boolean] `.\n- `Any` - Matches any data type.\n\n> It's generally best practice to use the most specific data type possible as a form of input validation.\n\n# Defined Resource Types\n\nSo far we have created classes to group together related resources. These are [singleton](https://en.wikipedia.org/wiki/Singleton_pattern) in nature, which means that **they can be declared only once**.\n\n> *\"Classes are singletons — although a given class can have very different behavior depending on how its parameters are set, the resources in it will only be evaluated once per compilation.\"* - [Classes](https://puppet.com/docs/puppet/5.3/lang_classes.html)\n\nThis might make sense for some classes you create, for example we likely only want to configure the Windows Time settings once on a server. However sometimes you want to be able to declare multiple instances of a class, in the same way as you can declare multiple instances of a specified Puppet resource (with unique titles). To permit this for our classes, we need to create a defined resource type. \n\nDefined resource types are very similar to classes, but we instead use the `define` keyword. Here is an example that could setup a user and a home directory on one of three file servers:\n```\ndefine user_with_homedirectory (\n  Enum[\n    'Server1',\n    'Server2',\n    'Server3',\n  ] $Server,\n) {\n  user { $title:\n    ensure => present,\n  } \n  file { \"//${Server}/${title}/\":\n    ensure => directory,\n    owner => $title\n  }    \n}\n```\nWe could then declare a user in a manifest as follows:\n```\nuser_with_homedirectory { 'mark':\n  server => Server2,\n}\n```\n\n> Note that in the above example we've used the automatic `$title` parameter that is always available in classes and defined resource types to use as the username value. This follows the general pattern you see in other resources. \n\nYou can nest defined resource types inside your singleton classes. By doing so you could enable a piece of software to provide the ability for users to configure multiple instances of a sub-component, e.g a `database_server` class could expose a `some_database` resource definition.\n\n# Type Aliases\n\nIf you need to reuse a specific type definition across multiple classes, or want to give a type a descriptive name, you can use the `Type` keyword to create a new [type alias](https://puppet.com/docs/puppet/5.3/lang_type_aliases.html).  For example:\n\n```\ntype FileServers = Enum['Server1','Server2','Server3']\n\ndefine user_with_homedirectory (\n  FileServers $Server,\n) {\n  ..\n}\n```\nThis is particularly helpful when you have a type that matches a complex pattern (for example an IP Address) which you might need to reuse.\n\nWhen creating types for modules, you should store them in a file named after the type in the `/types` subdirectory of the module. E.g our example above would be stored in `/types/fileservers.pp`.\n\n# Roles and Profiles\n\nThere are no specific keywords for defining [roles and profiles](https://puppet.com/docs/pe/2017.3/managing_nodes/designing_system_configs_roles_and_profiles.html) in Puppet. Rather the concept involves using the `class` keyword to create specific configurations of modules (as profiles) and to then group these profiles for a specific machine purpose (as roles). Together these define how specific servers should be configured. \n\nNote, you don't have to utilise the roles and profiles approach at all if you don't want to, but doing so can help to make your manifests more maintainable over time.\n\n> *\"Your typical goal with Puppet is to build complete system configurations, which manage all of the software, services, and configuration that you care about on a given system. The roles and profiles method can help keep complexity under control and make your code more reusable, reconfigurable, and refactorable.\"* - [Designing system configs: roles and profiles](https://puppet.com/docs/pe/2017.3/managing_nodes/designing_system_configs_roles_and_profiles.html)\n\nUsing Roles and Profiles separates your code into three levels:\n\n1. Component modules -- normal modules managing a particular technology (e.g [MSSQL](https://forge.puppet.com/puppetlabs/mssql)).\n- Profiles -- Wrapper classes that combine multiple modules to configure a layered technology stack (e.g a web server).\n- Roles -- Wrapper classes that combine multiple profiles to configure a complete system (for example our \"web server\" role might also be combined with a \"security\" role that hardens the server and a \"monitoring\" role that configures monitoring).\n\n# Profiles\n\n> *\"From a Puppet perspective, a profile is just a normal class stored in the profile module.\"*\n\nA profile will generally be a `class` that takes a particular module and adds additional code or logic to implement the it appropriately in your environment.\n\nHere is an example (from the official Puppet documentation) of a Jenkins Master profile:\n\n```\nclass profile::jenkins::master (\n  String $jenkins_port = '9091',\n  String $java_dist    = 'jdk',\n  String $java_version = 'latest',\n) {\n\n  class { 'jenkins':\n    configure_firewall => true,\n    install_java       => false,\n    port               => $jenkins_port,\n    config_hash        => {\n      'HTTP_PORT'    => { 'value' => $jenkins_port },\n      'JENKINS_PORT' => { 'value' => $jenkins_port },\n    },\n  }\n\n  class { 'java':\n    distribution => $java_dist,\n    version      => $java_version,\n    before       => Class['jenkins'],\n  }\n}\n```\n\n# Roles\n\n> *\"To write roles, we consider the machines we’ll be managing and decide what else they need..\"*\n\nA role class will generally only include profiles and is used to identify a particular function for a node. Continuing the example above, our Jenkins Master role might be a combination of the profile shown above as well as our (currently imaginary) security hardening and server monitoring profiles that were mentioned earlier. \n\n```\nclass role::jenkins::master {\n  include profile::security\n  include profile::monitoring\n  include profile::jenkins::master\n}\n```\n\n# Templates\n\nTemplates are used to interpolate values of Puppet variables, facts or Hiera data in to files (generally configuration files). \n\nWhile you can simply use the Puppet `file` resource to set the complete contents of a file via the `content =>` attribute or from another source file via the `source =>` attribute these only allow you to set a file based on a static source. Often you need the contents of a file to vary depending on where it is used. You could maintain multiple static source files, but Templates offer a more flexible alternative.\n\nA template is simply a standard text file with special placeholders markers (`<%= .. %>`) to identify where Puppet should inject values. For example:\n```\nsome_config_setting = <%= $some_config_value %>\n```\n\nYou still make use of template via the Puppet `file` resource and the `content =>` attribute. To do so, you make use of the `epp()` function:\n```\nfile { 'c:/windows/system32/drivers/etc/hosts':\n  content => epp('/source/hosts.epp'),\n}\n```\nThe above example simply interpolated a variable value into a template. The template tags can also contain any valid Puppet expression or code. This might make sense for example if we needed to set a configuration setting based on a variation of the machine the Puppet manifest is applied to (such as the amount of memory). For example if we needed some app memory setting to be set to 75% of total RAM:\n```\nsome_memory_setting = <%= $facts['memory']['system']['total_bytes'] * 3/4 %>\n```\nWe can also use Puppet's conditional statements to make certain parts of a template optional:\n```\n<% if $facts['os']['family'] == 'Windows' { -%>\n..some Windows specific config..\n<% } -%>\n```\nAnd iteration to generate a varying number of elements based on some collection:\n```\n<% $facts['networking']['interfaces'].each |String $interface, Hash $attrs| { -%>\n  some_interface_setting = <%= $interface %>\n<% } -%>\n```\n\n# Template Parameters\n\nBecause you can include various variables and Puppet code per the above examples, it can easily become difficult to know which of these a specific template contains. One way to manage this is to add parameter declarations at the top of your template. This is optional and is done using `|` pipe characters as follows:\n```\n<% | String $some_config_setting,\n     String $some_other_setting,\n| -%>\n```\nJust like Classes, parameters can be given default values and any that do not have default values are mandatory.  When you include parameter declarations you must then pass any mandatory parameters in hash form as the second argument of the `epp()` function:\n```\nfile { 'c:/windows/system32/drivers/etc/hosts':\n  content => epp('/source/hosts.epp',\n    {\n       'some_config_setting' => 'something',\n       'some_other_setting'  => 'something else',\n    },\n  ),\n}\n```\nThe values passed need not be literal, you could (for example) use `lookup()` to get Hiera values. In facts its generally best practice to do your Hiera lookups this way vs doing so in the templates directly.\n\n# Debugging Templates\n\nPuppet provides a tool to assist with validating templates:\n```\npuppet epp validate .\\path\\to\\template.epp\n```\nIf there is no output the template is valid. If there is any error in the template, you'll see an error message accordingly:\n\n![](/content/images/2018/02/Puppet-epp-validate.png)\n\nPuppet also provides a tool to generate what the template will look like so you can validate the result is what you expect:\n```\npuppet epp render\n```\nTo use this you must pass it a string that is a hashtable of values for the input variables. For example:\n```\npuppet epp render .\\example.epp --values { some_config_setting => 'something', 'some_other_setting'  => 'something else' }\n```\nReturns:\n![](/content/images/2018/02/Puppet-epp-render.png)\n*-- This .epp contains all the examples above so the other output values have been generated from the `$facts` variable.*\n\n# Summary\n\nThis post has been an exploration of the ways in which to wrap your Puppet code as classes, profiles and roles in order to make it more reusable and how you can use templates to handle configuration files in a more dynamic way. To read more on these topics I recommend the following links:\n\n- [Puppet Classes](https://puppet.com/docs/puppet/5.3/lang_classes.html)\n- [Designing Roles](https://puppet.com/docs/pe/2017.3/managing_nodes/designing_convenient_roles.html)\n- [Roles and Profiles Example](https://puppet.com/docs/pe/2017.2/r_n_p_full_example.html)\n- [Using Templates](https://puppet.com/docs/puppet/5.3/lang_template.html)",
                        "html": "<p>This post explores the Puppet topics of Classes, Roles and Profiles as well as how to utilise Puppet Template files. The <code>class</code> keyword is a key Puppet building block. There are various different ways to utilise the <code>class</code> keyword and these are explored below.</p>\n\n<p>This is a continuation of a series of posts on Puppet, with a focus on its use with Windows. If you are new Puppet, I suggest reviewing the following posts first:</p>\n\n<ul>\n<li><a href=\"http://wragg.io/getting-started-with-puppet-on-windows/\">Getting started with Puppet on Windows</a></li>\n<li><a href=\"http://wragg.io/puppet-variables-expressions-facts-and-hiera-on-windows/\">Puppet variables, expressions, facts and hiera</a></li>\n<li><a href=\"http://wragg.io/using-puppet-modules-forge-and-r10k/\">Using Puppet modules, forge and r10k</a></li>\n</ul>\n\n<h1 id=\"classes\">Classes</h1>\n\n<blockquote>\n  <p><em>\"Classes are named blocks of Puppet code that are stored in modules for later use and are not applied until they are invoked by name. Classes generally configure large or medium-sized chunks of functionality, such as all of the packages, config files, and services needed to run an application.\"</em> - <a href=\"https://puppet.com/docs/puppet/5.3/lang_classes.html\">Classes</a></p>\n</blockquote>\n\n<p>When you use the <code>class</code> keyword you're informing Puppet that a specified collection of resources should be grouped together and given a name, but that these resources shouldn't be applied (yet). You can then use the <code>include</code> keyword in a manifest to declare the class and have the resources executed. In my Puppet Modules post, we create a <code>windows_ntp</code> class, which grouped together a <code>registry</code> resource and a <code>service</code> resource to configure the Windows Time service:</p>\n\n<pre><code>class windows_ntp {  \n  registry::value { 'NtpServer':\n    key  =&gt; 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data =&gt; 'time.windows.com,0x9',\n  }\n  service { 'w32time':\n    ensure =&gt; running,\n    enable =&gt; true,\n  }\n}\n</code></pre>\n\n<p>We could then execute this class from a manifest by declaring <code>include windows_ntp</code>.</p>\n\n<h1 id=\"classparameters\">Class Parameters</h1>\n\n<p>While the above is already useful, we can make this class more powerful by including parameters, which allow us to pass input data to change how it's applied. For example, we could change our <code>windows_ntp</code> class to accept the time server address as a parameter named <code>$server</code>:</p>\n\n<pre><code>class windows_ntp (  \n  String $server = 'time.windows.com',\n) { \n  registry::value { 'NtpServer':\n    key  =&gt; 'HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\W32Time\\Parameters',\n    data =&gt; \"${server},0x9\",\n  }\n  service { 'w32time':\n    ensure =&gt; running,\n    enable =&gt; true,\n  }\n}\n</code></pre>\n\n<p>We declare this class via <code>include windows_ntp</code> Puppet will automatically check the Hiera data for any matching settings for the class. For example we could have the following line in Hiera:  </p>\n\n<pre><code>windows_ntp::server: '0.europe.pool.ntp.org'  \n</code></pre>\n\n<p>And this setting would override the default. If no setting was found, the <code>time.windows.com</code> default would apply.</p>\n\n<blockquote>\n  <p>You can probably already see how this is pretty powerful behaviour. We can have a standard class with sensible defaults, but create (for example) regional settings via Hiera that would be applied to different sets of servers as required.</p>\n</blockquote>\n\n<h1 id=\"parametertypes\">Parameter Types</h1>\n\n<p>In the above example we added a single <code>string</code> type parameter to our class. We could of course have multiple parameters and there are various different parameter <a href=\"https://puppet.com/docs/puppet/5.3/lang_data.html\">data types</a> available for use:</p>\n\n<ul>\n<li><code>String</code> - You can also specify one or two additional parameters to specify the minimum and maximum length of the string, e.g <code>String[1,5]</code>.</li>\n<li><code>Boolean</code> - <code>true</code> or <code>false</code>.</li>\n<li><code>Integer</code> - Any whole number.</li>\n<li><code>Float</code> - Any floating-point number with optional decimal fractions.</li>\n<li><code>Numeric</code> - Any integer or float.</li>\n<li><code>Array</code> - An array of values. You can also specify the type of value accepted, e.g <code>Array[Integer]</code> would only accept an array of integers.</li>\n<li><code>Hash</code> - A hash of values. Again, you can specify the accepted types, e.g <code>Hash[String, Integer]</code> would only accept a hashtable with string keys and integer values.</li>\n<li><code>Regexp</code> - Any or a specified regular expression. Beware confusing this with the <code>Pattern</code> abstract type.</li>\n<li><code>Undef</code> - Any variable or parameter that hasn't been assigned a value.</li>\n<li><code>Type</code> - One or more literal Puppet data types, per these lists.</li>\n</ul>\n\n<p>There are also <a href=\"https://puppet.com/docs/puppet/5.3/lang_data_abstract.html\">abstract data types</a>. Abstract data types let you do more sophisticated or permissive type checking:</p>\n\n<ul>\n<li><code>Optional</code> - Wraps one other data type, and results in a data type that matches anything that type would match plus undef. E.g <code>Optional[String]</code>.</li>\n<li><code>Pattern</code> - Matches strings against one or more regular expressions.</li>\n<li><code>Enum</code> - A specific list of valid values, e.g <code>Enum['red','yellow','green']</code>.</li>\n<li><code>Scalar</code> - Matches any valid values that are <code>integer</code>, <code>float</code>, <code>string</code>, <code>boolean</code> or <code>regexp</code>. Does <strong>not</strong> match <code>undef</code>, <code>array</code> or <code>hash</code>.</li>\n<li><code>Data</code> - Matches any value that would match <code>scalar</code> as well as <code>undef</code>, <code>array</code> or <code>hash</code>.</li>\n<li><code>Collection</code> - Any array or hash, regardless of values/keys they contain.</li>\n<li><code>Variant</code> - Matches a collection of other data types. E.g <code>Variant[Enum['true', 'false'], Boolean]</code>.</li>\n<li><code>Any</code> - Matches any data type.</li>\n</ul>\n\n<blockquote>\n  <p>It's generally best practice to use the most specific data type possible as a form of input validation.</p>\n</blockquote>\n\n<h1 id=\"definedresourcetypes\">Defined Resource Types</h1>\n\n<p>So far we have created classes to group together related resources. These are <a href=\"https://en.wikipedia.org/wiki/Singleton_pattern\">singleton</a> in nature, which means that <strong>they can be declared only once</strong>.</p>\n\n<blockquote>\n  <p><em>\"Classes are singletons — although a given class can have very different behavior depending on how its parameters are set, the resources in it will only be evaluated once per compilation.\"</em> - <a href=\"https://puppet.com/docs/puppet/5.3/lang_classes.html\">Classes</a></p>\n</blockquote>\n\n<p>This might make sense for some classes you create, for example we likely only want to configure the Windows Time settings once on a server. However sometimes you want to be able to declare multiple instances of a class, in the same way as you can declare multiple instances of a specified Puppet resource (with unique titles). To permit this for our classes, we need to create a defined resource type. </p>\n\n<p>Defined resource types are very similar to classes, but we instead use the <code>define</code> keyword. Here is an example that could setup a user and a home directory on one of three file servers:  </p>\n\n<pre><code>define user_with_homedirectory (  \n  Enum[\n    'Server1',\n    'Server2',\n    'Server3',\n  ] $Server,\n) {\n  user { $title:\n    ensure =&gt; present,\n  } \n  file { \"//${Server}/${title}/\":\n    ensure =&gt; directory,\n    owner =&gt; $title\n  }    \n}\n</code></pre>\n\n<p>We could then declare a user in a manifest as follows:  </p>\n\n<pre><code>user_with_homedirectory { 'mark':  \n  server =&gt; Server2,\n}\n</code></pre>\n\n<blockquote>\n  <p>Note that in the above example we've used the automatic <code>$title</code> parameter that is always available in classes and defined resource types to use as the username value. This follows the general pattern you see in other resources. </p>\n</blockquote>\n\n<p>You can nest defined resource types inside your singleton classes. By doing so you could enable a piece of software to provide the ability for users to configure multiple instances of a sub-component, e.g a <code>database_server</code> class could expose a <code>some_database</code> resource definition.</p>\n\n<h1 id=\"typealiases\">Type Aliases</h1>\n\n<p>If you need to reuse a specific type definition across multiple classes, or want to give a type a descriptive name, you can use the <code>Type</code> keyword to create a new <a href=\"https://puppet.com/docs/puppet/5.3/lang_type_aliases.html\">type alias</a>.  For example:</p>\n\n<pre><code>type FileServers = Enum['Server1','Server2','Server3']\n\ndefine user_with_homedirectory (  \n  FileServers $Server,\n) {\n  ..\n}\n</code></pre>\n\n<p>This is particularly helpful when you have a type that matches a complex pattern (for example an IP Address) which you might need to reuse.</p>\n\n<p>When creating types for modules, you should store them in a file named after the type in the <code>/types</code> subdirectory of the module. E.g our example above would be stored in <code>/types/fileservers.pp</code>.</p>\n\n<h1 id=\"rolesandprofiles\">Roles and Profiles</h1>\n\n<p>There are no specific keywords for defining <a href=\"https://puppet.com/docs/pe/2017.3/managing_nodes/designing_system_configs_roles_and_profiles.html\">roles and profiles</a> in Puppet. Rather the concept involves using the <code>class</code> keyword to create specific configurations of modules (as profiles) and to then group these profiles for a specific machine purpose (as roles). Together these define how specific servers should be configured. </p>\n\n<p>Note, you don't have to utilise the roles and profiles approach at all if you don't want to, but doing so can help to make your manifests more maintainable over time.</p>\n\n<blockquote>\n  <p><em>\"Your typical goal with Puppet is to build complete system configurations, which manage all of the software, services, and configuration that you care about on a given system. The roles and profiles method can help keep complexity under control and make your code more reusable, reconfigurable, and refactorable.\"</em> - <a href=\"https://puppet.com/docs/pe/2017.3/managing_nodes/designing_system_configs_roles_and_profiles.html\">Designing system configs: roles and profiles</a></p>\n</blockquote>\n\n<p>Using Roles and Profiles separates your code into three levels:</p>\n\n<ol>\n<li>Component modules -- normal modules managing a particular technology (e.g <a href=\"https://forge.puppet.com/puppetlabs/mssql\">MSSQL</a>).  </li>\n<li>Profiles -- Wrapper classes that combine multiple modules to configure a layered technology stack (e.g a web server).</li>\n<li>Roles -- Wrapper classes that combine multiple profiles to configure a complete system (for example our \"web server\" role might also be combined with a \"security\" role that hardens the server and a \"monitoring\" role that configures monitoring).</li>\n</ol>\n\n<h1 id=\"profiles\">Profiles</h1>\n\n<blockquote>\n  <p><em>\"From a Puppet perspective, a profile is just a normal class stored in the profile module.\"</em></p>\n</blockquote>\n\n<p>A profile will generally be a <code>class</code> that takes a particular module and adds additional code or logic to implement the it appropriately in your environment.</p>\n\n<p>Here is an example (from the official Puppet documentation) of a Jenkins Master profile:</p>\n\n<pre><code>class profile::jenkins::master (  \n  String $jenkins_port = '9091',\n  String $java_dist    = 'jdk',\n  String $java_version = 'latest',\n) {\n\n  class { 'jenkins':\n    configure_firewall =&gt; true,\n    install_java       =&gt; false,\n    port               =&gt; $jenkins_port,\n    config_hash        =&gt; {\n      'HTTP_PORT'    =&gt; { 'value' =&gt; $jenkins_port },\n      'JENKINS_PORT' =&gt; { 'value' =&gt; $jenkins_port },\n    },\n  }\n\n  class { 'java':\n    distribution =&gt; $java_dist,\n    version      =&gt; $java_version,\n    before       =&gt; Class['jenkins'],\n  }\n}\n</code></pre>\n\n<h1 id=\"roles\">Roles</h1>\n\n<blockquote>\n  <p><em>\"To write roles, we consider the machines we’ll be managing and decide what else they need..\"</em></p>\n</blockquote>\n\n<p>A role class will generally only include profiles and is used to identify a particular function for a node. Continuing the example above, our Jenkins Master role might be a combination of the profile shown above as well as our (currently imaginary) security hardening and server monitoring profiles that were mentioned earlier. </p>\n\n<pre><code>class role::jenkins::master {  \n  include profile::security\n  include profile::monitoring\n  include profile::jenkins::master\n}\n</code></pre>\n\n<h1 id=\"templates\">Templates</h1>\n\n<p>Templates are used to interpolate values of Puppet variables, facts or Hiera data in to files (generally configuration files). </p>\n\n<p>While you can simply use the Puppet <code>file</code> resource to set the complete contents of a file via the <code>content =&gt;</code> attribute or from another source file via the <code>source =&gt;</code> attribute these only allow you to set a file based on a static source. Often you need the contents of a file to vary depending on where it is used. You could maintain multiple static source files, but Templates offer a more flexible alternative.</p>\n\n<p>A template is simply a standard text file with special placeholders markers (<code>&lt;%= .. %&gt;</code>) to identify where Puppet should inject values. For example:  </p>\n\n<pre><code>some_config_setting = &lt;%= $some_config_value %&gt;  \n</code></pre>\n\n<p>You still make use of template via the Puppet <code>file</code> resource and the <code>content =&gt;</code> attribute. To do so, you make use of the <code>epp()</code> function:  </p>\n\n<pre><code>file { 'c:/windows/system32/drivers/etc/hosts':  \n  content =&gt; epp('/source/hosts.epp'),\n}\n</code></pre>\n\n<p>The above example simply interpolated a variable value into a template. The template tags can also contain any valid Puppet expression or code. This might make sense for example if we needed to set a configuration setting based on a variation of the machine the Puppet manifest is applied to (such as the amount of memory). For example if we needed some app memory setting to be set to 75% of total RAM:  </p>\n\n<pre><code>some_memory_setting = &lt;%= $facts['memory']['system']['total_bytes'] * 3/4 %&gt;  \n</code></pre>\n\n<p>We can also use Puppet's conditional statements to make certain parts of a template optional:  </p>\n\n<pre><code>&lt;% if $facts['os']['family'] == 'Windows' { -%&gt;  \n..some Windows specific config..\n&lt;% } -%&gt;  \n</code></pre>\n\n<p>And iteration to generate a varying number of elements based on some collection:  </p>\n\n<pre><code>&lt;% $facts['networking']['interfaces'].each |String $interface, Hash $attrs| { -%&gt;  \n  some_interface_setting = &lt;%= $interface %&gt;\n&lt;% } -%&gt;  \n</code></pre>\n\n<h1 id=\"templateparameters\">Template Parameters</h1>\n\n<p>Because you can include various variables and Puppet code per the above examples, it can easily become difficult to know which of these a specific template contains. One way to manage this is to add parameter declarations at the top of your template. This is optional and is done using <code>|</code> pipe characters as follows:  </p>\n\n<pre><code>&lt;% | String $some_config_setting,  \n     String $some_other_setting,\n| -%&gt;\n</code></pre>\n\n<p>Just like Classes, parameters can be given default values and any that do not have default values are mandatory.  When you include parameter declarations you must then pass any mandatory parameters in hash form as the second argument of the <code>epp()</code> function:  </p>\n\n<pre><code>file { 'c:/windows/system32/drivers/etc/hosts':  \n  content =&gt; epp('/source/hosts.epp',\n    {\n       'some_config_setting' =&gt; 'something',\n       'some_other_setting'  =&gt; 'something else',\n    },\n  ),\n}\n</code></pre>\n\n<p>The values passed need not be literal, you could (for example) use <code>lookup()</code> to get Hiera values. In facts its generally best practice to do your Hiera lookups this way vs doing so in the templates directly.</p>\n\n<h1 id=\"debuggingtemplates\">Debugging Templates</h1>\n\n<p>Puppet provides a tool to assist with validating templates:  </p>\n\n<pre><code>puppet epp validate .\\path\\to\\template.epp  \n</code></pre>\n\n<p>If there is no output the template is valid. If there is any error in the template, you'll see an error message accordingly:</p>\n\n<p><img src=\"/content/images/2018/02/Puppet-epp-validate.png\" alt=\"\" /></p>\n\n<p>Puppet also provides a tool to generate what the template will look like so you can validate the result is what you expect:  </p>\n\n<pre><code>puppet epp render  \n</code></pre>\n\n<p>To use this you must pass it a string that is a hashtable of values for the input variables. For example:  </p>\n\n<pre><code>puppet epp render .\\example.epp --values { some_config_setting =&gt; 'something', 'some_other_setting'  =&gt; 'something else' }  \n</code></pre>\n\n<p>Returns: <br />\n<img src=\"/content/images/2018/02/Puppet-epp-render.png\" alt=\"\" />\n<em>-- This .epp contains all the examples above so the other output values have been generated from the <code>$facts</code> variable.</em></p>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>This post has been an exploration of the ways in which to wrap your Puppet code as classes, profiles and roles in order to make it more reusable and how you can use templates to handle configuration files in a more dynamic way. To read more on these topics I recommend the following links:</p>\n\n<ul>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/lang_classes.html\">Puppet Classes</a></li>\n<li><a href=\"https://puppet.com/docs/pe/2017.3/managing_nodes/designing_convenient_roles.html\">Designing Roles</a></li>\n<li><a href=\"https://puppet.com/docs/pe/2017.2/r_n_p_full_example.html\">Roles and Profiles Example</a></li>\n<li><a href=\"https://puppet.com/docs/puppet/5.3/lang_template.html\">Using Templates</a></li>\n</ul>",
                        "image": "/content/images/2018/01/Matryoshka-Red-Riding-Hood-Line-Up.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-01-31 14:22:29",
                        "created_by": 1,
                        "updated_at": "2018-02-14 12:26:47",
                        "updated_by": 1,
                        "published_at": "2018-02-14 12:26:47",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 68,
                        "uuid": "8b364f33-92ee-4eff-8bba-a38e42027db7",
                        "title": "Keep PowerShell Core updated on Windows with Chocolatey",
                        "slug": "keep-powershell-core-updated-with-chocolatey",
                        "markdown": "PowerShell Core is the cross-platform version of PowerShell that runs on Windows, Mac and Linux. If you are not familar with it, [check out my previous blog post on the topic](http://wragg.io/powershell-core/). It's likely that PowerShell Core will see more regular releases than we've had historically with Windows PowerShell. While you will be able to download the .msi installer for these releases to update your version, this blog post covers how can use the Windows package management tool [Chocolatey](https://chocolatey.org) to manage your upgrades instead.\n\nSince [February 2017](https://blogs.msdn.microsoft.com/powershell/2017/02/01/installing-latest-powershell-core-6-0-release-on-linux-just-got-easier/) its been possible on Linux to install and upgrade PowerShell via the package management tools `apt-get` and `yum`. Windows doesn't natively have a package management tool and Chocolatey exists to fill that void. Package managers greatly simplify the task of installing and managing software packages.\n\n> *\"Chocolatey is a package manager for Windows (like apt-get or yum but for Windows). It was designed to be a decentralized framework for quickly installing applications and tools that you need. It is built on the NuGet infrastructure currently using PowerShell as its focus for delivering packages from the distros to your door, err computer.\"*\n\nIf you don't have chocolatey already, there are several installation options detailed here: https://chocolatey.org/install (which I suggest you check for the latest information). For example, you can open an administrative PowerShell prompt and simply run:\n```\nSet-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n```\nOnce this has completed you can check chocolatey is installed by running `choco` at the command-line, which should return something like this:\n\n![](/content/images/2018/02/Check-Chocolatey-Is-Installed.png)\n\nNow that you have Chocolatey, you can use it to install PowerShell Core.\n\n> Note: if you already have a version of PowerShell Core installed manually, you should uninstall it first.\n\nYou can see the [PowerShell Core Chocolatey Package here](https://chocolatey.org/packages/powershell-core). Per that page, to install PowerShell Core with Chocolatey run (from an administrative level cmd or powershell window):\n```\nchoco install powershell-core\n```\n(add `-y` to have it auto accept/bypass any prompts).\n\nThis should complete as follows:\n![Chocolatey installation of PowerShell Core](/content/images/2018/02/Chocolatey-install-powershell-core.png)\n\nAfter which you should find PowerShell Core in your start menu:\n\n![PowerShell Core in Start Menu](/content/images/2018/02/PowerShell-Core-Startmenu.png)\n\nIf/when PowerShell Core becomes outdated, you can check if a newer version is available by running\n```\nchoco outdated\n```\n![Checking if PowerShell Core is outdated with Chocolatey](/content/images/2018/02/choco-outdated-powershell-core.png)\n\nand upgrade it by running (in an administrative console):\n```\nchoco upgrade powershell-core\n```\nIf you ever need to install a specific version of PowerShell Core, you can do so with the `--version` switch. For example:\n```\nchoco install powershell-core --version 6.0.0\n```\n> Note that if you do this from within PowerShell Core the upgrade will complete but your console will close as a result.\n\n# Summary\n\nThis has been a quick intro to Chocolatey and how you can use it to manage packages. By using the above workflow you can manage and maintain your PowerShell Core version from the console, in the same way as you can on Linux.",
                        "html": "<p>PowerShell Core is the cross-platform version of PowerShell that runs on Windows, Mac and Linux. If you are not familar with it, <a href=\"http://wragg.io/powershell-core/\">check out my previous blog post on the topic</a>. It's likely that PowerShell Core will see more regular releases than we've had historically with Windows PowerShell. While you will be able to download the .msi installer for these releases to update your version, this blog post covers how can use the Windows package management tool <a href=\"https://chocolatey.org\">Chocolatey</a> to manage your upgrades instead.</p>\n\n<p>Since <a href=\"https://blogs.msdn.microsoft.com/powershell/2017/02/01/installing-latest-powershell-core-6-0-release-on-linux-just-got-easier/\">February 2017</a> its been possible on Linux to install and upgrade PowerShell via the package management tools <code>apt-get</code> and <code>yum</code>. Windows doesn't natively have a package management tool and Chocolatey exists to fill that void. Package managers greatly simplify the task of installing and managing software packages.</p>\n\n<blockquote>\n  <p><em>\"Chocolatey is a package manager for Windows (like apt-get or yum but for Windows). It was designed to be a decentralized framework for quickly installing applications and tools that you need. It is built on the NuGet infrastructure currently using PowerShell as its focus for delivering packages from the distros to your door, err computer.\"</em></p>\n</blockquote>\n\n<p>If you don't have chocolatey already, there are several installation options detailed here: <a href=\"https://chocolatey.org/install\">https://chocolatey.org/install</a> (which I suggest you check for the latest information). For example, you can open an administrative PowerShell prompt and simply run:  </p>\n\n<pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))  \n</code></pre>\n\n<p>Once this has completed you can check chocolatey is installed by running <code>choco</code> at the command-line, which should return something like this:</p>\n\n<p><img src=\"/content/images/2018/02/Check-Chocolatey-Is-Installed.png\" alt=\"\" /></p>\n\n<p>Now that you have Chocolatey, you can use it to install PowerShell Core.</p>\n\n<blockquote>\n  <p>Note: if you already have a version of PowerShell Core installed manually, you should uninstall it first.</p>\n</blockquote>\n\n<p>You can see the <a href=\"https://chocolatey.org/packages/powershell-core\">PowerShell Core Chocolatey Package here</a>. Per that page, to install PowerShell Core with Chocolatey run (from an administrative level cmd or powershell window):  </p>\n\n<pre><code>choco install powershell-core  \n</code></pre>\n\n<p>(add <code>-y</code> to have it auto accept/bypass any prompts).</p>\n\n<p>This should complete as follows: <br />\n<img src=\"/content/images/2018/02/Chocolatey-install-powershell-core.png\" alt=\"Chocolatey installation of PowerShell Core\" /></p>\n\n<p>After which you should find PowerShell Core in your start menu:</p>\n\n<p><img src=\"/content/images/2018/02/PowerShell-Core-Startmenu.png\" alt=\"PowerShell Core in Start Menu\" /></p>\n\n<p>If/when PowerShell Core becomes outdated, you can check if a newer version is available by running  </p>\n\n<pre><code>choco outdated  \n</code></pre>\n\n<p><img src=\"/content/images/2018/02/choco-outdated-powershell-core.png\" alt=\"Checking if PowerShell Core is outdated with Chocolatey\" /></p>\n\n<p>and upgrade it by running (in an administrative console):  </p>\n\n<pre><code>choco upgrade powershell-core  \n</code></pre>\n\n<p>If you ever need to install a specific version of PowerShell Core, you can do so with the <code>--version</code> switch. For example:  </p>\n\n<pre><code>choco install powershell-core --version 6.0.0  \n</code></pre>\n\n<blockquote>\n  <p>Note that if you do this from within PowerShell Core the upgrade will complete but your console will close as a result.</p>\n</blockquote>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>This has been a quick intro to Chocolatey and how you can use it to manage packages. By using the above workflow you can manage and maintain your PowerShell Core version from the console, in the same way as you can on Linux.</p>",
                        "image": "/content/images/2018/02/brownie-dessert-cake-sweet-45202.jpeg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-02-28 14:09:39",
                        "created_by": 1,
                        "updated_at": "2018-02-28 15:08:35",
                        "updated_by": 1,
                        "published_at": "2018-02-28 15:01:22",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 69,
                        "uuid": "37ea649a-d0c0-432d-9eb1-3c06bcad54ae",
                        "title": "Azure Containers",
                        "slug": "azure-containers",
                        "markdown": "Microsoft Azure has two container services:\n\n- [Azure Container Service](https://azure.microsoft.com/en-gb/overview/containers/) (generally available [since April 2016](https://azure.microsoft.com/en-us/blog/azure-container-service-is-now-generally-available/)  – you can also use Kubernetes as an orchestrator)\n- [Azure Container Service - AKS](https://azure.microsoft.com/en-gb/services/container-service/) (preview) - also known as container services (managed) is a dedicated managed Kubernetes service, in preview since Oct 2017\nThere are also the following services:\n\n- [Container Groups](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-container-groups) (preview) - a top level resource for Azure Container instances. A container group is a collection of containers that get scheduled on the same host machine. They share a lifecycle, local network and storage volumes.\n- [Container Registries](https://azure.microsoft.com/en-gb/services/container-registry/) - a private docker registry to store and manage container images.\n- [Container Instances](https://azure.microsoft.com/en-gb/services/container-instances/) (preview) - a service for easily running containers without needing to worry about orchestration. I think this service is aimed at testing/development.\n\n# Azure Container Service\n\n- Available in all regions\n- The master Kubernetes machines are exposed to you in the Azure portal and are yours to manage (and pay for).\n\n# Azure Container Service (AKS)\n- Kubernetes as a service. Currently in preview\n- Only available in the East US and Central US regions\n- The master Kubernetes machines are a managed service grouped and referred to as the \"Hosted Control Plane\". You still control if/when Kubernetes version is upgraded (which can be done without downtime) but you aren't having to otherwise pay for or manage the management machines.\n- Only pay for the virtual machines instances, storage and networking resources consumed by your Kubernetes cluster.\n- Azure Container Service is a free service, therefore it does not have a financially backed SLA. However, for the availability of underlying virtual machines, the Virtual Machine SLA applies.",
                        "html": "<p>Microsoft Azure has two container services:</p>\n\n<ul>\n<li><a href=\"https://azure.microsoft.com/en-gb/overview/containers/\">Azure Container Service</a> (generally available <a href=\"https://azure.microsoft.com/en-us/blog/azure-container-service-is-now-generally-available/\">since April 2016</a>  – you can also use Kubernetes as an orchestrator)</li>\n<li><p><a href=\"https://azure.microsoft.com/en-gb/services/container-service/\">Azure Container Service - AKS</a> (preview) - also known as container services (managed) is a dedicated managed Kubernetes service, in preview since Oct 2017\nThere are also the following services:</p></li>\n<li><p><a href=\"https://docs.microsoft.com/en-us/azure/container-instances/container-instances-container-groups\">Container Groups</a> (preview) - a top level resource for Azure Container instances. A container group is a collection of containers that get scheduled on the same host machine. They share a lifecycle, local network and storage volumes.</p></li>\n<li><a href=\"https://azure.microsoft.com/en-gb/services/container-registry/\">Container Registries</a> - a private docker registry to store and manage container images.</li>\n<li><a href=\"https://azure.microsoft.com/en-gb/services/container-instances/\">Container Instances</a> (preview) - a service for easily running containers without needing to worry about orchestration. I think this service is aimed at testing/development.</li>\n</ul>\n\n<h1 id=\"azurecontainerservice\">Azure Container Service</h1>\n\n<ul>\n<li>Available in all regions</li>\n<li>The master Kubernetes machines are exposed to you in the Azure portal and are yours to manage (and pay for).</li>\n</ul>\n\n<h1 id=\"azurecontainerserviceaks\">Azure Container Service (AKS)</h1>\n\n<ul>\n<li>Kubernetes as a service. Currently in preview</li>\n<li>Only available in the East US and Central US regions</li>\n<li>The master Kubernetes machines are a managed service grouped and referred to as the \"Hosted Control Plane\". You still control if/when Kubernetes version is upgraded (which can be done without downtime) but you aren't having to otherwise pay for or manage the management machines.</li>\n<li>Only pay for the virtual machines instances, storage and networking resources consumed by your Kubernetes cluster.</li>\n<li>Azure Container Service is a free service, therefore it does not have a financially backed SLA. However, for the availability of underlying virtual machines, the Virtual Machine SLA applies.</li>\n</ul>",
                        "image": null,
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-03-03 11:51:34",
                        "created_by": 1,
                        "updated_at": "2018-03-03 11:53:49",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 71,
                        "uuid": "d7f77b4e-8829-4027-8fc6-ddef6ed2efa7",
                        "title": "Watch for changes with PowerShell",
                        "slug": "watch-for-changes-with-powershell",
                        "markdown": "I recently needed to make a change to the membership of an Active Directory group which was enforced via Puppet. While waiting for the Puppet manifest to apply I used `Get-ADGroupMember` in PowerShell to check if the change had taken effect. Finding that it had not, I then wrote a crude loop to continually check the group membership until it changed. It occurred to me that this kind of functionality might be useful as a cmdlet and as such I have created `Watch-Command`. This blog post details how it works as well as some inventive but controversial design decisions I made.\n\n![The cats of the nights watch](/content/images/2018/03/Nights-watch-cats.png)\n\n> While working on this cmdlet I discovered that [Lee Holmes provides a very similar cmdlet](http://www.powershellcookbook.com/recipe/PrtD/program-monitor-a-command-for-changes) as part of his Windows PowerShell Cookbook. While I didn't see his version until mine was mostly formed, it takes a very similar but simpler approach and you might want to check it out.\n\nIf you want to try my version of `Watch-Command`, it is available via a module named [Watch](https://powershellgallery.com/packages/Watch/) in the PowerShell Gallery. If you have PS5 or newer you can install it via:\n```\nInstall-Module Watch -Scope CurrentUser\n```\nThere are three different ways to use the cmdlet. The most straightforward is to provide it with a ScriptBlock as input. You define a ScriptBlock by wrapping one or more commands in curly braces `{ .. }`. For example:\n\n```\nWatch-Command {\n   $Seconds = (Get-Date).Second\n   $Seconds - ($Seconds % 10)\n} -Verbose\n```\nThis gets the seconds portion of the current time and rounds it down to the nearest ten (using the mod `%` math operator). `Watch-Command` will therefore repeat this every second (by default) until the second count moves in to the next ten.\n\n![Watch-Command Example](/content/images/2018/03/watch-command-seconds.png)\n\nIf you want `Watch-Command` to run continuously you can add the `-Continuous` switch:\n\n```\nWatch-Command {\n   $Seconds = (Get-Date).Second\n   $Seconds - ($Seconds % 10)\n} -Continuous -Verbose\n```\n\nWhen you want it to stop, just press `CTRL+C`.\n\n![](/content/images/2018/03/watch-command-seconds-continuous.png)\n\nThe second way you can use the cmdlet is to provide a ScriptBlock via the pipeline. For example:\n```\n{ Get-Date } | Watch-Command\n```\nThe third and final way to use the cmdlet is where things get controversial. It occurred to me that if you had a very long command you'd already entered in to the console, it could be slightly annoying to have to cursor back to the beginning and end of that command to wrap it in curly braces. Ideally you would just want to be able to up arrow and throw `| Watch-Command` at the end of a pipeline, and have it take on repeat execution of the command. \n\nThe cmdlet allows you to do just that:\n\n![Watch-Command Example using pipeline](/content/images/2018/03/watch-command-pipleine.png)\n\nIn this example we're checking the `winrm` service and then waiting for it to change (this example also uses the `wc` alias for `Watch-Command`).\n\n**This is controversial because by doing this I don't handle the pipeline the way a cmdlet should.**  \n\nAlthough the object produced by the pipeline that proceeded `Watch-Cmdlet` is sent to the `-ScriptBlock` parameter, if it is not a ScriptBlock I throw it away and replace it with whatever commands preceeded the cmdlet in the pipeline. \n\n## How does this work?\n\nI discovered I could get the commandline that invoked the cmdlet via the automatic variable `$MyInvocation` and (specifically) its `.Line` property. \n\n> `$MyInvocation` is an automatic variable available within scripts, functions and script blocks that contains information about the current command, such as the name, parameters, parameter values and information about how the command was started or invoked.\n\nHere's an example function that exposes `$MyInvocation`:\n\n```\nFunction Show-MyInvocation {\n    Param(\n        [parameter(ValueFromPipeline)]\n        $InputObject\n    )\n    $MyInvocation\n}\n\nGet-Service | Where {$_.name -eq 'winrm'} | Select name | Show-MyInvocation\n```\n\n![](/content/images/2018/03/show-myinvocation.png)\n\nThe `Watch-Command` cmdlet uses several properties of `$MyInvocation` as follows:\n\n```\nif ($ScriptBlock -isnot [scriptblock]){\n    if ($MyInvocation.PipelinePosition -gt 1){        \n        $ScriptBlock = [Scriptblock]::Create( ($MyInvocation.Line -Split \"\\|\\s*$($MyInvocation.InvocationName)\")[0] )\n    }\n    else{\n        Throw 'The -ScriptBlock parameter must be provided an object of type ScriptBlock unless invoked via the Pipeline.'\n    }\n}\n```\nFirst it checks that it has't received a ScriptBlock type object. If it hasn't, then it uses the `PipelinePositon` property (which tells it how many commands preceded it) to check that there was at least one command before it in the pipeline (without which we have nothing to process). \n\nIt then uses `-Split` and a Regular Expression to find where its cmdlet name appears in the `Line` property next to a `|` and zero or more spaces. It uses the `$MyInvocation.InvocationName` property to reference its own cmdlet name so that if it was invoked via an Alias it remains correct.\n\nHaving got the command, it then uses the `[ScriptBlock]::Create` method to convert the string to a scriptblock.\n\n## Usage Examples\n\nThe default behaviour of `Watch-Command` is just to run some block of code and then return the full result as soon as the output changes from its first iteration.\n\nHowever you can also add a `-Difference` switch to have it only return the differences between the first and changed result. This comparison is performed via the `Compare-Object` cmdlet, and then we only return the right-side changes from that cmdlet. This is so that generally you get just one result (e.g the change), vs it returning what it was before and after the change.\n\nBy using `-Difference` with `-Continuous` you can use `Watch-Command` to perform ongoing monitoring of state changes. For example:\n```\nGet-Service | Watch-Command -Diff -Cont -Verbose\n``` \nThis will show ongoing output each time a service changes state:\n\n![Watch-Command continuous service monitoring example](/content/images/2018/03/watch-command-get-service-continuous.png)\n\nHaving used the `-Verbose` switch you can see (in the output above) which specific properties are being 'watched'. By default the cmdlet will look to see if the command being executed returns a [Default Display Set](https://blogs.msdn.microsoft.com/powershell/2010/02/18/psstandardmembers-the-stealth-property/). If one exists, it uses these properties by default in order to limit how many properties are being monitored to a sensible set of defaults. If there isn't a Default Display Set then by default it monitors all properties.\n\nIf you want to explicitly specify what properties are monitored, you can do so with the `-Property` parameter. Here's an example of where this might be useful:\n```\nGet-Process | Watch-Command -Diff -Cont -Property id\n```\n\n![](/content/images/2018/03/watch-command-get-process-id-continuous.png)\n\nBy default, `Get-Process` displays Id, Handles, CPU, SI and Name properties. Obviously the result of CPU changes frequently, so if we want to set up `Watch-Command` to report to us only when new processes start, we can just monitor the Id property.\n\nIf you want to force the command to monitor all object properties (where there is a Default Display Set taking precedence) you can do so by specifying `-Property *`.\n\nYou can use `Watch-Command` to monitor non-PowerShell command output also (which will generally be treated as strings). Here's an example of monitoring the output of `ipconfig /all` for a change to the DNS server addresses:\n\n![Watch-Command example monitoring ipconfig change](/content/images/2018/03/watch-command-ipconfig.png)\n\nThere are a few other way to customise your use of `Watch-Command`:\n\n- Per the above example, if you want to change the delay between checks, you can specify it with the `-Seconds` parameter. You can set this to `0` if you want the checks to occur at the speed of PowerShell.\n- If you want to forcibly convert the output of you command to an array of strings for the comparison, you can use the `-AsString` parameter. When comparing strings, no specific object properties are monitored (unless you specify one) as you probably want to monitor changes to the string values, vs one of the string objects properties.\n\nFor full details of the parameters and some further examples, have a look at `Get-Help Watch-Command -Full`.\n\n# Summary\n\nThis post has been a quick overview of the `Watch-Command` cmdlet. If you want to look at the code for the function in full, you can see it in Github [here](https://github.com/markwragg/PowerShell-Watch/blob/master/Watch/Public/Watch-Command.ps1).",
                        "html": "<p>I recently needed to make a change to the membership of an Active Directory group which was enforced via Puppet. While waiting for the Puppet manifest to apply I used <code>Get-ADGroupMember</code> in PowerShell to check if the change had taken effect. Finding that it had not, I then wrote a crude loop to continually check the group membership until it changed. It occurred to me that this kind of functionality might be useful as a cmdlet and as such I have created <code>Watch-Command</code>. This blog post details how it works as well as some inventive but controversial design decisions I made.</p>\n\n<p><img src=\"/content/images/2018/03/Nights-watch-cats.png\" alt=\"The cats of the nights watch\" /></p>\n\n<blockquote>\n  <p>While working on this cmdlet I discovered that <a href=\"http://www.powershellcookbook.com/recipe/PrtD/program-monitor-a-command-for-changes\">Lee Holmes provides a very similar cmdlet</a> as part of his Windows PowerShell Cookbook. While I didn't see his version until mine was mostly formed, it takes a very similar but simpler approach and you might want to check it out.</p>\n</blockquote>\n\n<p>If you want to try my version of <code>Watch-Command</code>, it is available via a module named <a href=\"https://powershellgallery.com/packages/Watch/\">Watch</a> in the PowerShell Gallery. If you have PS5 or newer you can install it via:  </p>\n\n<pre><code>Install-Module Watch -Scope CurrentUser  \n</code></pre>\n\n<p>There are three different ways to use the cmdlet. The most straightforward is to provide it with a ScriptBlock as input. You define a ScriptBlock by wrapping one or more commands in curly braces <code>{ .. }</code>. For example:</p>\n\n<pre><code>Watch-Command {  \n   $Seconds = (Get-Date).Second\n   $Seconds - ($Seconds % 10)\n} -Verbose\n</code></pre>\n\n<p>This gets the seconds portion of the current time and rounds it down to the nearest ten (using the mod <code>%</code> math operator). <code>Watch-Command</code> will therefore repeat this every second (by default) until the second count moves in to the next ten.</p>\n\n<p><img src=\"/content/images/2018/03/watch-command-seconds.png\" alt=\"Watch-Command Example\" /></p>\n\n<p>If you want <code>Watch-Command</code> to run continuously you can add the <code>-Continuous</code> switch:</p>\n\n<pre><code>Watch-Command {  \n   $Seconds = (Get-Date).Second\n   $Seconds - ($Seconds % 10)\n} -Continuous -Verbose\n</code></pre>\n\n<p>When you want it to stop, just press <code>CTRL+C</code>.</p>\n\n<p><img src=\"/content/images/2018/03/watch-command-seconds-continuous.png\" alt=\"\" /></p>\n\n<p>The second way you can use the cmdlet is to provide a ScriptBlock via the pipeline. For example:  </p>\n\n<pre><code>{ Get-Date } | Watch-Command\n</code></pre>\n\n<p>The third and final way to use the cmdlet is where things get controversial. It occurred to me that if you had a very long command you'd already entered in to the console, it could be slightly annoying to have to cursor back to the beginning and end of that command to wrap it in curly braces. Ideally you would just want to be able to up arrow and throw <code>| Watch-Command</code> at the end of a pipeline, and have it take on repeat execution of the command. </p>\n\n<p>The cmdlet allows you to do just that:</p>\n\n<p><img src=\"/content/images/2018/03/watch-command-pipleine.png\" alt=\"Watch-Command Example using pipeline\" /></p>\n\n<p>In this example we're checking the <code>winrm</code> service and then waiting for it to change (this example also uses the <code>wc</code> alias for <code>Watch-Command</code>).</p>\n\n<p><strong>This is controversial because by doing this I don't handle the pipeline the way a cmdlet should.</strong>  </p>\n\n<p>Although the object produced by the pipeline that proceeded <code>Watch-Cmdlet</code> is sent to the <code>-ScriptBlock</code> parameter, if it is not a ScriptBlock I throw it away and replace it with whatever commands preceeded the cmdlet in the pipeline. </p>\n\n<h2 id=\"howdoesthiswork\">How does this work?</h2>\n\n<p>I discovered I could get the commandline that invoked the cmdlet via the automatic variable <code>$MyInvocation</code> and (specifically) its <code>.Line</code> property. </p>\n\n<blockquote>\n  <p><code>$MyInvocation</code> is an automatic variable available within scripts, functions and script blocks that contains information about the current command, such as the name, parameters, parameter values and information about how the command was started or invoked.</p>\n</blockquote>\n\n<p>Here's an example function that exposes <code>$MyInvocation</code>:</p>\n\n<pre><code>Function Show-MyInvocation {  \n    Param(\n        [parameter(ValueFromPipeline)]\n        $InputObject\n    )\n    $MyInvocation\n}\n\nGet-Service | Where {$_.name -eq 'winrm'} | Select name | Show-MyInvocation  \n</code></pre>\n\n<p><img src=\"/content/images/2018/03/show-myinvocation.png\" alt=\"\" /></p>\n\n<p>The <code>Watch-Command</code> cmdlet uses several properties of <code>$MyInvocation</code> as follows:</p>\n\n<pre><code>if ($ScriptBlock -isnot [scriptblock]){  \n    if ($MyInvocation.PipelinePosition -gt 1){        \n        $ScriptBlock = [Scriptblock]::Create( ($MyInvocation.Line -Split \"\\|\\s*$($MyInvocation.InvocationName)\")[0] )\n    }\n    else{\n        Throw 'The -ScriptBlock parameter must be provided an object of type ScriptBlock unless invoked via the Pipeline.'\n    }\n}\n</code></pre>\n\n<p>First it checks that it has't received a ScriptBlock type object. If it hasn't, then it uses the <code>PipelinePositon</code> property (which tells it how many commands preceded it) to check that there was at least one command before it in the pipeline (without which we have nothing to process). </p>\n\n<p>It then uses <code>-Split</code> and a Regular Expression to find where its cmdlet name appears in the <code>Line</code> property next to a <code>|</code> and zero or more spaces. It uses the <code>$MyInvocation.InvocationName</code> property to reference its own cmdlet name so that if it was invoked via an Alias it remains correct.</p>\n\n<p>Having got the command, it then uses the <code>[ScriptBlock]::Create</code> method to convert the string to a scriptblock.</p>\n\n<h2 id=\"usageexamples\">Usage Examples</h2>\n\n<p>The default behaviour of <code>Watch-Command</code> is just to run some block of code and then return the full result as soon as the output changes from its first iteration.</p>\n\n<p>However you can also add a <code>-Difference</code> switch to have it only return the differences between the first and changed result. This comparison is performed via the <code>Compare-Object</code> cmdlet, and then we only return the right-side changes from that cmdlet. This is so that generally you get just one result (e.g the change), vs it returning what it was before and after the change.</p>\n\n<p>By using <code>-Difference</code> with <code>-Continuous</code> you can use <code>Watch-Command</code> to perform ongoing monitoring of state changes. For example:  </p>\n\n<pre><code>Get-Service | Watch-Command -Diff -Cont -Verbose  \n</code></pre>\n\n<p>\nThis will show ongoing output each time a service changes state:</p>\n\n<p><img src=\"/content/images/2018/03/watch-command-get-service-continuous.png\" alt=\"Watch-Command continuous service monitoring example\" /></p>\n\n<p>Having used the <code>-Verbose</code> switch you can see (in the output above) which specific properties are being 'watched'. By default the cmdlet will look to see if the command being executed returns a <a href=\"https://blogs.msdn.microsoft.com/powershell/2010/02/18/psstandardmembers-the-stealth-property/\">Default Display Set</a>. If one exists, it uses these properties by default in order to limit how many properties are being monitored to a sensible set of defaults. If there isn't a Default Display Set then by default it monitors all properties.</p>\n\n<p>If you want to explicitly specify what properties are monitored, you can do so with the <code>-Property</code> parameter. Here's an example of where this might be useful:  </p>\n\n<pre><code>Get-Process | Watch-Command -Diff -Cont -Property id  \n</code></pre>\n\n<p><img src=\"/content/images/2018/03/watch-command-get-process-id-continuous.png\" alt=\"\" /></p>\n\n<p>By default, <code>Get-Process</code> displays Id, Handles, CPU, SI and Name properties. Obviously the result of CPU changes frequently, so if we want to set up <code>Watch-Command</code> to report to us only when new processes start, we can just monitor the Id property.</p>\n\n<p>If you want to force the command to monitor all object properties (where there is a Default Display Set taking precedence) you can do so by specifying <code>-Property *</code>.</p>\n\n<p>You can use <code>Watch-Command</code> to monitor non-PowerShell command output also (which will generally be treated as strings). Here's an example of monitoring the output of <code>ipconfig /all</code> for a change to the DNS server addresses:</p>\n\n<p><img src=\"/content/images/2018/03/watch-command-ipconfig.png\" alt=\"Watch-Command example monitoring ipconfig change\" /></p>\n\n<p>There are a few other way to customise your use of <code>Watch-Command</code>:</p>\n\n<ul>\n<li>Per the above example, if you want to change the delay between checks, you can specify it with the <code>-Seconds</code> parameter. You can set this to <code>0</code> if you want the checks to occur at the speed of PowerShell.</li>\n<li>If you want to forcibly convert the output of you command to an array of strings for the comparison, you can use the <code>-AsString</code> parameter. When comparing strings, no specific object properties are monitored (unless you specify one) as you probably want to monitor changes to the string values, vs one of the string objects properties.</li>\n</ul>\n\n<p>For full details of the parameters and some further examples, have a look at <code>Get-Help Watch-Command -Full</code>.</p>\n\n<h1 id=\"summary\">Summary</h1>\n\n<p>This post has been a quick overview of the <code>Watch-Command</code> cmdlet. If you want to look at the code for the function in full, you can see it in Github <a href=\"https://github.com/markwragg/PowerShell-Watch/blob/master/Watch/Public/Watch-Command.ps1\">here</a>.</p>",
                        "image": "/content/images/2018/03/waiting_cat.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "Watch-Command: a PowerShell cmdlet to monitor and wait for changes",
                        "meta_description": "This post details a watch-command cmdlet for PowerShell which can be used to repeatedly execute one or more commands until a change in output occurs.",
                        "author_id": 1,
                        "created_at": "2018-03-19 13:52:48",
                        "created_by": 1,
                        "updated_at": "2018-12-05 12:37:30",
                        "updated_by": 1,
                        "published_at": "2018-03-21 19:46:47",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 72,
                        "uuid": "47a68a43-bad2-493e-929e-e8b7081641af",
                        "title": "Adding a code coverage badge to a PowerShell project",
                        "slug": "add-a-code-coverage-badge-to-your-powershell-deployment-pipeline",
                        "markdown": "This blog post describes how you can add a code coverage badge to the readme.md of your PowerShell projects in Github as well as have them automatically updated with the current coverage percentage via your Continuous Integration pipeline (if you've implemented one).\n\n> \"Code coverage\" (or test coverage) is the concept of reporting how much of your code is executed by your tests. A program with high test coverage is likely to have a lower chance of containing undetected bugs, compared to a program with low test coverage.\n\nAdding badges to your projects readme.md gives potential users of your code a quick way to assess the health of the project. Here is an example of what the badges currently look like, on my [PowerShell-Influx](https://github.com/markwragg/PowerShell-Influx) project:\n\n![PowerShell-Influx project readme.md showing code coverage badge](/content/images/2018/04/code-coverage-example-powershell-influx.png)\n\n\n> In order to follow this guide completely you need to have already implemented Pester tests for your projects and a CI pipeline (e.g via AppVeyor or similar) to run those tests automatically when code changes occur. \n>\n> - If you haven't implemented tests for PowerShell before, I strongly recommend you check out [Pester](https://github.com/pester/Pester). \n> - If you haven't implemented a CI pipeline for a PowerShell project before, I recommend you check out [Warren Frame's excellent guide](http://ramblingcookiemonster.github.io/PSDeploy-Inception/).\n>\n> If you haven't implemented a CI pipeline (and/or don't want to yet) you could still implement the code coverage badge (per most of the guidance below), you'd just need to ensure you kicked off your `build.ps1` script manually before each time you make a commit to the project in Github.\n\nThere are three steps you need to complete in order to implement the badge:\n\n**1. You need to modify your call of `Invoke-Pester` so that it returns a code coverage report on completion.**\n\nIf you have a CI pipeline setup, you likely have a `build.ps1` or similar script that is invoked by your CI tool when a commit is made. It may be in here that you invoke Pester to run your tests, or (like me) you might make use of [PSake](https://github.com/psake/psake) which is a handy tool for organising your build in to phases.\n\nWithin my `psake.ps1` (triggered by my `build.ps1` which just ensures the modules my pipeline depends on are installed) I have a 'Test' task where Pester is invoked. This task has now been modified as follows:\n```\n$CodeFiles = (Get-ChildItem $ENV:BHModulePath -Recurse -Include \"*.psm1\",\"*.ps1\").FullName\n\n$Script:TestResults = Invoke-Pester -Path $ProjectRoot\\Tests -CodeCoverage $CodeFiles -PassThru -OutputFormat NUnitXml -OutputFile \"$ProjectRoot\\$TestFile\" -ExcludeTag Integration\n```\nFirst it populates a `$CodeFiles` variable via `Get-ChildItem` with all the `.ps1` and `.psm1` files that are within my module directory (therefore excluding `.ps1` files that relate to the build pipeline, as these are in the folder above the module).\n\nThis variable is then sent to the `-CodeCoverage` parameter of `Invoke-Pester`, which takes an array of files as input with which to perform code coverage analysis against.\n\nI also return the result of `Invoke-Pester` to a variable (`$Script:TestResults`) as this is where the code coverage result is then stored. This variable is `$Script:` scoped so that I can access it from the next PSake task in my pipeline (the Build task).\n\nWhen you use the `-CodeCoverage` parameter, Pester outputs a report at the end which shows you exactly which lines were not covered by your tests. Use this to guide you if you want to improve your code coverage score:\n\n![PowerShell Pester Code coverage report output](/content/images/2018/05/Pester-Code-Coverage.png)\n\nFor example, in the output above none of my tests result in a certain `Write-Error` command being output, so if I want to test that this works correctly I could add a Pester tests with some input data that would cause this error to occur and then have the test verify that it does as expected.\n\nIf you have a CI pipeline, you'll also find the code coverage report in the output of your build:\n\n![PowerShell Pester Code coverage report output in an Appveyor build job](/content/images/2018/05/Pester-Code-Coverage-AppVeyor-Output.png)\n\n**2. You need to implement a small function that I stole (with permission) from my good friend and PowerShell aficionado [Sam Martin](https://sammart.in/).**\n\nWithin your build script you need to add the following function (by Sam) that modifies the readme.md file to update the URL of the code coverage badge with the current coverage percentage results:\n```\nfunction Update-CodeCoveragePercent {\n    [cmdletbinding(supportsshouldprocess)]\n    param(\n        [int]\n        $CodeCoverage = 0,\n        \n        [string]\n        $TextFilePath = \"$Env:BHProjectPath\\Readme.md\"\n    )\n    \n    $BadgeColor = switch ($CodeCoverage) {\n        {$_ -in 90..100} { 'brightgreen' }\n        {$_ -in 75..89}  { 'yellow' }\n        {$_ -in 60..74}  { 'orange' }\n        default          { 'red' }\n    }\n    \n    if ($PSCmdlet.ShouldProcess($TextFilePath)) {\n        $ReadmeContent = (Get-Content $TextFilePath)\n        $ReadmeContent = $ReadmeContent -replace \"!\\[Test Coverage\\].+\\)\", \"![Test Coverage](https://img.shields.io/badge/coverage-$CodeCoverage%25-$BadgeColor.svg?maxAge=60)\" \n        $ReadmeContent | Set-Content -Path $TextFilePath\n    }\n}\n```\nYou then make immediate use of the function as follows, by taking the earlier generated Pester results variable and using its properties to calculate the coverage percentage, which is then sent to the above function as a parameter:\n```\n$CoveragePercent = [math]::floor(100 - (($Script:TestResults.CodeCoverage.NumberOfCommandsMissed / $Script:TestResults.CodeCoverage.NumberOfCommandsAnalyzed) * 100))\n\nUpdate-CodeCoveragePercent -CodeCoverage $CoveragePercent\n```\nYou also of course need to add the Code Coverage badge to your readme.md to start with (so that there's something there for the function to modify). This is made possible via [shields.io](http://shields.io), who provide various badges that can be customised via parameters in the querystring.\n\nTo add their code coverage badge, include this in your Readme.md:\n\n```\n ![Test Coverage](https://img.shields.io/badge/coverage.svg)\n```\nThis version won’t actually work because it doesn't yet include the colour or percentage parameters, but those values will be added when the build runs.\n\n**3. You need to ensure your CI pipeline checks back in modified files after a successful run, in a way that doesn't trigger another build.**\n\nFinally (for a completely automated solution), I added an `on_success` step to my AppVeyor build pipeline that uses git to check back in the modified readme.md to the project in a way that doesn’t trigger another build (lest you wish to be stuck an infinite build loop). \n\nThis was done by modifying my `appveyor.yml` file as follows:\n```\non_success:\n  - git config --global credential.helper store\n  - ps: Add-Content \"$HOME\\.git-credentials\" \"https://$($env:GitToken):x-oauth-basic@github.com`n\"\n  - git config --global user.email \"build@appveyor.com\"\n  - git config --global user.name \"Appveyor\"\n  - git checkout %APPVEYOR_REPO_BRANCH%\n  - git add *.psd1\n  - git add *.md\n  - git commit -m \"[skip ci] AppVeyor Build %APPVEYOR_BUILD_VERSION%\"\n  - git push\n```\nFor this to work you also need to configure a personal access token within Github to allow AppVeyor permission to write to your repository. See [this guide from AppVeyor](https://www.appveyor.com/docs/how-to/git-push/) on how to do this.\n\n> It's very important that you include `[skip ci]` in the commit message (as shown above). By doing so AppVeyor will not trigger another build when this commit occurs.\n\nYou might also have noticed in the code above that I am using Git to commit any modified `*.md` and `*.psd1` files. The .md files are of course to capture the updated `readme.md`. I commit changed .psd1 files because my CI pipeline also publishes my module to the PowerShell Gallery and when doing so it increments the module version in the .psd1 file. I simply check this back in to Github so that the Git repo is consistent with what is in the gallery.\n\n---\nIf you've followed the steps above, you should now have a completely automated way to indicate code coverage results on your PowerShell projects, just like this:\n\n![Code coverage badge example](/content/images/2018/05/Code-Coverage-Badge-Example.png)\n\n*-- In this example the badge is red because the coverage is less than 60%, as I specified in the `Update-CodeCoveragePercent` function. Between 75% and 89% it would be Orange, 90% and above it would be Green. Obviously you can set your own colour/score preferences.*\n\nIn the future you may be able to replace/augment much of the above with https://codecov.io, which is a code coverage reporting service for Github projects. To use CodeCov you need to be able to upload a code coverage report from which CodeCov  generates you a micro-site with detailed information about the code coverage of your project, as well as then a link to a badge you can put in your readme.md. \n\nI believe Pester doesn't yet generate the needed code coverage reports, but an [issue is open on Github for the feature](https://github.com/pester/Pester/issues/212), so watch that if you want to see when it occurs. However it seems that at least one [PowerShell project](https://github.com/aaronpowell/ps-nvm) has developed a workaround for this and is successfully using CodeCov for a PowerShell project.\n\nIn the meantime, if you just want a simple code coverage badge for your readme.md, hopefully the above was helpful. Let me know how you get on implementing it via the comments below.",
                        "html": "<p>This blog post describes how you can add a code coverage badge to the readme.md of your PowerShell projects in Github as well as have them automatically updated with the current coverage percentage via your Continuous Integration pipeline (if you've implemented one).</p>\n\n<blockquote>\n  <p>\"Code coverage\" (or test coverage) is the concept of reporting how much of your code is executed by your tests. A program with high test coverage is likely to have a lower chance of containing undetected bugs, compared to a program with low test coverage.</p>\n</blockquote>\n\n<p>Adding badges to your projects readme.md gives potential users of your code a quick way to assess the health of the project. Here is an example of what the badges currently look like, on my <a href=\"https://github.com/markwragg/PowerShell-Influx\">PowerShell-Influx</a> project:</p>\n\n<p><img src=\"/content/images/2018/04/code-coverage-example-powershell-influx.png\" alt=\"PowerShell-Influx project readme.md showing code coverage badge\" /></p>\n\n<blockquote>\n  <p>In order to follow this guide completely you need to have already implemented Pester tests for your projects and a CI pipeline (e.g via AppVeyor or similar) to run those tests automatically when code changes occur. </p>\n  \n  <ul>\n  <li>If you haven't implemented tests for PowerShell before, I strongly recommend you check out <a href=\"https://github.com/pester/Pester\">Pester</a>. </li>\n  <li>If you haven't implemented a CI pipeline for a PowerShell project before, I recommend you check out <a href=\"http://ramblingcookiemonster.github.io/PSDeploy-Inception/\">Warren Frame's excellent guide</a>.</li>\n  </ul>\n  \n  <p>If you haven't implemented a CI pipeline (and/or don't want to yet) you could still implement the code coverage badge (per most of the guidance below), you'd just need to ensure you kicked off your <code>build.ps1</code> script manually before each time you make a commit to the project in Github.</p>\n</blockquote>\n\n<p>There are three steps you need to complete in order to implement the badge:</p>\n\n<p><strong>1. You need to modify your call of <code>Invoke-Pester</code> so that it returns a code coverage report on completion.</strong></p>\n\n<p>If you have a CI pipeline setup, you likely have a <code>build.ps1</code> or similar script that is invoked by your CI tool when a commit is made. It may be in here that you invoke Pester to run your tests, or (like me) you might make use of <a href=\"https://github.com/psake/psake\">PSake</a> which is a handy tool for organising your build in to phases.</p>\n\n<p>Within my <code>psake.ps1</code> (triggered by my <code>build.ps1</code> which just ensures the modules my pipeline depends on are installed) I have a 'Test' task where Pester is invoked. This task has now been modified as follows:  </p>\n\n<pre><code>$CodeFiles = (Get-ChildItem $ENV:BHModulePath -Recurse -Include \"*.psm1\",\"*.ps1\").FullName\n\n$Script:TestResults = Invoke-Pester -Path $ProjectRoot\\Tests -CodeCoverage $CodeFiles -PassThru -OutputFormat NUnitXml -OutputFile \"$ProjectRoot\\$TestFile\" -ExcludeTag Integration\n</code></pre>\n\n<p>First it populates a <code>$CodeFiles</code> variable via <code>Get-ChildItem</code> with all the <code>.ps1</code> and <code>.psm1</code> files that are within my module directory (therefore excluding <code>.ps1</code> files that relate to the build pipeline, as these are in the folder above the module).</p>\n\n<p>This variable is then sent to the <code>-CodeCoverage</code> parameter of <code>Invoke-Pester</code>, which takes an array of files as input with which to perform code coverage analysis against.</p>\n\n<p>I also return the result of <code>Invoke-Pester</code> to a variable (<code>$Script:TestResults</code>) as this is where the code coverage result is then stored. This variable is <code>$Script:</code> scoped so that I can access it from the next PSake task in my pipeline (the Build task).</p>\n\n<p>When you use the <code>-CodeCoverage</code> parameter, Pester outputs a report at the end which shows you exactly which lines were not covered by your tests. Use this to guide you if you want to improve your code coverage score:</p>\n\n<p><img src=\"/content/images/2018/05/Pester-Code-Coverage.png\" alt=\"PowerShell Pester Code coverage report output\" /></p>\n\n<p>For example, in the output above none of my tests result in a certain <code>Write-Error</code> command being output, so if I want to test that this works correctly I could add a Pester tests with some input data that would cause this error to occur and then have the test verify that it does as expected.</p>\n\n<p>If you have a CI pipeline, you'll also find the code coverage report in the output of your build:</p>\n\n<p><img src=\"/content/images/2018/05/Pester-Code-Coverage-AppVeyor-Output.png\" alt=\"PowerShell Pester Code coverage report output in an Appveyor build job\" /></p>\n\n<p><strong>2. You need to implement a small function that I stole (with permission) from my good friend and PowerShell aficionado <a href=\"https://sammart.in/\">Sam Martin</a>.</strong></p>\n\n<p>Within your build script you need to add the following function (by Sam) that modifies the readme.md file to update the URL of the code coverage badge with the current coverage percentage results:  </p>\n\n<pre><code>function Update-CodeCoveragePercent {  \n    [cmdletbinding(supportsshouldprocess)]\n    param(\n        [int]\n        $CodeCoverage = 0,\n\n        [string]\n        $TextFilePath = \"$Env:BHProjectPath\\Readme.md\"\n    )\n\n    $BadgeColor = switch ($CodeCoverage) {\n        {$_ -in 90..100} { 'brightgreen' }\n        {$_ -in 75..89}  { 'yellow' }\n        {$_ -in 60..74}  { 'orange' }\n        default          { 'red' }\n    }\n\n    if ($PSCmdlet.ShouldProcess($TextFilePath)) {\n        $ReadmeContent = (Get-Content $TextFilePath)\n        $ReadmeContent = $ReadmeContent -replace \"!\\[Test Coverage\\].+\\)\", \"![Test Coverage](https://img.shields.io/badge/coverage-$CodeCoverage%25-$BadgeColor.svg?maxAge=60)\" \n        $ReadmeContent | Set-Content -Path $TextFilePath\n    }\n}\n</code></pre>\n\n<p>You then make immediate use of the function as follows, by taking the earlier generated Pester results variable and using its properties to calculate the coverage percentage, which is then sent to the above function as a parameter:  </p>\n\n<pre><code>$CoveragePercent = [math]::floor(100 - (($Script:TestResults.CodeCoverage.NumberOfCommandsMissed / $Script:TestResults.CodeCoverage.NumberOfCommandsAnalyzed) * 100))\n\nUpdate-CodeCoveragePercent -CodeCoverage $CoveragePercent  \n</code></pre>\n\n<p>You also of course need to add the Code Coverage badge to your readme.md to start with (so that there's something there for the function to modify). This is made possible via <a href=\"http://shields.io\">shields.io</a>, who provide various badges that can be customised via parameters in the querystring.</p>\n\n<p>To add their code coverage badge, include this in your Readme.md:</p>\n\n<pre><code> ![Test Coverage](https://img.shields.io/badge/coverage.svg)\n</code></pre>\n\n<p>This version won’t actually work because it doesn't yet include the colour or percentage parameters, but those values will be added when the build runs.</p>\n\n<p><strong>3. You need to ensure your CI pipeline checks back in modified files after a successful run, in a way that doesn't trigger another build.</strong></p>\n\n<p>Finally (for a completely automated solution), I added an <code>on_success</code> step to my AppVeyor build pipeline that uses git to check back in the modified readme.md to the project in a way that doesn’t trigger another build (lest you wish to be stuck an infinite build loop). </p>\n\n<p>This was done by modifying my <code>appveyor.yml</code> file as follows:  </p>\n\n<pre><code>on_success:  \n  - git config --global credential.helper store\n  - ps: Add-Content \"$HOME\\.git-credentials\" \"https://$($env:GitToken):x-oauth-basic@github.com`n\"\n  - git config --global user.email \"build@appveyor.com\"\n  - git config --global user.name \"Appveyor\"\n  - git checkout %APPVEYOR_REPO_BRANCH%\n  - git add *.psd1\n  - git add *.md\n  - git commit -m \"[skip ci] AppVeyor Build %APPVEYOR_BUILD_VERSION%\"\n  - git push\n</code></pre>\n\n<p>For this to work you also need to configure a personal access token within Github to allow AppVeyor permission to write to your repository. See <a href=\"https://www.appveyor.com/docs/how-to/git-push/\">this guide from AppVeyor</a> on how to do this.</p>\n\n<blockquote>\n  <p>It's very important that you include <code>[skip ci]</code> in the commit message (as shown above). By doing so AppVeyor will not trigger another build when this commit occurs.</p>\n</blockquote>\n\n<p>You might also have noticed in the code above that I am using Git to commit any modified <code>*.md</code> and <code>*.psd1</code> files. The .md files are of course to capture the updated <code>readme.md</code>. I commit changed .psd1 files because my CI pipeline also publishes my module to the PowerShell Gallery and when doing so it increments the module version in the .psd1 file. I simply check this back in to Github so that the Git repo is consistent with what is in the gallery.</p>\n\n<hr />\n\n<p>If you've followed the steps above, you should now have a completely automated way to indicate code coverage results on your PowerShell projects, just like this:</p>\n\n<p><img src=\"/content/images/2018/05/Code-Coverage-Badge-Example.png\" alt=\"Code coverage badge example\" /></p>\n\n<p><em>-- In this example the badge is red because the coverage is less than 60%, as I specified in the <code>Update-CodeCoveragePercent</code> function. Between 75% and 89% it would be Orange, 90% and above it would be Green. Obviously you can set your own colour/score preferences.</em></p>\n\n<p>In the future you may be able to replace/augment much of the above with <a href=\"https://codecov.io\">https://codecov.io</a>, which is a code coverage reporting service for Github projects. To use CodeCov you need to be able to upload a code coverage report from which CodeCov  generates you a micro-site with detailed information about the code coverage of your project, as well as then a link to a badge you can put in your readme.md. </p>\n\n<p>I believe Pester doesn't yet generate the needed code coverage reports, but an <a href=\"https://github.com/pester/Pester/issues/212\">issue is open on Github for the feature</a>, so watch that if you want to see when it occurs. However it seems that at least one <a href=\"https://github.com/aaronpowell/ps-nvm\">PowerShell project</a> has developed a workaround for this and is successfully using CodeCov for a PowerShell project.</p>\n\n<p>In the meantime, if you just want a simple code coverage badge for your readme.md, hopefully the above was helpful. Let me know how you get on implementing it via the comments below.</p>",
                        "image": "/content/images/2018/04/code-coverage.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-04-11 10:30:15",
                        "created_by": 1,
                        "updated_at": "2018-05-14 10:46:11",
                        "updated_by": 1,
                        "published_at": "2018-05-14 10:46:11",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 73,
                        "uuid": "6e85d533-d716-4d58-bb70-03dcbe05feff",
                        "title": "Copy files with hash difference via PowerShell",
                        "slug": "a-powershell-cmdlet-to-copy-files-based-on-hash-difference",
                        "markdown": "This blog post details a PowerShell Core compatible cmdlet that I have authored named `Copy-FileHash` that you can use to copy modified files from one path tree to another. The cmdlet determines which files have different contents by calculating their hash values through the `Get-FileHash` cmdlet. This might be useful if you need to copy just files that have been modified between two paths and aren't able to rely on the modified date of those files to determine which have changed. \n\n> **What is a hash value?**\n>\n> *\"A hash value is a unique value that corresponds to the content of the file. Rather than identifying the contents of a file by its file name, extension, or other designation, a hash assigns a unique value to the contents of a file. File names and extensions can be changed without altering the content of the file, and without changing the hash value. Similarly, the file's content can be changed without changing the name or extension. However, changing even a single character in the contents of a file changes the hash value of the file.\"*\n>\n> -- Source: [Get-FileHash Official Documentation](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/get-filehash?view=powershell-6)\n\nIf you want to go directly to the code, you can find it [in GitHub here](https://github.com/markwragg/PowerShell-HashCopy), or you can [install it from the PowerShell Gallery](https://www.powershellgallery.com/packages/HashCopy/) by executing:\n```\nInstall-Module HashCopy -Scope CurrentUser\n```\nYou can then use:\n```\nGet-Help Copy-FileHash -Full\n```\nFor full details on how the cmdlet works.\n\n> `Copy-FileHash` requires Windows PowerShell version 4 or above, or PowerShell Core version 6 or above (through which the cmdlet should work on MacOS, Ubuntu or Windows). \n\nHere is an example of `Copy-FileHash` in action:\n\n![Copy-FileHash](/content/images/2018/08/Copy-FileHash-1.png)\n\nIn the above example `1.txt` was in the source path but not in the destination path so was created and then copied, `2.txt` was found to be different in the source path and so was overwritten. The file objects for the resultant modified files were returned because `-PassThru` was used.\n\n#### Why might this be useful?\n\nI recently had a situation where I needed to be able to deploy only changed files from a set of files. Those files had been put in to a .zip package, created from a git-based Source Control and ultimately deployed to a server via Octopus Deploy. It's a [built-in and necessary behaviour of Git to change the modified date of files](https://git.wiki.kernel.org/index.php/GitFaq#Why_isn.27t_Git_preserving_modification_time_on_files.3F) as it manages them. As a result, I couldn't rely on comparing the modified date of the files to know which had changed (as in general, it would behave as if all the files were newer) but I knew that the source control versions were canonical, so if any of the files in source were different to the files on the server, the source control version should be used.\n\nSince version 4, PowerShell has had a standard cmdlet for generating a hash value for a file, called `Get-FileHash`. The cmdlet was introduced primarily to help with Desired State Configuration (DSC). A Pull Server implementation of DSC will hash the configurations to determine if a change has occurred and then apply it. Per the above, as long as you're using a secure algorithm you can safely assume that if two files calculate different hash values they are different versions of the same file.\n\nMy cmdlet is relatively simple, but it has a couple of helpful features that my scenario required:\n\n- You can provide a source path and use the `-Recurse` parameter to synchronise all modified files and folders in that path with a specified destination path. \n- If there are any files in the source path that are not in the destination, the cmdlet will copy those files across, as well as create any missing sub-folders beneath those files, as needed.\n\n#### How does it work?\n\nHere's a breakdown of the code:\n\n```language-powershell\n$SourcePath = If ($PSBoundParameters.ContainsKey('LiteralPath')) {\n    (Resolve-Path -LiteralPath $LiteralPath).Path\n}\nElse {\n    (Resolve-Path -Path $Path).Path\n}\n\nIf (-Not (Test-Path $Destination)){\n    New-Item -Path $Destination -ItemType Container | Out-Null\n    Write-Warning \"$Destination did not exist and has been created as a folder path.\"\n}\n\n$Destination = Join-Path ((Resolve-Path -Path $Destination).Path) -ChildPath '/'\n\n$SourceFiles = (Get-ChildItem -Path $Source -Recurse:$Recurse -File).FullName\n```\nThis initial stage handles whether the user has elected to use the `LiteralPath` or `Path` parameters.\n\n> `-LiteralPath` is a standard parameter on a number of the built-in file handling cmdlets. By default `-Path` will interpret certain special characters as wildcard or other operators (square brackets for example). If you want to ensure your paths are handled explicitly, you should use the `-LiteralPath` parameter.\n\nIt then uses `Resolve-Path` to ensure that we have the full real path for both the source and destination paths (this proved to be necessary when Pester testing and using the `TestDrive:\\` Pester path, which was handled poorly without converting it to it's actual path in the cmdlet).\n\nIf the `-Destination` path doesn't exist, it creates it as a folder and warns the user that we've done so. It then retrieves all files from the source path, including those in sub-folders if `-Recurse` has been used.\n\n```language-powershell\nForEach ($Source in $SourcePath) {\n    $SourceFiles = (Get-ChildItem -Path $Source -Recurse:$Recurse -File).FullName\n\n    ForEach ($SourceFile in $SourceFiles) {\n        $DestFile = Join-Path (Split-Path -Parent $SourceFile) -ChildPath '/'\n        $DestFile = $DestFile -Replace \"^$([Regex]::Escape($Source))\", $Destination\n        $DestFile = Join-Path -Path $DestFile -ChildPath (Split-Path -Leaf $SourceFile)\n\n        $SourceHash = (Get-FileHash $SourceFile -Algorithm $Algorithm).hash\n\n        If (Test-Path $DestFile) {\n            $DestHash = (Get-FileHash $DestFile -Algorithm $Algorithm).hash\n        }\n        Else {\n            If ($PSCmdlet.ShouldProcess($DestFile, 'New-Item')) {\n                New-Item -Path $DestFile -Value (Get-Date).Ticks -Force | Out-Null\n            }\n            $DestHash = $null\n        }\n\n        If (($SourceHash -ne $DestHash) -and $PSCmdlet.ShouldProcess($SourceFile, 'Copy-Item')) {\n            Copy-Item -Path $SourceFile -Destination $DestFile -Force:$Force -PassThru:$PassThru\n        }\n    }\n}\n```\n\nThe second part of the script iterates through each file found in the source path via a `ForEach` loop. It determines what the equivalent destination path would be for a file by doing a regular expression replace on the source path with the destination path (note that the regex pattern includes the `^` special character that ensures it only does this replace where it occurs from the beginning of the string). We have to use the .NET regex escape method to ensure that characters in the path are not mis-interpreted as regex special characters. This part of the script is also making use of `Split-Path` and `Join-Path` to deconstruct and reconstruct the path. By doing so, we allow PowerShell to handle how paths should be constructed and therefore make this cmdlet cross-platform compatible (e.g it will work via PowerShell Core on Ubuntu).\n\nHaving got the destination file path, we then check if it exists. If it doesn't we use `New-Item -Force` to create it, which has the helpful side-effect of also creating any sub-folders missing in that files path.\n\nNow we use `Get-FileHash` to get the hash of each file (this is now possible even where the destination file didn't originally exist as `New-Item` has created it with a random value -- the reason for the random value is so that the file still gets copied in the later step, because if someone has used the scripts `-PassThru` parameter this then ensures that newly created destination files will also get returned, even where they are empty).\n\nFinally we compare the two hash results and perform a copy where the results differ.\n\nNote that the cmdlet is using `SupportsShouldProcess` so if you want to see a dry run result before you perform a copy, you can do so via the `-WhatIf` and all the destructive actions will be listed via verbose statements.\n\nAgain, if you would like to try `Copy-FileHash` out yourself, you can install it from the PowerShell Gallery via:\n```\nInstall-Module HashCopy -Scope CurrentUser\n```\nIf you'd like to review the code or you have any ideas for improvements, check out the GitHub repo [here](https://github.com/markwragg/PowerShell-HashCopy).",
                        "html": "<p>This blog post details a PowerShell Core compatible cmdlet that I have authored named <code>Copy-FileHash</code> that you can use to copy modified files from one path tree to another. The cmdlet determines which files have different contents by calculating their hash values through the <code>Get-FileHash</code> cmdlet. This might be useful if you need to copy just files that have been modified between two paths and aren't able to rely on the modified date of those files to determine which have changed. </p>\n\n<blockquote>\n  <p><strong>What is a hash value?</strong></p>\n  \n  <p><em>\"A hash value is a unique value that corresponds to the content of the file. Rather than identifying the contents of a file by its file name, extension, or other designation, a hash assigns a unique value to the contents of a file. File names and extensions can be changed without altering the content of the file, and without changing the hash value. Similarly, the file's content can be changed without changing the name or extension. However, changing even a single character in the contents of a file changes the hash value of the file.\"</em></p>\n  \n  <p>-- Source: <a href=\"https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/get-filehash?view=powershell-6\">Get-FileHash Official Documentation</a></p>\n</blockquote>\n\n<p>If you want to go directly to the code, you can find it <a href=\"https://github.com/markwragg/PowerShell-HashCopy\">in GitHub here</a>, or you can <a href=\"https://www.powershellgallery.com/packages/HashCopy/\">install it from the PowerShell Gallery</a> by executing:  </p>\n\n<pre><code>Install-Module HashCopy -Scope CurrentUser  \n</code></pre>\n\n<p>You can then use:  </p>\n\n<pre><code>Get-Help Copy-FileHash -Full  \n</code></pre>\n\n<p>For full details on how the cmdlet works.</p>\n\n<blockquote>\n  <p><code>Copy-FileHash</code> requires Windows PowerShell version 4 or above, or PowerShell Core version 6 or above (through which the cmdlet should work on MacOS, Ubuntu or Windows). </p>\n</blockquote>\n\n<p>Here is an example of <code>Copy-FileHash</code> in action:</p>\n\n<p><img src=\"/content/images/2018/08/Copy-FileHash-1.png\" alt=\"Copy-FileHash\" /></p>\n\n<p>In the above example <code>1.txt</code> was in the source path but not in the destination path so was created and then copied, <code>2.txt</code> was found to be different in the source path and so was overwritten. The file objects for the resultant modified files were returned because <code>-PassThru</code> was used.</p>\n\n<h4 id=\"whymightthisbeuseful\">Why might this be useful?</h4>\n\n<p>I recently had a situation where I needed to be able to deploy only changed files from a set of files. Those files had been put in to a .zip package, created from a git-based Source Control and ultimately deployed to a server via Octopus Deploy. It's a <a href=\"https://git.wiki.kernel.org/index.php/GitFaq#Why_isn.27t_Git_preserving_modification_time_on_files.3F\">built-in and necessary behaviour of Git to change the modified date of files</a> as it manages them. As a result, I couldn't rely on comparing the modified date of the files to know which had changed (as in general, it would behave as if all the files were newer) but I knew that the source control versions were canonical, so if any of the files in source were different to the files on the server, the source control version should be used.</p>\n\n<p>Since version 4, PowerShell has had a standard cmdlet for generating a hash value for a file, called <code>Get-FileHash</code>. The cmdlet was introduced primarily to help with Desired State Configuration (DSC). A Pull Server implementation of DSC will hash the configurations to determine if a change has occurred and then apply it. Per the above, as long as you're using a secure algorithm you can safely assume that if two files calculate different hash values they are different versions of the same file.</p>\n\n<p>My cmdlet is relatively simple, but it has a couple of helpful features that my scenario required:</p>\n\n<ul>\n<li>You can provide a source path and use the <code>-Recurse</code> parameter to synchronise all modified files and folders in that path with a specified destination path. </li>\n<li>If there are any files in the source path that are not in the destination, the cmdlet will copy those files across, as well as create any missing sub-folders beneath those files, as needed.</li>\n</ul>\n\n<h4 id=\"howdoesitwork\">How does it work?</h4>\n\n<p>Here's a breakdown of the code:</p>\n\n<pre><code class=\"language-powershell\">$SourcePath = If ($PSBoundParameters.ContainsKey('LiteralPath')) {\n    (Resolve-Path -LiteralPath $LiteralPath).Path\n}\nElse {  \n    (Resolve-Path -Path $Path).Path\n}\n\nIf (-Not (Test-Path $Destination)){  \n    New-Item -Path $Destination -ItemType Container | Out-Null\n    Write-Warning \"$Destination did not exist and has been created as a folder path.\"\n}\n\n$Destination = Join-Path ((Resolve-Path -Path $Destination).Path) -ChildPath '/'\n\n$SourceFiles = (Get-ChildItem -Path $Source -Recurse:$Recurse -File).FullName\n</code></pre>\n\n<p>This initial stage handles whether the user has elected to use the <code>LiteralPath</code> or <code>Path</code> parameters.</p>\n\n<blockquote>\n  <p><code>-LiteralPath</code> is a standard parameter on a number of the built-in file handling cmdlets. By default <code>-Path</code> will interpret certain special characters as wildcard or other operators (square brackets for example). If you want to ensure your paths are handled explicitly, you should use the <code>-LiteralPath</code> parameter.</p>\n</blockquote>\n\n<p>It then uses <code>Resolve-Path</code> to ensure that we have the full real path for both the source and destination paths (this proved to be necessary when Pester testing and using the <code>TestDrive:\\</code> Pester path, which was handled poorly without converting it to it's actual path in the cmdlet).</p>\n\n<p>If the <code>-Destination</code> path doesn't exist, it creates it as a folder and warns the user that we've done so. It then retrieves all files from the source path, including those in sub-folders if <code>-Recurse</code> has been used.</p>\n\n<pre><code class=\"language-powershell\">ForEach ($Source in $SourcePath) {  \n    $SourceFiles = (Get-ChildItem -Path $Source -Recurse:$Recurse -File).FullName\n\n    ForEach ($SourceFile in $SourceFiles) {\n        $DestFile = Join-Path (Split-Path -Parent $SourceFile) -ChildPath '/'\n        $DestFile = $DestFile -Replace \"^$([Regex]::Escape($Source))\", $Destination\n        $DestFile = Join-Path -Path $DestFile -ChildPath (Split-Path -Leaf $SourceFile)\n\n        $SourceHash = (Get-FileHash $SourceFile -Algorithm $Algorithm).hash\n\n        If (Test-Path $DestFile) {\n            $DestHash = (Get-FileHash $DestFile -Algorithm $Algorithm).hash\n        }\n        Else {\n            If ($PSCmdlet.ShouldProcess($DestFile, 'New-Item')) {\n                New-Item -Path $DestFile -Value (Get-Date).Ticks -Force | Out-Null\n            }\n            $DestHash = $null\n        }\n\n        If (($SourceHash -ne $DestHash) -and $PSCmdlet.ShouldProcess($SourceFile, 'Copy-Item')) {\n            Copy-Item -Path $SourceFile -Destination $DestFile -Force:$Force -PassThru:$PassThru\n        }\n    }\n}\n</code></pre>\n\n<p>The second part of the script iterates through each file found in the source path via a <code>ForEach</code> loop. It determines what the equivalent destination path would be for a file by doing a regular expression replace on the source path with the destination path (note that the regex pattern includes the <code>^</code> special character that ensures it only does this replace where it occurs from the beginning of the string). We have to use the .NET regex escape method to ensure that characters in the path are not mis-interpreted as regex special characters. This part of the script is also making use of <code>Split-Path</code> and <code>Join-Path</code> to deconstruct and reconstruct the path. By doing so, we allow PowerShell to handle how paths should be constructed and therefore make this cmdlet cross-platform compatible (e.g it will work via PowerShell Core on Ubuntu).</p>\n\n<p>Having got the destination file path, we then check if it exists. If it doesn't we use <code>New-Item -Force</code> to create it, which has the helpful side-effect of also creating any sub-folders missing in that files path.</p>\n\n<p>Now we use <code>Get-FileHash</code> to get the hash of each file (this is now possible even where the destination file didn't originally exist as <code>New-Item</code> has created it with a random value -- the reason for the random value is so that the file still gets copied in the later step, because if someone has used the scripts <code>-PassThru</code> parameter this then ensures that newly created destination files will also get returned, even where they are empty).</p>\n\n<p>Finally we compare the two hash results and perform a copy where the results differ.</p>\n\n<p>Note that the cmdlet is using <code>SupportsShouldProcess</code> so if you want to see a dry run result before you perform a copy, you can do so via the <code>-WhatIf</code> and all the destructive actions will be listed via verbose statements.</p>\n\n<p>Again, if you would like to try <code>Copy-FileHash</code> out yourself, you can install it from the PowerShell Gallery via:  </p>\n\n<pre><code>Install-Module HashCopy -Scope CurrentUser  \n</code></pre>\n\n<p>If you'd like to review the code or you have any ideas for improvements, check out the GitHub repo <a href=\"https://github.com/markwragg/PowerShell-HashCopy\">here</a>.</p>",
                        "image": "/content/images/2018/08/apple-1868383_1920.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": "",
                        "meta_description": "Copy-FileHash is a PowerShell cmdlet to enable file copies between directory trees for any files where the Get-FileHash calculated hash value differs.",
                        "author_id": 1,
                        "created_at": "2018-08-06 18:54:29",
                        "created_by": 1,
                        "updated_at": "2018-08-08 13:23:43",
                        "updated_by": 1,
                        "published_at": "2018-08-08 13:23:43",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 74,
                        "uuid": "59fcf286-707e-4c6b-a908-ee439a813bec",
                        "title": "Testing a PowerShell Core module on Ubuntu and Windows with AppVeyor",
                        "slug": "testing-a-cross-platform-powershell-module-on-ubuntu-and-windows",
                        "markdown": "This blog post details how you can configure the Continuous Integration tool AppVeyor to test one of your PowerShell projects on both a Windows and Ubuntu VM each time you make a commit. This is obviously particularly useful if you're authoring a cross-platform PowerShell Core module and want to be certain that the module functions successfully across multiple platforms each time you make a commit.\n\nI recently authored a module called HashCopy, which I talked about in my previous blog post. As part of authoring the module, I realised that there was very little in the code that would stop it from working under PowerShell Core and on any of the supported operating systems. My module primarily interacts with the file system, so the only thing I had to do was make sure that the module handled file paths in a generic way. Doing this meant using the `Resolve-Path`, `Split-Path` and `Join-Path` cmdlets as part of handling file paths so that responsibility for constructing those paths correctly was left to PowerShell (vs me hard-coding in a backslash into a path that would be invalid on a non-Windows system, for example).\n\nOnce I'd made those changes I obviously wanted to verify that the module would work correctly on a non-Windows system. I (ideally) didn't want to have to spin up my own non-Windows VM for testing and I recalled seeing a while back that AppVeyor was going to start providing [Linux builds](https://www.appveyor.com/docs/getting-started-with-appveyor-for-linux/).\n\n> If you're not currently using AppVeyor as a Continuous Integration (CI) tool for your PowerShell modules, have a look at one of these guides for how you can start to leverage this functionality:\n>\n> - ..\n> - ..\n>\n> Setting up CI is of great value in combination with ensuring your PowerShell modules/code are paired with a detailed set of Pester tests, as you can then have the CI run those tests automatically each time a new commit is made, informing you as soon as possible if you've introduced any new functionality that has caused issues.\n\n\n\n\n",
                        "html": "<p>This blog post details how you can configure the Continuous Integration tool AppVeyor to test one of your PowerShell projects on both a Windows and Ubuntu VM each time you make a commit. This is obviously particularly useful if you're authoring a cross-platform PowerShell Core module and want to be certain that the module functions successfully across multiple platforms each time you make a commit.</p>\n\n<p>I recently authored a module called HashCopy, which I talked about in my previous blog post. As part of authoring the module, I realised that there was very little in the code that would stop it from working under PowerShell Core and on any of the supported operating systems. My module primarily interacts with the file system, so the only thing I had to do was make sure that the module handled file paths in a generic way. Doing this meant using the <code>Resolve-Path</code>, <code>Split-Path</code> and <code>Join-Path</code> cmdlets as part of handling file paths so that responsibility for constructing those paths correctly was left to PowerShell (vs me hard-coding in a backslash into a path that would be invalid on a non-Windows system, for example).</p>\n\n<p>Once I'd made those changes I obviously wanted to verify that the module would work correctly on a non-Windows system. I (ideally) didn't want to have to spin up my own non-Windows VM for testing and I recalled seeing a while back that AppVeyor was going to start providing <a href=\"https://www.appveyor.com/docs/getting-started-with-appveyor-for-linux/\">Linux builds</a>.</p>\n\n<blockquote>\n  <p>If you're not currently using AppVeyor as a Continuous Integration (CI) tool for your PowerShell modules, have a look at one of these guides for how you can start to leverage this functionality:</p>\n  \n  <ul>\n  <li>..</li>\n  <li>..</li>\n  </ul>\n  \n  <p>Setting up CI is of great value in combination with ensuring your PowerShell modules/code are paired with a detailed set of Pester tests, as you can then have the CI run those tests automatically each time a new commit is made, informing you as soon as possible if you've introduced any new functionality that has caused issues.</p>\n</blockquote>",
                        "image": "/content/images/2018/08/ubuntu_windows_7_style-widescreen_wallpapers.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-08-11 16:33:13",
                        "created_by": 1,
                        "updated_at": "2018-08-13 23:38:17",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 75,
                        "uuid": "59b67c57-b42d-4149-822d-477ccef5255f",
                        "title": "Deploying Influx and Grafana on Windows with Terraform",
                        "slug": "deploying-an-influx-and-grafana-metrics-server-on-windows",
                        "markdown": "I have previously [blogged about how you can use Influx, Grafana and PowerShell to build and populate metrics dashboards](http://wragg.io/windows-based-grafana-analytics-platform-via-influxdb-and-powershell/) for visualising data from multiple sources. This blog post details how you can use Terraform to quickly and easily deploy Grafana and Influx on a Windows instance in AWS as a proof of concept implementation of these tools. \n\nIf you'd like to go directly to the code, you can find it here: https://github.com/markwragg/Terraform-MetricStack\n\n> **What is Terraform?**\n>\n> [Terraform](https://www.terraform.io/) is a tool that you can use to define infrastructure as code. You use one or more [providers](https://www.terraform.io/docs/providers/) to define the resources (infrastructure components) that you want to be deployed and then Terraform makes the required changes in those services (such as AWS) to implement your infrastructure. Terraform then records the state of the infrastructure, so any further changes you want to make to your infrastructure via Terraform are made through minimal changes. By using a tool like Terraform you can automate and version control your infrastructure.\n\nBefore we look at the code, it's worth noting the following caveats:\n\n- I describe this as a \"proof of concept implementation\" because if you want to use these tools in production you will likely want to implement them in a way that is more scalable and resilient. For example, [Influx provide their own Terraform module for deploying InfluxDB](https://github.com/influxdata/terraform-aws-influxdb) across multiple nodes.\n- Both Influx and Grafana can be installed on a Unix OS and it's likely more sensible to do so from a performance and cost perspective. I chose to implement it on Windows in this instance mostly to see how complex it would be.\n- The deployment code does some minimal configuration of Grafana and Influx. To do more detailed configuration you could RDP to the server and make further changes, or look to implement a configuration management tool to manage the configuration files more fully.\n-  If you have an AWS account with Free Tier then the defaults defined in my Terraform code should not incur you any AWS costs. If you do not have Free Tier then minimal costs may apply.\n\n## Getting Started\n\n- If you don't have an AWS account, [you can sign up for one free here](https://portal.aws.amazon.com/billing/signup).\n- You need to [create an access key in AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey) to grant Terraform permissions to your account.\n- You need to configure the access key locally. I recommend [installing the AWS CLI tools](https://aws.amazon.com/cli/) and then doing `aws configure` to set your credentials. You then don't need to explicitly add your access key to the Terraform code, Terraform will discover it automatically.\n- You need to [install Terraform](https://www.terraform.io/intro/getting-started/install.html). Terraform is a single executable, so its just a case of downloading the appropriate package for your system and putting it somewhere accessible (like one of the default PATH directories, or in a directory that you then add to PATH).\n\nTo use my code you then simply need to:\n\n- [Clone the repository](https://github.com/markwragg/Terraform-MetricStack) to your local machine.\n- Open a terminal/shell and change to the directory with the code.\n- Run `Terraform Init`. This will download any plugins required by the code (and you only need to do this the first time).\n- (Optionally) run `Terraform Plan` to view what configuration changes the code will make.\n- Run `Terraform Apply` to see the Plan again and then enter \"yes\" to start the deployment.\n- Terraform will also prompt you to provide a value for `admin_password`. Whatever you define will be used to set the password for the local admin account on the Windows server so that you can RDP to it if you want to.\n\n![Terraform Apply Influx Grafana Windows Server in AWS](/content/images/2018/10/Terraform-apply-influx-grafa.png)\n\nIt will likely take just a few minutes to complete the deployment, at the end of which it will output the URL for Grafana and the URL for writing metrics to Influx. However, (because we're using a T2.micro instance by default) you need to wait a bit longer before Grafana and Influx are ready as the server will still be downloading, installing and configuring them. **Usually within about 20 minutes the Grafana URL will respond and not long after that Influx should also be working.**\n\nOnce the Grafana URL is working, you should get to a login page and initial credentials are the defaults (at time of writing admin / admin):\n\n![Grafana](/content/images/2018/10/Grafana.png)\n\n> Have a look at [my previous blog post](http://wragg.io/windows-based-grafana-analytics-platform-via-influxdb-and-powershell/) for how you can now write some metrics into Influx (using my [Influx PowerShell module](https://github.com/markwragg/PowerShell-Influx) if you want to) and visualise them with Grafana. \n>\n> You can skip over the initial parts of the blog post which cover installing Influx and Grafana, as Terraform has just done that for you :).\n\n## The Code\n\nThe main Terraform code lives in `main.tf`. It starts by defining that we want to use the AWS provider in order to manage AWS resources, and specify the region we want to deploy to via a variable:\n\n```\nprovider \"aws\" {\n  region = \"${var.aws_region}\"\n}\n```\n\nThen we define a data source which loads the `user_data` we need to send to our AWS instance from a separate file. You can define the `user_data` within the resource directly, but having it in a template file is neater. The `user_data` is scripts that we want to run on the server after its deployed in order to configure it. There are a number of parts of the script that can be modified via the code's variables (which I'll cover shortly), so to make use of these we need to provide a block of them via `vars`:\n```\ndata \"template_file\" \"metricserver\" {\n  template = \"${file(\"metricserver.tpl\")}\"\n\n  vars {\n    admin_password      = \"${var.admin_password}\"\n    grafana_port        = \"${var.grafana_port}\"\n    grafana_version     = \"${var.grafana_version}\"\n    influx_port         = \"${var.influx_port}\"\n    influx_database     = \"${var.influx_database}\"\n    influx_version      = \"${var.influx_version}\"\n    influx_udp_port     = \"${var.influx_udp_port}\"\n    influx_udp_database = \"${var.influx_udp_database}\"\n  }\n}\n```\nThis next data source is used to get the AMI (Amazon Machine Image) ID from AWS. New AMIs are released all the time (with that latest patches etc.) so by using this data source we ensure we are always using the latest one. This data source also makes use of a variable in which we specify a partial AMI name. By default this is Windows 2016 Core:\n```\ndata \"aws_ami\" \"windows\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"${var.windows_ami_filter}\"]\n  }\n}\n```\nNow we start to declare our resources. The first is an `aws_instance` and you can see that it's referencing the data source for it's AMI. We load the instance_type from a variable (T2.micro by default) and then we have the instance associated with a `security_group` which is defined lower down. Note how the resources don't need to be written in any particular order, Terraform will work out the dependencies itself and deploy things in the correct order:\n```\nresource \"aws_instance\" \"metricserver\" {\n  ami             = \"${data.aws_ami.windows.id}\"\n  instance_type   = \"${var.instance_type}\"\n  security_groups = [\"${aws_security_group.metricserver_inbound.name}\"]\n\n  user_data = \"${data.template_file.metricserver.rendered}\"\n}\n```\nNext is the `aws_security_group` resource definition, which is very basic as we're defining the rules as separate resources below it:\n```\nresource \"aws_security_group\" \"metricserver_inbound\" {\n  name        = \"metricserver_inbound\"\n  description = \"Allow inbound traffic to metricserver\"\n}\n```\nAnd here are the `aws_security_group_rule` resources. You can see that each is linked to the security group we defined. Some of the ports are also defined via variables (so someone can customise them if they want to) as well as the inbound CIDR blocks, which is ANY/Internet by default but someone might want to restrict access. Note also how some have a `count` parameter. This allows these rules to be enabled or disabled via one of the variables:\n```\nresource \"aws_security_group_rule\" \"influx\" {\n  type              = \"ingress\"\n  from_port         = \"${var.influx_port}\"\n  to_port           = \"${var.influx_port}\"\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"grafana\" {\n  type              = \"ingress\"\n  from_port         = \"${var.grafana_port}\"\n  to_port           = \"${var.grafana_port}\"\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"rdp\" {\n  count             = \"${var.enable_rdp ? 1 : 0}\"\n  type              = \"ingress\"\n  from_port         = 3389\n  to_port           = 3389\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"udp_listener\" {\n  count             = \"${var.enable_udp_listener ? 1 : 0}\"\n  type              = \"ingress\"\n  from_port         = \"${var.influx_udp_port}\"\n  to_port           = \"${var.influx_udp_port}\"\n  protocol          = \"udp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"default_egress\" {\n  type              = \"egress\"\n  from_port         = 0\n  to_port           = 0\n  protocol          = -1\n  cidr_blocks       = [\"0.0.0.0/0\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n```\nThat's all the Terraform code. As mentioned, there are lots of variables in this code and those are all defined in `variables.tf`. You seemingly don't have to put them in a separate file, but it is a common practice and doing so I think makes it easier for someone using the code to see what they can easily configure. You don't have to reference this file anywhere, when Terraform runs it processes all .tf files in the current directory.\n\nVariables in Terraform can have a default, or they can be defined without any value in which case it will prompt for one when you do `terraform apply`. Here's my `variables.tf` file:\n```\nvariable \"admin_password\" {}\n\nvariable \"aws_region\" {\n  default = \"us-west-2\"\n}\n\nvariable \"windows_ami_filter\" {\n  default = \"Windows_Server-2016-English-Core-Base*\"\n}\n\nvariable \"instance_type\" {\n  default = \"t2.micro\"\n}\n\nvariable \"grafana_port\" {\n  default = 8080\n}\n\nvariable \"influx_port\" {\n  default = 8086\n}\n\nvariable \"influx_database\" {\n  default = \"metrics\"\n}\n\nvariable \"inbound_cidr_blocks\" {\n  default = [\"0.0.0.0/0\"]\n}\n\nvariable \"grafana_version\" {\n  default = \"5.3.2\"\n}\n\nvariable \"influx_version\" {\n  default = \"1.6.4\"\n}\n\nvariable \"influx_udp_port\" {\n  default = 8089\n}\n\nvariable \"influx_udp_database\" {\n  default = \"udp\"\n}\n\nvariable \"enable_rdp\" {\n  default = false\n}\n\nvariable \"enable_udp_listener\" {\n  default = true\n}\n```\nWhile you could just modify `variables.tf` to change these, a better option is to create a `terraform.tfvars` file. Again, Terraform will discover this automatically where it exists and any variables you define in it will populate variables or override the default values. For example, If I put this in a `terraform.tfvars` file:\n```\nadmin_password = \"MyPassword1234!\ngrafana_port = 80\nenable_rdp = true\n```\nIt won't have to prompt me for an admin_password, it would use Port 80 for Grafana and it would add a AWS security rule that allows me to RDP to the server (disabled by default).\n\nNote that if you'd already done a deployment with the defaults, then decided to set the above variables via a `terraform.tfvars` file and ran `Terraform Apply`, Terraform would make minimal changes to the AWS security group/rules (to enable RDP), but it would also show you that it has to destroy and recreate the AWS instance (it will say \"forces new resource\") because in order to change the admin password and Grafana port on the server it knows it has to change the `user_data` which is only applied when the resource is first deployed:\n\n![Terraform Apply variable changes](/content/images/2018/10/Terraform-Apply-Changes.png)\n\nIt's for this reason that having a configuration management tool to manage the server settings (the setup of which you could bootstrap via `user_data`) is often a better option as you can then make application or other host-level config changes without entirely rebuilding the server.\n\n> **Keep sensitive config out of source control**\n>\n> If you make use of a `terraform.tfvars` file, you might populate it with sensitive settings (like credentials) that you don't want to exist anywhere else but locally. If you check your code into source control, you could use a `.gitignore` file to ensure files like this aren't committed. \n>\n> There are other Terraform files that you might not want to bother checking in to source control and you can find a [template for a standard .gitignore for Terraform here](https://github.com/github/gitignore/blob/master/Terraform.gitignore).\n\nThe final terraform file in my code is `outputs.tf`. This simply defines what values we want to return to the user after they execute `Terraform Apply` and is how we output the Grafana and Influx URLs. You can see how those are built from multiple variables, so if the user overrides things like the default ports, the output will still be correct. Its also getting the Public IP from the AWS Instance, which means we can discover this without having to go into the AWS console to find it:\n```\noutput \"grafana_url\" {\n  value = \"http://${aws_instance.metricserver.public_ip}:${var.grafana_port}\"\n}\n\noutput \"influx_url\" {\n  value = \"http://${aws_instance.metricserver.public_ip}:${var.influx_port}\"\n}\n```\nThe only other file that makes up this solution is the `metricserver.tpl` file that contains the `user_data` script executed when the instance is first deployed. I won't cover this in detail but you can [view it here](https://github.com/markwragg/Terraform-MetricStack/blob/master/metricserver.tpl). It does the following:\n\n- Installs Chocolatey.\n- Sets the Local Administrator account password to whatever you define for the `admin_password` variable.\n- Uses chocolatey to install nssm (the Non-Sucking Service Manager) so that we can run Influx and Grafana as services.\n- Downloads and extracts Grafana, then modifies its config file so its running on whatever port is defined via the variable.\n- Downloads and extracts Influx, then modifies its config as needed. Optionally adds a UDP listener to Influx if we've enabled it via the variable (it is enabled by default).\n- Adds local Windows Firewall rules for the Influx and Grafana ports.\n- Installs Influx and Grafana as a service and starts them.\n\n## Summary\n\nFor me, this has been an interesting and illuminating first attempt at Terraform. I personally quite like the way the code is structured and it's not too difficult to read a `.tf` file and understand what infrastructure is being defined. By using variables you can make it easier for others to set or override default settings which makes the code more reusable. However, there are some interesting complexities (for example: making some of the security group rules optional based on boolean variables took a while to figure out as there's no way to do it if you define the rules within the security group resource vs as separate resources).\n\nTerraform is still undergoing a lot of change and it's intentionally not reached version 1.0 yet. While you can find a lot of guidance online, because of the rate of change it does tend to go out of date quickly. Overall though I think Terraform is a valuable and impressive tool and I'm looking forward to doing more with it.",
                        "html": "<p>I have previously <a href=\"http://wragg.io/windows-based-grafana-analytics-platform-via-influxdb-and-powershell/\">blogged about how you can use Influx, Grafana and PowerShell to build and populate metrics dashboards</a> for visualising data from multiple sources. This blog post details how you can use Terraform to quickly and easily deploy Grafana and Influx on a Windows instance in AWS as a proof of concept implementation of these tools. </p>\n\n<p>If you'd like to go directly to the code, you can find it here: <a href=\"https://github.com/markwragg/Terraform-MetricStack\">https://github.com/markwragg/Terraform-MetricStack</a></p>\n\n<blockquote>\n  <p><strong>What is Terraform?</strong></p>\n  \n  <p><a href=\"https://www.terraform.io/\">Terraform</a> is a tool that you can use to define infrastructure as code. You use one or more <a href=\"https://www.terraform.io/docs/providers/\">providers</a> to define the resources (infrastructure components) that you want to be deployed and then Terraform makes the required changes in those services (such as AWS) to implement your infrastructure. Terraform then records the state of the infrastructure, so any further changes you want to make to your infrastructure via Terraform are made through minimal changes. By using a tool like Terraform you can automate and version control your infrastructure.</p>\n</blockquote>\n\n<p>Before we look at the code, it's worth noting the following caveats:</p>\n\n<ul>\n<li>I describe this as a \"proof of concept implementation\" because if you want to use these tools in production you will likely want to implement them in a way that is more scalable and resilient. For example, <a href=\"https://github.com/influxdata/terraform-aws-influxdb\">Influx provide their own Terraform module for deploying InfluxDB</a> across multiple nodes.</li>\n<li>Both Influx and Grafana can be installed on a Unix OS and it's likely more sensible to do so from a performance and cost perspective. I chose to implement it on Windows in this instance mostly to see how complex it would be.</li>\n<li>The deployment code does some minimal configuration of Grafana and Influx. To do more detailed configuration you could RDP to the server and make further changes, or look to implement a configuration management tool to manage the configuration files more fully.</li>\n<li>If you have an AWS account with Free Tier then the defaults defined in my Terraform code should not incur you any AWS costs. If you do not have Free Tier then minimal costs may apply.</li>\n</ul>\n\n<h2 id=\"gettingstarted\">Getting Started</h2>\n\n<ul>\n<li>If you don't have an AWS account, <a href=\"https://portal.aws.amazon.com/billing/signup\">you can sign up for one free here</a>.</li>\n<li>You need to <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey\">create an access key in AWS</a> to grant Terraform permissions to your account.</li>\n<li>You need to configure the access key locally. I recommend <a href=\"https://aws.amazon.com/cli/\">installing the AWS CLI tools</a> and then doing <code>aws configure</code> to set your credentials. You then don't need to explicitly add your access key to the Terraform code, Terraform will discover it automatically.</li>\n<li>You need to <a href=\"https://www.terraform.io/intro/getting-started/install.html\">install Terraform</a>. Terraform is a single executable, so its just a case of downloading the appropriate package for your system and putting it somewhere accessible (like one of the default PATH directories, or in a directory that you then add to PATH).</li>\n</ul>\n\n<p>To use my code you then simply need to:</p>\n\n<ul>\n<li><a href=\"https://github.com/markwragg/Terraform-MetricStack\">Clone the repository</a> to your local machine.</li>\n<li>Open a terminal/shell and change to the directory with the code.</li>\n<li>Run <code>Terraform Init</code>. This will download any plugins required by the code (and you only need to do this the first time).</li>\n<li>(Optionally) run <code>Terraform Plan</code> to view what configuration changes the code will make.</li>\n<li>Run <code>Terraform Apply</code> to see the Plan again and then enter \"yes\" to start the deployment.</li>\n<li>Terraform will also prompt you to provide a value for <code>admin_password</code>. Whatever you define will be used to set the password for the local admin account on the Windows server so that you can RDP to it if you want to.</li>\n</ul>\n\n<p><img src=\"/content/images/2018/10/Terraform-apply-influx-grafa.png\" alt=\"Terraform Apply Influx Grafana Windows Server in AWS\" /></p>\n\n<p>It will likely take just a few minutes to complete the deployment, at the end of which it will output the URL for Grafana and the URL for writing metrics to Influx. However, (because we're using a T2.micro instance by default) you need to wait a bit longer before Grafana and Influx are ready as the server will still be downloading, installing and configuring them. <strong>Usually within about 20 minutes the Grafana URL will respond and not long after that Influx should also be working.</strong></p>\n\n<p>Once the Grafana URL is working, you should get to a login page and initial credentials are the defaults (at time of writing admin / admin):</p>\n\n<p><img src=\"/content/images/2018/10/Grafana.png\" alt=\"Grafana\" /></p>\n\n<blockquote>\n  <p>Have a look at <a href=\"http://wragg.io/windows-based-grafana-analytics-platform-via-influxdb-and-powershell/\">my previous blog post</a> for how you can now write some metrics into Influx (using my <a href=\"https://github.com/markwragg/PowerShell-Influx\">Influx PowerShell module</a> if you want to) and visualise them with Grafana. </p>\n  \n  <p>You can skip over the initial parts of the blog post which cover installing Influx and Grafana, as Terraform has just done that for you :).</p>\n</blockquote>\n\n<h2 id=\"thecode\">The Code</h2>\n\n<p>The main Terraform code lives in <code>main.tf</code>. It starts by defining that we want to use the AWS provider in order to manage AWS resources, and specify the region we want to deploy to via a variable:</p>\n\n<pre><code>provider \"aws\" {  \n  region = \"${var.aws_region}\"\n}\n</code></pre>\n\n<p>Then we define a data source which loads the <code>user_data</code> we need to send to our AWS instance from a separate file. You can define the <code>user_data</code> within the resource directly, but having it in a template file is neater. The <code>user_data</code> is scripts that we want to run on the server after its deployed in order to configure it. There are a number of parts of the script that can be modified via the code's variables (which I'll cover shortly), so to make use of these we need to provide a block of them via <code>vars</code>:  </p>\n\n<pre><code>data \"template_file\" \"metricserver\" {  \n  template = \"${file(\"metricserver.tpl\")}\"\n\n  vars {\n    admin_password      = \"${var.admin_password}\"\n    grafana_port        = \"${var.grafana_port}\"\n    grafana_version     = \"${var.grafana_version}\"\n    influx_port         = \"${var.influx_port}\"\n    influx_database     = \"${var.influx_database}\"\n    influx_version      = \"${var.influx_version}\"\n    influx_udp_port     = \"${var.influx_udp_port}\"\n    influx_udp_database = \"${var.influx_udp_database}\"\n  }\n}\n</code></pre>\n\n<p>This next data source is used to get the AMI (Amazon Machine Image) ID from AWS. New AMIs are released all the time (with that latest patches etc.) so by using this data source we ensure we are always using the latest one. This data source also makes use of a variable in which we specify a partial AMI name. By default this is Windows 2016 Core:  </p>\n\n<pre><code>data \"aws_ami\" \"windows\" {  \n  most_recent = true\n  owners      = [\"amazon\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"${var.windows_ami_filter}\"]\n  }\n}\n</code></pre>\n\n<p>Now we start to declare our resources. The first is an <code>aws_instance</code> and you can see that it's referencing the data source for it's AMI. We load the instance_type from a variable (T2.micro by default) and then we have the instance associated with a <code>security_group</code> which is defined lower down. Note how the resources don't need to be written in any particular order, Terraform will work out the dependencies itself and deploy things in the correct order:  </p>\n\n<pre><code>resource \"aws_instance\" \"metricserver\" {  \n  ami             = \"${data.aws_ami.windows.id}\"\n  instance_type   = \"${var.instance_type}\"\n  security_groups = [\"${aws_security_group.metricserver_inbound.name}\"]\n\n  user_data = \"${data.template_file.metricserver.rendered}\"\n}\n</code></pre>\n\n<p>Next is the <code>aws_security_group</code> resource definition, which is very basic as we're defining the rules as separate resources below it:  </p>\n\n<pre><code>resource \"aws_security_group\" \"metricserver_inbound\" {  \n  name        = \"metricserver_inbound\"\n  description = \"Allow inbound traffic to metricserver\"\n}\n</code></pre>\n\n<p>And here are the <code>aws_security_group_rule</code> resources. You can see that each is linked to the security group we defined. Some of the ports are also defined via variables (so someone can customise them if they want to) as well as the inbound CIDR blocks, which is ANY/Internet by default but someone might want to restrict access. Note also how some have a <code>count</code> parameter. This allows these rules to be enabled or disabled via one of the variables:  </p>\n\n<pre><code>resource \"aws_security_group_rule\" \"influx\" {  \n  type              = \"ingress\"\n  from_port         = \"${var.influx_port}\"\n  to_port           = \"${var.influx_port}\"\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"grafana\" {  \n  type              = \"ingress\"\n  from_port         = \"${var.grafana_port}\"\n  to_port           = \"${var.grafana_port}\"\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"rdp\" {  \n  count             = \"${var.enable_rdp ? 1 : 0}\"\n  type              = \"ingress\"\n  from_port         = 3389\n  to_port           = 3389\n  protocol          = \"tcp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"udp_listener\" {  \n  count             = \"${var.enable_udp_listener ? 1 : 0}\"\n  type              = \"ingress\"\n  from_port         = \"${var.influx_udp_port}\"\n  to_port           = \"${var.influx_udp_port}\"\n  protocol          = \"udp\"\n  cidr_blocks       = [\"${var.inbound_cidr_blocks}\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n\nresource \"aws_security_group_rule\" \"default_egress\" {  \n  type              = \"egress\"\n  from_port         = 0\n  to_port           = 0\n  protocol          = -1\n  cidr_blocks       = [\"0.0.0.0/0\"]\n  security_group_id = \"${aws_security_group.metricserver_inbound.id}\"\n}\n</code></pre>\n\n<p>That's all the Terraform code. As mentioned, there are lots of variables in this code and those are all defined in <code>variables.tf</code>. You seemingly don't have to put them in a separate file, but it is a common practice and doing so I think makes it easier for someone using the code to see what they can easily configure. You don't have to reference this file anywhere, when Terraform runs it processes all .tf files in the current directory.</p>\n\n<p>Variables in Terraform can have a default, or they can be defined without any value in which case it will prompt for one when you do <code>terraform apply</code>. Here's my <code>variables.tf</code> file:  </p>\n\n<pre><code>variable \"admin_password\" {}\n\nvariable \"aws_region\" {  \n  default = \"us-west-2\"\n}\n\nvariable \"windows_ami_filter\" {  \n  default = \"Windows_Server-2016-English-Core-Base*\"\n}\n\nvariable \"instance_type\" {  \n  default = \"t2.micro\"\n}\n\nvariable \"grafana_port\" {  \n  default = 8080\n}\n\nvariable \"influx_port\" {  \n  default = 8086\n}\n\nvariable \"influx_database\" {  \n  default = \"metrics\"\n}\n\nvariable \"inbound_cidr_blocks\" {  \n  default = [\"0.0.0.0/0\"]\n}\n\nvariable \"grafana_version\" {  \n  default = \"5.3.2\"\n}\n\nvariable \"influx_version\" {  \n  default = \"1.6.4\"\n}\n\nvariable \"influx_udp_port\" {  \n  default = 8089\n}\n\nvariable \"influx_udp_database\" {  \n  default = \"udp\"\n}\n\nvariable \"enable_rdp\" {  \n  default = false\n}\n\nvariable \"enable_udp_listener\" {  \n  default = true\n}\n</code></pre>\n\n<p>While you could just modify <code>variables.tf</code> to change these, a better option is to create a <code>terraform.tfvars</code> file. Again, Terraform will discover this automatically where it exists and any variables you define in it will populate variables or override the default values. For example, If I put this in a <code>terraform.tfvars</code> file:  </p>\n\n<pre><code>admin_password = \"MyPassword1234!  \ngrafana_port = 80  \nenable_rdp = true  \n</code></pre>\n\n<p>It won't have to prompt me for an admin_password, it would use Port 80 for Grafana and it would add a AWS security rule that allows me to RDP to the server (disabled by default).</p>\n\n<p>Note that if you'd already done a deployment with the defaults, then decided to set the above variables via a <code>terraform.tfvars</code> file and ran <code>Terraform Apply</code>, Terraform would make minimal changes to the AWS security group/rules (to enable RDP), but it would also show you that it has to destroy and recreate the AWS instance (it will say \"forces new resource\") because in order to change the admin password and Grafana port on the server it knows it has to change the <code>user_data</code> which is only applied when the resource is first deployed:</p>\n\n<p><img src=\"/content/images/2018/10/Terraform-Apply-Changes.png\" alt=\"Terraform Apply variable changes\" /></p>\n\n<p>It's for this reason that having a configuration management tool to manage the server settings (the setup of which you could bootstrap via <code>user_data</code>) is often a better option as you can then make application or other host-level config changes without entirely rebuilding the server.</p>\n\n<blockquote>\n  <p><strong>Keep sensitive config out of source control</strong></p>\n  \n  <p>If you make use of a <code>terraform.tfvars</code> file, you might populate it with sensitive settings (like credentials) that you don't want to exist anywhere else but locally. If you check your code into source control, you could use a <code>.gitignore</code> file to ensure files like this aren't committed. </p>\n  \n  <p>There are other Terraform files that you might not want to bother checking in to source control and you can find a <a href=\"https://github.com/github/gitignore/blob/master/Terraform.gitignore\">template for a standard .gitignore for Terraform here</a>.</p>\n</blockquote>\n\n<p>The final terraform file in my code is <code>outputs.tf</code>. This simply defines what values we want to return to the user after they execute <code>Terraform Apply</code> and is how we output the Grafana and Influx URLs. You can see how those are built from multiple variables, so if the user overrides things like the default ports, the output will still be correct. Its also getting the Public IP from the AWS Instance, which means we can discover this without having to go into the AWS console to find it:  </p>\n\n<pre><code>output \"grafana_url\" {  \n  value = \"http://${aws_instance.metricserver.public_ip}:${var.grafana_port}\"\n}\n\noutput \"influx_url\" {  \n  value = \"http://${aws_instance.metricserver.public_ip}:${var.influx_port}\"\n}\n</code></pre>\n\n<p>The only other file that makes up this solution is the <code>metricserver.tpl</code> file that contains the <code>user_data</code> script executed when the instance is first deployed. I won't cover this in detail but you can <a href=\"https://github.com/markwragg/Terraform-MetricStack/blob/master/metricserver.tpl\">view it here</a>. It does the following:</p>\n\n<ul>\n<li>Installs Chocolatey.</li>\n<li>Sets the Local Administrator account password to whatever you define for the <code>admin_password</code> variable.</li>\n<li>Uses chocolatey to install nssm (the Non-Sucking Service Manager) so that we can run Influx and Grafana as services.</li>\n<li>Downloads and extracts Grafana, then modifies its config file so its running on whatever port is defined via the variable.</li>\n<li>Downloads and extracts Influx, then modifies its config as needed. Optionally adds a UDP listener to Influx if we've enabled it via the variable (it is enabled by default).</li>\n<li>Adds local Windows Firewall rules for the Influx and Grafana ports.</li>\n<li>Installs Influx and Grafana as a service and starts them.</li>\n</ul>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>For me, this has been an interesting and illuminating first attempt at Terraform. I personally quite like the way the code is structured and it's not too difficult to read a <code>.tf</code> file and understand what infrastructure is being defined. By using variables you can make it easier for others to set or override default settings which makes the code more reusable. However, there are some interesting complexities (for example: making some of the security group rules optional based on boolean variables took a while to figure out as there's no way to do it if you define the rules within the security group resource vs as separate resources).</p>\n\n<p>Terraform is still undergoing a lot of change and it's intentionally not reached version 1.0 yet. While you can find a lot of guidance online, because of the rate of change it does tend to go out of date quickly. Overall though I think Terraform is a valuable and impressive tool and I'm looking forward to doing more with it.</p>",
                        "image": "/content/images/2018/10/kate-rowe-240285-unsplash-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": "This blog post details how you can use Terraform to deploy Grafana and Influx on a Windows instance in AWS as a proof of concept implementation.",
                        "author_id": 1,
                        "created_at": "2018-10-31 07:43:00",
                        "created_by": 1,
                        "updated_at": "2018-10-31 16:40:41",
                        "updated_by": 1,
                        "published_at": "2018-10-31 16:40:41",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 76,
                        "uuid": "f9f5dfe9-2a7b-4db7-8197-ebc1ef4ebfe6",
                        "title": "Using Terraform to create Failover records in Route53",
                        "slug": "using-terraform-to-create-failover-records-in-route53",
                        "markdown": "I was recently tasked with creating some DNS records in AWS's Route53 service using Terraform. These records were to use a Failover Routing policy so that traffic would be routed to a secondary site if the first was unavailable. While working on this I found the Terraform documentation around configuring failover in Route53 was a bit brief, so am documenting my code via this blog post.\n\n> Note: This isn't necessarily best practice, or even correct practice, i'm still new to Terraform. Use with caution.\n\n```Terraform\nvariable \"primary_example_com\" {}\nvariable \"secndry_example_com\" {}\n\nresource \"aws_route53_zone\" \"example_com\" {\n    name = \"example.com\"\n}\n  \n# example.com\n  \nresource \"aws_route53_record\" \"example_com_primary\" {\n    zone_id = \"${aws_route53_zone.example_com.zone_id}\"\n    name    = \"${aws_route53_zone.example_com.name}\"\n    type    = \"A\"\n    ttl     = \"60\"\n    records = [\"${var.primary_example_com}\"]\n  \n    failover_routing_policy {\n        type = \"PRIMARY\"\n    }\n  \n    set_identifier  = \"example_com_primary\"\n    health_check_id = \"${aws_route53_health_check.example_com_primary.id}\"\n}\n  \nresource \"aws_route53_health_check\" \"example_com_primary\" {\n    ip_address        = \"${var.primary_example_com}\"\n    port              = \"80\"\n    type              = \"HTTP\"\n    resource_path     = \"/\"\n    failure_threshold = \"3\"\n    request_interval  = \"30\"\n}\n  \nresource \"aws_route53_record\" \"example_com_secondary\" {\n    zone_id = \"${aws_route53_zone.example_com.zone_id}\"\n    name    = \"${aws_route53_zone.example_com.name}\"\n    type    = \"A\"\n    ttl     = \"60\"\n    records = [\"${var.secndry_example_com}\"]\n  \n    failover_routing_policy {\n        type = \"SECONDARY\"\n    }\n  \n    set_identifier  = \"example_com_secondary\"\n    health_check_id = \"${aws_route53_health_check.example_com_secondary.id}\"\n}\n  \nresource \"aws_route53_health_check\" \"example_com_secondary\" {\n    ip_address        = \"${var.secndry_example_com}\"\n    port              = \"80\"\n    type              = \"HTTP\"\n    resource_path     = \"/\"\n    failure_threshold = \"3\"\n    request_interval  = \"30\"\n}\n```",
                        "html": "<p>I was recently tasked with creating some DNS records in AWS's Route53 service using Terraform. These records were to use a Failover Routing policy so that traffic would be routed to a secondary site if the first was unavailable. While working on this I found the Terraform documentation around configuring failover in Route53 was a bit brief, so am documenting my code via this blog post.</p>\n\n<blockquote>\n  <p>Note: This isn't necessarily best practice, or even correct practice, i'm still new to Terraform. Use with caution.</p>\n</blockquote>\n\n<pre><code class=\"language-Terraform\">variable \"primary_example_com\" {}  \nvariable \"secndry_example_com\" {}\n\nresource \"aws_route53_zone\" \"example_com\" {  \n    name = \"example.com\"\n}\n\n# example.com\n\nresource \"aws_route53_record\" \"example_com_primary\" {  \n    zone_id = \"${aws_route53_zone.example_com.zone_id}\"\n    name    = \"${aws_route53_zone.example_com.name}\"\n    type    = \"A\"\n    ttl     = \"60\"\n    records = [\"${var.primary_example_com}\"]\n\n    failover_routing_policy {\n        type = \"PRIMARY\"\n    }\n\n    set_identifier  = \"example_com_primary\"\n    health_check_id = \"${aws_route53_health_check.example_com_primary.id}\"\n}\n\nresource \"aws_route53_health_check\" \"example_com_primary\" {  \n    ip_address        = \"${var.primary_example_com}\"\n    port              = \"80\"\n    type              = \"HTTP\"\n    resource_path     = \"/\"\n    failure_threshold = \"3\"\n    request_interval  = \"30\"\n}\n\nresource \"aws_route53_record\" \"example_com_secondary\" {  \n    zone_id = \"${aws_route53_zone.example_com.zone_id}\"\n    name    = \"${aws_route53_zone.example_com.name}\"\n    type    = \"A\"\n    ttl     = \"60\"\n    records = [\"${var.secndry_example_com}\"]\n\n    failover_routing_policy {\n        type = \"SECONDARY\"\n    }\n\n    set_identifier  = \"example_com_secondary\"\n    health_check_id = \"${aws_route53_health_check.example_com_secondary.id}\"\n}\n\nresource \"aws_route53_health_check\" \"example_com_secondary\" {  \n    ip_address        = \"${var.secndry_example_com}\"\n    port              = \"80\"\n    type              = \"HTTP\"\n    resource_path     = \"/\"\n    failure_threshold = \"3\"\n    request_interval  = \"30\"\n}\n</code></pre>",
                        "image": "/content/images/2018/11/nasa-53884-unsplash.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2018-11-23 13:52:04",
                        "created_by": 1,
                        "updated_at": "2018-11-23 14:15:54",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 77,
                        "uuid": "751fa06d-0062-472d-ae0f-84879381ee95",
                        "title": "Validating ARM Templates with New-AzResourceGroupDeployment -WhatIf",
                        "slug": "arm-template-validation-with-new-azresourcegroupdeployment-whatif",
                        "markdown": "Microsoft have recently released a `WhatIf` operation feature for ARM (Azure Resource Manager) deployments. The feature is still in \"preview\" but its now freely available for anyone to try. While the results of the tool aren't perfect, its a game changer for developing ARM templates and introduces a Terraform-like preview of changes without needing to maintain a local state file.\n\nPreviously the tools for validating ARM templates were pretty limited. There was a basic command for ensuring the syntax of your template was correct, but beyond that having any foresight into what changes a deployment would trigger was pretty limited. In general the best approach was to have a non-Production environment that you could test your deployments against first to ensure you got the result you expected. Even so, it could be easy to misconfigure your environment and have those changes still be \"valid\" from a deployment perspective. Equally it was easily possible to have a deployment fail for some small reason that then wasn't caught until deployment time. With the `WhatIf` feature catching those kind of issues is quicker and easier.\n\n ",
                        "html": "<p>Microsoft have recently released a <code>WhatIf</code> operation feature for ARM (Azure Resource Manager) deployments. The feature is still in \"preview\" but its now freely available for anyone to try. While the results of the tool aren't perfect, its a game changer for developing ARM templates and introduces a Terraform-like preview of changes without needing to maintain a local state file.</p>\n\n<p>Previously the tools for validating ARM templates were pretty limited. There was a basic command for ensuring the syntax of your template was correct, but beyond that having any foresight into what changes a deployment would trigger was pretty limited. In general the best approach was to have a non-Production environment that you could test your deployments against first to ensure you got the result you expected. Even so, it could be easy to misconfigure your environment and have those changes still be \"valid\" from a deployment perspective. Equally it was easily possible to have a deployment fail for some small reason that then wasn't caught until deployment time. With the <code>WhatIf</code> feature catching those kind of issues is quicker and easier.</p>",
                        "image": "/content/images/2020/08/Grumpy-Cat.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "draft",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2020-08-08 13:56:38",
                        "created_by": 1,
                        "updated_at": "2020-08-08 14:17:15",
                        "updated_by": 1,
                        "published_at": null,
                        "published_by": null,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 78,
                        "uuid": "cfcacdb0-6074-4a0d-8de2-df02ec0f6db4",
                        "title": "ARM template error \"Operation PutLoadBalancerOperation was canceled and superseded by operation InternalOperation\"",
                        "slug": "arm-operation-putloadbalanceroperation-was-canceled-and-superseded-by-operation-internaloperation-loadbalancer",
                        "markdown": "Last week I spent a day troubleshooting an ARM (Azure Resource Manager) template deployment error that was frustratingly vague. A Load Balancer resource in the template was returning a result of \"conflict\" and the following error:\n\n> Note: I've blanked out the operation IDs in the example error below as these were unique to my deployment.\n\n```\n{\n    \"status\": \"Canceled\",\n    \"error\": {\n        \"code\": \"ResourceDeploymentFailure\",\n        \"message\": \"The resource operation completed with terminal provisioning state 'Canceled'.\",\n        \"details\": [\n            {\n                \"code\": \"Canceled\",\n                \"message\": \"Operation was canceled.\",\n                \"details\": [\n                    {\n                        \"code\": \"CanceledAndSupersededDueToAnotherOperation\",\n                        \"message\": \"Operation PutLoadBalancerOperation (xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx) was canceled and superseded by operation InternalOperation (xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx).\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n```\n\nThe template that was being deployed wasn't actually making any modifications to the Load Balancer at all (and was being executed in Incremental deployment mode). However the VMSS (Virtual Machine ScaleSet) associated with the resource was being modified, with changes being made to the extensions configured for the ScaleSet VMs.\n\nThe error turned out to be due to a missing dependency. I thought I'd blog it quickly here because there was nothing in the error that really pointed to that as the cause, and other examples I found on Google for this issue also didn't point to dependencies as the issue.\n\nTo resolve my issue I just had to make sure that the Load Balancer was listed in the `dependsOn` property for the VMSS it was associated with. Due to some convoluted logic in the template design (which I inherited) copy loops are being used but not all of the items in the array get created by the copy loop, so it had been missed (the existing dependency references the copy loop name). I'm not sure why this had never caught us out before, but the problem only seemed to surface when the VMSS was being modified after the initial deployment, whereas the initial deployment for the template worked just fine.\n\nFor more information on how to define resource dependencies in ARM templates see here:\n\n- https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/define-resource-dependency",
                        "html": "<p>Last week I spent a day troubleshooting an ARM (Azure Resource Manager) template deployment error that was frustratingly vague. A Load Balancer resource in the template was returning a result of \"conflict\" and the following error:</p>\n\n<blockquote>\n  <p>Note: I've blanked out the operation IDs in the example error below as these were unique to my deployment.</p>\n</blockquote>\n\n<pre><code>{\n    \"status\": \"Canceled\",\n    \"error\": {\n        \"code\": \"ResourceDeploymentFailure\",\n        \"message\": \"The resource operation completed with terminal provisioning state 'Canceled'.\",\n        \"details\": [\n            {\n                \"code\": \"Canceled\",\n                \"message\": \"Operation was canceled.\",\n                \"details\": [\n                    {\n                        \"code\": \"CanceledAndSupersededDueToAnotherOperation\",\n                        \"message\": \"Operation PutLoadBalancerOperation (xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx) was canceled and superseded by operation InternalOperation (xxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx).\"\n                    }\n                ]\n            }\n        ]\n    }\n}\n</code></pre>\n\n<p>The template that was being deployed wasn't actually making any modifications to the Load Balancer at all (and was being executed in Incremental deployment mode). However the VMSS (Virtual Machine ScaleSet) associated with the resource was being modified, with changes being made to the extensions configured for the ScaleSet VMs.</p>\n\n<p>The error turned out to be due to a missing dependency. I thought I'd blog it quickly here because there was nothing in the error that really pointed to that as the cause, and other examples I found on Google for this issue also didn't point to dependencies as the issue.</p>\n\n<p>To resolve my issue I just had to make sure that the Load Balancer was listed in the <code>dependsOn</code> property for the VMSS it was associated with. Due to some convoluted logic in the template design (which I inherited) copy loops are being used but not all of the items in the array get created by the copy loop, so it had been missed (the existing dependency references the copy loop name). I'm not sure why this had never caught us out before, but the problem only seemed to surface when the VMSS was being modified after the initial deployment, whereas the initial deployment for the template worked just fine.</p>\n\n<p>For more information on how to define resource dependencies in ARM templates see here:</p>\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/define-resource-dependency\">https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/define-resource-dependency</a></li>\n</ul>",
                        "image": "/content/images/2020/08/dropped-icecream-1.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2020-08-10 09:56:59",
                        "created_by": 1,
                        "updated_at": "2020-08-10 16:13:37",
                        "updated_by": 1,
                        "published_at": "2020-08-10 10:02:25",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    },
                    {
                        "id": 79,
                        "uuid": "49f66ed4-2066-4a38-a63c-f70e4da67c35",
                        "title": "ARM Deployment \"JToken type is not valid\" error",
                        "slug": "arm-deployment-jtoken-type-is-not-valid-error",
                        "markdown": "I have recently added tasks in to our Azure DevOps ARM template deployment pipeline to run the new `-WhatIf` parameter on the `New-AzResourceGroupDeployment` to preview the changes an ARM deployment will make, per this guide:\n\n- https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-deploy-what-if?tabs=azure-powershell\n\n![ARM deployment whatif example](/content/images/2020/09/resource-manager-deployment-whatif-change-types.png)\n\nThis week I discovered that my ARM template validation tasks were no longer working in my pipeline. This was curious because I hadn't changed the script or ARM template since it last ran successfully. The error being returned was:\n\n> InvalidTemplate - Deployment template validation failed: 'Template parameter JToken type is not valid. Expected\n'String, Uri'. Actual 'Object'. Please see https://aka.ms/resource-manager-parameter-files for usage details.'.\n\n![Jtoken type is not valid](/content/images/2020/09/Jtoken-type-is-not-valid-error.png)\n\nI eventually discovered this issue:\n\n- https://github.com/Azure/azure-powershell/issues/12792\n\nWhich indicates the problem is due to a bug in the Az module and how it handles `SecureString` typed inputs. \n\nThe bug has apparently been fixed in version 4.7.0 of the Az module, but if you're using the Azure DevOps hosted agent \"vs2017-win2016\" then that version of Az is not currently available (as of 29th Sept 2020). The workaround is therefore to rollback to using an older version of the Az module. After a bit of trial and error the version that worked for me was 4.3.0. Simply set the \"Preferred Azure PowerShell Version\" setting on your Azure PowerShell tasks to this and you should no longer experience the bug.\n\nObviously once 4.7.0 is available on the agent image you can revert to using the latest Az version again.",
                        "html": "<p>I have recently added tasks in to our Azure DevOps ARM template deployment pipeline to run the new <code>-WhatIf</code> parameter on the <code>New-AzResourceGroupDeployment</code> to preview the changes an ARM deployment will make, per this guide:</p>\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-deploy-what-if?tabs=azure-powershell\">https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-deploy-what-if?tabs=azure-powershell</a></li>\n</ul>\n\n<p><img src=\"/content/images/2020/09/resource-manager-deployment-whatif-change-types.png\" alt=\"ARM deployment whatif example\" /></p>\n\n<p>This week I discovered that my ARM template validation tasks were no longer working in my pipeline. This was curious because I hadn't changed the script or ARM template since it last ran successfully. The error being returned was:</p>\n\n<blockquote>\n  <p>InvalidTemplate - Deployment template validation failed: 'Template parameter JToken type is not valid. Expected\n  'String, Uri'. Actual 'Object'. Please see <a href=\"https://aka.ms/resource-manager-parameter-files\">https://aka.ms/resource-manager-parameter-files</a> for usage details.'.</p>\n</blockquote>\n\n<p><img src=\"/content/images/2020/09/Jtoken-type-is-not-valid-error.png\" alt=\"Jtoken type is not valid\" /></p>\n\n<p>I eventually discovered this issue:</p>\n\n<ul>\n<li><a href=\"https://github.com/Azure/azure-powershell/issues/12792\">https://github.com/Azure/azure-powershell/issues/12792</a></li>\n</ul>\n\n<p>Which indicates the problem is due to a bug in the Az module and how it handles <code>SecureString</code> typed inputs. </p>\n\n<p>The bug has apparently been fixed in version 4.7.0 of the Az module, but if you're using the Azure DevOps hosted agent \"vs2017-win2016\" then that version of Az is not currently available (as of 29th Sept 2020). The workaround is therefore to rollback to using an older version of the Az module. After a bit of trial and error the version that worked for me was 4.3.0. Simply set the \"Preferred Azure PowerShell Version\" setting on your Azure PowerShell tasks to this and you should no longer experience the bug.</p>\n\n<p>Obviously once 4.7.0 is available on the agent image you can revert to using the latest Az version again.</p>",
                        "image": "/content/images/2020/09/windows-keyboard-black.jpg",
                        "featured": 0,
                        "page": 0,
                        "status": "published",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "author_id": 1,
                        "created_at": "2020-09-29 10:48:39",
                        "created_by": 1,
                        "updated_at": "2020-09-29 11:40:28",
                        "updated_by": 1,
                        "published_at": "2020-09-29 10:56:49",
                        "published_by": 1,
                        "visibility": "public",
                        "mobiledoc": null,
                        "amp": null
                    }
                ],
                "users": [
                    {
                        "id": 1,
                        "uuid": "e8f83ccd-3323-4d8b-9609-94318b11efd4",
                        "name": "Mark Wragg",
                        "slug": "mark",
                        "password": "$2a$10$yzokpQqhb987zM/1fb54augWLF9sO.J51J/dNhSD6H5MJHGfxX9Ey",
                        "email": "mark.wragg@gmail.com",
                        "image": "//www.gravatar.com/avatar/e8e1b8d0e98f84b10a03f9430334b02f?s=250&d=mm&r=x",
                        "cover": "/content/images/2017/01/3840x2400-snow_winter_sun_ice_blocks_on_river_bed_mountain_river_sky_ice_nature-8895.jpg",
                        "bio": null,
                        "website": null,
                        "location": "UK",
                        "accessibility": null,
                        "status": "active",
                        "language": "en_US",
                        "meta_title": null,
                        "meta_description": null,
                        "tour": null,
                        "last_login": "2021-10-03 13:14:27",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2021-10-03 13:14:27",
                        "updated_by": 1,
                        "visibility": "public",
                        "facebook": null,
                        "twitter": "@markwragg"
                    }
                ],
                "roles": [
                    {
                        "id": 1,
                        "uuid": "17884ce8-1634-4a4a-86d9-69f2358262d7",
                        "name": "Administrator",
                        "description": "Administrators",
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 2,
                        "uuid": "223d2737-0c05-41f7-bf27-9afcc59a45c3",
                        "name": "Editor",
                        "description": "Editors",
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 3,
                        "uuid": "47ff9dae-4950-4b73-9f9d-88d800d60c59",
                        "name": "Author",
                        "description": "Authors",
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 4,
                        "uuid": "9aa2aa21-b103-438a-8e58-af1c7b098243",
                        "name": "Owner",
                        "description": "Blog Owner",
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    }
                ],
                "roles_users": [
                    {
                        "id": 1,
                        "role_id": 4,
                        "user_id": 1
                    },
                    {
                        "id": 2,
                        "role_id": 1,
                        "user_id": 2
                    }
                ],
                "permissions": [
                    {
                        "id": 1,
                        "uuid": "9cbc6a41-01bc-41e2-b838-e8a031025a80",
                        "name": "Export database",
                        "object_type": "db",
                        "action_type": "exportContent",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 2,
                        "uuid": "f4d2ecc7-34c0-4eee-bd83-1bbeb2c09981",
                        "name": "Import database",
                        "object_type": "db",
                        "action_type": "importContent",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 3,
                        "uuid": "9093bf2f-d03f-4fb3-8ff1-1e6019292168",
                        "name": "Delete all content",
                        "object_type": "db",
                        "action_type": "deleteAllContent",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 4,
                        "uuid": "66ddecc1-e098-4864-a956-467f013acdb2",
                        "name": "Send mail",
                        "object_type": "mail",
                        "action_type": "send",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 5,
                        "uuid": "9b8c3480-5f88-438e-ab93-6d665857d6bc",
                        "name": "Browse notifications",
                        "object_type": "notification",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 6,
                        "uuid": "64165222-9bfc-41a6-a9ea-393ac6f64f0e",
                        "name": "Add notifications",
                        "object_type": "notification",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 7,
                        "uuid": "0ffd011b-bdbf-4d64-9de0-79f6bc98205e",
                        "name": "Delete notifications",
                        "object_type": "notification",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 8,
                        "uuid": "5fbd3901-d77e-4b23-b79f-f0c4c4ec81ba",
                        "name": "Browse posts",
                        "object_type": "post",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 9,
                        "uuid": "a160dcbd-8db9-4005-920b-f41b9612145b",
                        "name": "Read posts",
                        "object_type": "post",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 10,
                        "uuid": "b4a6e28f-ce48-4495-922d-9170afed0cab",
                        "name": "Edit posts",
                        "object_type": "post",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 11,
                        "uuid": "7e44b84a-4fa8-47a8-8bb8-0bcb9645ccc5",
                        "name": "Add posts",
                        "object_type": "post",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 12,
                        "uuid": "606a536a-e042-44d2-9639-9bfe425086d6",
                        "name": "Delete posts",
                        "object_type": "post",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 13,
                        "uuid": "55da8703-e9ee-4a42-91f3-f51ce7af8982",
                        "name": "Browse settings",
                        "object_type": "setting",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 14,
                        "uuid": "6ea64b96-5e23-42cf-9597-9d4f847340f5",
                        "name": "Read settings",
                        "object_type": "setting",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 15,
                        "uuid": "c6970c99-ac24-4445-8c72-7f9dcd5fe6fc",
                        "name": "Edit settings",
                        "object_type": "setting",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 16,
                        "uuid": "fce88b2d-f34e-4f57-ab5d-3d1bfa018291",
                        "name": "Generate slugs",
                        "object_type": "slug",
                        "action_type": "generate",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 17,
                        "uuid": "f2d4dc1e-1055-4acb-bd9f-40bba8358049",
                        "name": "Browse tags",
                        "object_type": "tag",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 18,
                        "uuid": "81bc2b31-bbbd-4884-983a-690cc4f777bc",
                        "name": "Read tags",
                        "object_type": "tag",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 19,
                        "uuid": "85e3082c-21ef-4511-93c4-a6e769fcf950",
                        "name": "Edit tags",
                        "object_type": "tag",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 20,
                        "uuid": "cea377eb-46dc-486c-b4c4-0e152990ae14",
                        "name": "Add tags",
                        "object_type": "tag",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 21,
                        "uuid": "8d4d10e8-b02b-4462-88a5-01f7158f9bb6",
                        "name": "Delete tags",
                        "object_type": "tag",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 22,
                        "uuid": "a9fc8170-7a12-45ed-b7b2-f24977b98c93",
                        "name": "Browse themes",
                        "object_type": "theme",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 23,
                        "uuid": "e7540686-8fce-44d6-9eea-7e51c62723c6",
                        "name": "Edit themes",
                        "object_type": "theme",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 24,
                        "uuid": "9ebf0a71-2530-4db9-8921-cc871156881e",
                        "name": "Browse users",
                        "object_type": "user",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 25,
                        "uuid": "844fa9b5-6242-4560-95ad-a4c9ee5c6608",
                        "name": "Read users",
                        "object_type": "user",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 26,
                        "uuid": "8df81c49-a748-4506-b5b1-cb2e6e464d77",
                        "name": "Edit users",
                        "object_type": "user",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 27,
                        "uuid": "a44b61d7-84de-4bf7-a860-3bd5e85bb8c6",
                        "name": "Add users",
                        "object_type": "user",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 28,
                        "uuid": "3c964d7a-55bd-4d39-a3b9-631dd1c12828",
                        "name": "Delete users",
                        "object_type": "user",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 29,
                        "uuid": "84d06d1b-afbe-4e4b-8bf7-17f16f9a00ac",
                        "name": "Assign a role",
                        "object_type": "role",
                        "action_type": "assign",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 30,
                        "uuid": "3c7a9c7b-d5f6-4b2f-88a8-448b4f3548af",
                        "name": "Browse roles",
                        "object_type": "role",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-05-02 11:07:14",
                        "created_by": 1,
                        "updated_at": "2016-05-02 11:07:14",
                        "updated_by": 1
                    },
                    {
                        "id": 31,
                        "uuid": "858dc5c2-7b44-472e-9987-2bc5c85e4355",
                        "name": "Browse clients",
                        "object_type": "client",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 32,
                        "uuid": "0aa2b840-95ce-42c4-b448-e702d2c6cf0c",
                        "name": "Read clients",
                        "object_type": "client",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 33,
                        "uuid": "29ada388-9083-4545-a3f8-8ed58334616b",
                        "name": "Edit clients",
                        "object_type": "client",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 34,
                        "uuid": "dd77e5c8-69c3-4aba-a887-34f886c502c3",
                        "name": "Add clients",
                        "object_type": "client",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 35,
                        "uuid": "3fa4823f-9f08-4a2a-a607-627adf86a08c",
                        "name": "Delete clients",
                        "object_type": "client",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 36,
                        "uuid": "811396cb-3204-4003-bd12-d456050afe36",
                        "name": "Browse subscribers",
                        "object_type": "subscriber",
                        "action_type": "browse",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 37,
                        "uuid": "10845d60-dd71-460b-a967-a13501b32b8d",
                        "name": "Read subscribers",
                        "object_type": "subscriber",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 38,
                        "uuid": "c7eb935c-f56a-4eac-9099-90f4c796344d",
                        "name": "Edit subscribers",
                        "object_type": "subscriber",
                        "action_type": "edit",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 39,
                        "uuid": "5874da3c-f1a9-4ce0-93ff-1a7987bab8cd",
                        "name": "Add subscribers",
                        "object_type": "subscriber",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 40,
                        "uuid": "6efab07b-f924-44a3-9f38-e53bf5b83480",
                        "name": "Delete subscribers",
                        "object_type": "subscriber",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:14:47",
                        "updated_by": 1
                    },
                    {
                        "id": 41,
                        "uuid": "8eead343-b451-430c-9402-11cba51c70ef",
                        "name": "Upload themes",
                        "object_type": "theme",
                        "action_type": "add",
                        "object_id": null,
                        "created_at": "2017-01-04 14:20:11",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:11",
                        "updated_by": 1
                    },
                    {
                        "id": 42,
                        "uuid": "e1fd29cc-dc79-42a6-8a94-35c476abb0b6",
                        "name": "Download themes",
                        "object_type": "theme",
                        "action_type": "read",
                        "object_id": null,
                        "created_at": "2017-01-04 14:20:11",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:11",
                        "updated_by": 1
                    },
                    {
                        "id": 43,
                        "uuid": "22f43a26-f161-4acb-9b34-acbbf2e52359",
                        "name": "Delete themes",
                        "object_type": "theme",
                        "action_type": "destroy",
                        "object_id": null,
                        "created_at": "2017-01-04 14:20:11",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:11",
                        "updated_by": 1
                    }
                ],
                "permissions_users": [],
                "permissions_roles": [
                    {
                        "id": 1,
                        "role_id": 1,
                        "permission_id": 1
                    },
                    {
                        "id": 2,
                        "role_id": 1,
                        "permission_id": 2
                    },
                    {
                        "id": 3,
                        "role_id": 1,
                        "permission_id": 3
                    },
                    {
                        "id": 4,
                        "role_id": 1,
                        "permission_id": 4
                    },
                    {
                        "id": 5,
                        "role_id": 1,
                        "permission_id": 5
                    },
                    {
                        "id": 6,
                        "role_id": 1,
                        "permission_id": 6
                    },
                    {
                        "id": 7,
                        "role_id": 1,
                        "permission_id": 7
                    },
                    {
                        "id": 8,
                        "role_id": 1,
                        "permission_id": 8
                    },
                    {
                        "id": 9,
                        "role_id": 1,
                        "permission_id": 9
                    },
                    {
                        "id": 10,
                        "role_id": 1,
                        "permission_id": 10
                    },
                    {
                        "id": 11,
                        "role_id": 1,
                        "permission_id": 11
                    },
                    {
                        "id": 12,
                        "role_id": 1,
                        "permission_id": 12
                    },
                    {
                        "id": 13,
                        "role_id": 1,
                        "permission_id": 13
                    },
                    {
                        "id": 14,
                        "role_id": 1,
                        "permission_id": 14
                    },
                    {
                        "id": 15,
                        "role_id": 1,
                        "permission_id": 15
                    },
                    {
                        "id": 16,
                        "role_id": 1,
                        "permission_id": 16
                    },
                    {
                        "id": 17,
                        "role_id": 1,
                        "permission_id": 17
                    },
                    {
                        "id": 18,
                        "role_id": 1,
                        "permission_id": 18
                    },
                    {
                        "id": 19,
                        "role_id": 1,
                        "permission_id": 19
                    },
                    {
                        "id": 20,
                        "role_id": 1,
                        "permission_id": 20
                    },
                    {
                        "id": 21,
                        "role_id": 1,
                        "permission_id": 21
                    },
                    {
                        "id": 22,
                        "role_id": 1,
                        "permission_id": 22
                    },
                    {
                        "id": 23,
                        "role_id": 1,
                        "permission_id": 23
                    },
                    {
                        "id": 24,
                        "role_id": 1,
                        "permission_id": 24
                    },
                    {
                        "id": 25,
                        "role_id": 1,
                        "permission_id": 25
                    },
                    {
                        "id": 26,
                        "role_id": 1,
                        "permission_id": 26
                    },
                    {
                        "id": 27,
                        "role_id": 1,
                        "permission_id": 27
                    },
                    {
                        "id": 28,
                        "role_id": 1,
                        "permission_id": 28
                    },
                    {
                        "id": 29,
                        "role_id": 1,
                        "permission_id": 29
                    },
                    {
                        "id": 30,
                        "role_id": 1,
                        "permission_id": 30
                    },
                    {
                        "id": 31,
                        "role_id": 2,
                        "permission_id": 8
                    },
                    {
                        "id": 32,
                        "role_id": 2,
                        "permission_id": 9
                    },
                    {
                        "id": 33,
                        "role_id": 2,
                        "permission_id": 10
                    },
                    {
                        "id": 34,
                        "role_id": 2,
                        "permission_id": 11
                    },
                    {
                        "id": 35,
                        "role_id": 2,
                        "permission_id": 12
                    },
                    {
                        "id": 36,
                        "role_id": 2,
                        "permission_id": 13
                    },
                    {
                        "id": 37,
                        "role_id": 2,
                        "permission_id": 14
                    },
                    {
                        "id": 38,
                        "role_id": 2,
                        "permission_id": 16
                    },
                    {
                        "id": 39,
                        "role_id": 2,
                        "permission_id": 17
                    },
                    {
                        "id": 40,
                        "role_id": 2,
                        "permission_id": 18
                    },
                    {
                        "id": 41,
                        "role_id": 2,
                        "permission_id": 19
                    },
                    {
                        "id": 42,
                        "role_id": 2,
                        "permission_id": 20
                    },
                    {
                        "id": 43,
                        "role_id": 2,
                        "permission_id": 21
                    },
                    {
                        "id": 44,
                        "role_id": 2,
                        "permission_id": 24
                    },
                    {
                        "id": 45,
                        "role_id": 2,
                        "permission_id": 25
                    },
                    {
                        "id": 46,
                        "role_id": 2,
                        "permission_id": 26
                    },
                    {
                        "id": 47,
                        "role_id": 2,
                        "permission_id": 27
                    },
                    {
                        "id": 48,
                        "role_id": 2,
                        "permission_id": 28
                    },
                    {
                        "id": 49,
                        "role_id": 2,
                        "permission_id": 29
                    },
                    {
                        "id": 50,
                        "role_id": 2,
                        "permission_id": 30
                    },
                    {
                        "id": 51,
                        "role_id": 3,
                        "permission_id": 8
                    },
                    {
                        "id": 52,
                        "role_id": 3,
                        "permission_id": 9
                    },
                    {
                        "id": 53,
                        "role_id": 3,
                        "permission_id": 11
                    },
                    {
                        "id": 54,
                        "role_id": 3,
                        "permission_id": 13
                    },
                    {
                        "id": 55,
                        "role_id": 3,
                        "permission_id": 14
                    },
                    {
                        "id": 56,
                        "role_id": 3,
                        "permission_id": 16
                    },
                    {
                        "id": 57,
                        "role_id": 3,
                        "permission_id": 17
                    },
                    {
                        "id": 58,
                        "role_id": 3,
                        "permission_id": 18
                    },
                    {
                        "id": 59,
                        "role_id": 3,
                        "permission_id": 20
                    },
                    {
                        "id": 60,
                        "role_id": 3,
                        "permission_id": 24
                    },
                    {
                        "id": 61,
                        "role_id": 3,
                        "permission_id": 25
                    },
                    {
                        "id": 62,
                        "role_id": 3,
                        "permission_id": 30
                    },
                    {
                        "id": 63,
                        "role_id": 1,
                        "permission_id": 31
                    },
                    {
                        "id": 64,
                        "role_id": 1,
                        "permission_id": 35
                    },
                    {
                        "id": 65,
                        "role_id": 1,
                        "permission_id": 33
                    },
                    {
                        "id": 66,
                        "role_id": 1,
                        "permission_id": 34
                    },
                    {
                        "id": 67,
                        "role_id": 1,
                        "permission_id": 32
                    },
                    {
                        "id": 68,
                        "role_id": 2,
                        "permission_id": 31
                    },
                    {
                        "id": 69,
                        "role_id": 2,
                        "permission_id": 32
                    },
                    {
                        "id": 70,
                        "role_id": 2,
                        "permission_id": 33
                    },
                    {
                        "id": 71,
                        "role_id": 2,
                        "permission_id": 34
                    },
                    {
                        "id": 72,
                        "role_id": 2,
                        "permission_id": 35
                    },
                    {
                        "id": 73,
                        "role_id": 3,
                        "permission_id": 31
                    },
                    {
                        "id": 74,
                        "role_id": 3,
                        "permission_id": 32
                    },
                    {
                        "id": 75,
                        "role_id": 3,
                        "permission_id": 33
                    },
                    {
                        "id": 76,
                        "role_id": 3,
                        "permission_id": 34
                    },
                    {
                        "id": 77,
                        "role_id": 3,
                        "permission_id": 35
                    },
                    {
                        "id": 78,
                        "role_id": 1,
                        "permission_id": 36
                    },
                    {
                        "id": 79,
                        "role_id": 1,
                        "permission_id": 37
                    },
                    {
                        "id": 80,
                        "role_id": 1,
                        "permission_id": 38
                    },
                    {
                        "id": 81,
                        "role_id": 1,
                        "permission_id": 39
                    },
                    {
                        "id": 82,
                        "role_id": 1,
                        "permission_id": 40
                    },
                    {
                        "id": 83,
                        "role_id": 2,
                        "permission_id": 39
                    },
                    {
                        "id": 84,
                        "role_id": 3,
                        "permission_id": 39
                    },
                    {
                        "id": 85,
                        "role_id": 1,
                        "permission_id": 41
                    },
                    {
                        "id": 86,
                        "role_id": 1,
                        "permission_id": 42
                    },
                    {
                        "id": 87,
                        "role_id": 1,
                        "permission_id": 43
                    }
                ],
                "permissions_apps": [],
                "settings": [
                    {
                        "id": 1,
                        "uuid": "81d68480-8a30-45d3-9a86-911947883eaf",
                        "key": "databaseVersion",
                        "value": "009",
                        "type": "core",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:12",
                        "updated_by": 1
                    },
                    {
                        "id": 2,
                        "uuid": "59d2ea69-790f-4906-8672-14ba6a0518a4",
                        "key": "dbHash",
                        "value": "73ce5848-e29b-4c34-b7de-b9b8a25e7b31",
                        "type": "core",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:12",
                        "updated_by": 1
                    },
                    {
                        "id": 3,
                        "uuid": "2f0bfcf7-ce97-48b5-b3bb-af1b12a3bcc5",
                        "key": "nextUpdateCheck",
                        "value": "1633353264",
                        "type": "core",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2021-10-03 13:14:24",
                        "updated_by": 1
                    },
                    {
                        "id": 4,
                        "uuid": "68968f1f-8ccd-47c6-99b6-c2e5ddf0f978",
                        "key": "displayUpdateNotification",
                        "value": "0.11.14",
                        "type": "core",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2021-10-03 13:14:24",
                        "updated_by": 1
                    },
                    {
                        "id": 5,
                        "uuid": "441db9ff-1390-42ba-adef-6098520ed7e6",
                        "key": "title",
                        "value": "Mark Wragg - Blog | wragg.io",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 6,
                        "uuid": "11a930f6-d5fa-468f-8b86-dce8b1e46193",
                        "key": "description",
                        "value": "Windows, Automation, Powershell, Pester, Chef, AWS, Azure",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 7,
                        "uuid": "7196d8ee-17fc-4fd1-b38c-ea5a271f6ee1",
                        "key": "logo",
                        "value": "",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 8,
                        "uuid": "9f083b7e-d2c8-41a0-93af-1911d6b3eaeb",
                        "key": "cover",
                        "value": "/content/images/2020/08/bots-1.jpg",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 9,
                        "uuid": "04905b0e-8780-4139-a8d6-9bae793c5452",
                        "key": "defaultLang",
                        "value": "en_US",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 10,
                        "uuid": "a668d5bd-8ec9-4835-b35c-7fb327db62c0",
                        "key": "postsPerPage",
                        "value": "5",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 11,
                        "uuid": "5990c904-ac84-4791-8ca9-98b7c000d254",
                        "key": "forceI18n",
                        "value": "true",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 12,
                        "uuid": "f88127c6-a419-40ae-a121-fab8765a8563",
                        "key": "permalinks",
                        "value": "/:slug/",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 13,
                        "uuid": "0c16a500-991c-4703-9943-511c1f5bad5e",
                        "key": "ghost_head",
                        "value": "<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-77304125-1', 'auto');\n  ga('send', 'pageview');    \n</script>",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 14,
                        "uuid": "c32dafb2-285d-4e1e-8286-dec4b4f0c168",
                        "key": "ghost_foot",
                        "value": "",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 15,
                        "uuid": "e6e800a2-4549-4a78-beb5-d6ee361bb457",
                        "key": "labs",
                        "value": "{\"subscribers\":false}",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 16,
                        "uuid": "2548ffa7-a1ca-4dbf-82a6-c18bef5002c7",
                        "key": "navigation",
                        "value": "[{\"label\":\"About Me\",\"url\":\"/about-me\"},{\"label\":\"Resources\",\"url\":\"/resources\"},{\"label\":\"Subscribe\",\"url\":\"/subscribe\"}]",
                        "type": "blog",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 17,
                        "uuid": "3a6121cc-1d58-40e8-9f31-c88df34f8a14",
                        "key": "activeApps",
                        "value": "[]",
                        "type": "app",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:12",
                        "updated_by": 1
                    },
                    {
                        "id": 18,
                        "uuid": "f0dccdfd-980c-4693-abe1-1211729ed42f",
                        "key": "installedApps",
                        "value": "[]",
                        "type": "app",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2021-10-03 13:12:47",
                        "updated_by": 1
                    },
                    {
                        "id": 19,
                        "uuid": "a2359bdc-e92a-4b46-a836-16628ef9942c",
                        "key": "isPrivate",
                        "value": "false",
                        "type": "private",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 20,
                        "uuid": "3be9aa4a-beba-4363-ab48-9d4416a9ff4b",
                        "key": "password",
                        "value": "null",
                        "type": "private",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 21,
                        "uuid": "9be0328f-43ec-45e1-9552-eda87a4e95a3",
                        "key": "activeTheme",
                        "value": "ghost-material-master",
                        "type": "theme",
                        "created_at": "2016-05-02 11:07:15",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 22,
                        "uuid": "cbf266fb-3728-4010-8d5b-0a4b1df9aff8",
                        "key": "seenNotifications",
                        "value": "[]",
                        "type": "core",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:12",
                        "updated_by": 1
                    },
                    {
                        "id": 23,
                        "uuid": "b432618a-ece9-41e8-a172-b26ac6f89a9f",
                        "key": "migrations",
                        "value": "{\"006/01\":\"2017-01-04T14:20:12Z\"}",
                        "type": "core",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2017-01-04 14:20:12",
                        "updated_by": 1
                    },
                    {
                        "id": 24,
                        "uuid": "444570ca-e084-4e2d-920e-955e7fc0bc02",
                        "key": "activeTimezone",
                        "value": "Europe/Dublin",
                        "type": "blog",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 25,
                        "uuid": "c3741fc0-644d-4cf0-80e5-2cd1f788c943",
                        "key": "facebook",
                        "value": "",
                        "type": "blog",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 26,
                        "uuid": "5474f8f9-b053-40ad-a9c5-5f879db86dd5",
                        "key": "twitter",
                        "value": "@markwragg",
                        "type": "blog",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 27,
                        "uuid": "e6f784ce-6c57-4fd9-a6f5-3766fd2f98fb",
                        "key": "slack",
                        "value": "[{\"url\":\"\"}]",
                        "type": "blog",
                        "created_at": "2016-09-08 21:14:47",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    },
                    {
                        "id": 28,
                        "uuid": "80c3e034-5867-4291-985c-128c7489ff3a",
                        "key": "amp",
                        "value": "true",
                        "type": "blog",
                        "created_at": "2017-08-21 22:00:21",
                        "created_by": 1,
                        "updated_at": "2020-08-10 12:03:10",
                        "updated_by": 1
                    }
                ],
                "posts_tags": [
                    {
                        "id": 4,
                        "post_id": 6,
                        "tag_id": 4,
                        "sort_order": 0
                    },
                    {
                        "id": 5,
                        "post_id": 14,
                        "tag_id": 3,
                        "sort_order": 0
                    },
                    {
                        "id": 6,
                        "post_id": 11,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 7,
                        "post_id": 15,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 8,
                        "post_id": 18,
                        "tag_id": 5,
                        "sort_order": 0
                    },
                    {
                        "id": 9,
                        "post_id": 17,
                        "tag_id": 5,
                        "sort_order": 0
                    },
                    {
                        "id": 10,
                        "post_id": 17,
                        "tag_id": 6,
                        "sort_order": 2
                    },
                    {
                        "id": 11,
                        "post_id": 17,
                        "tag_id": 7,
                        "sort_order": 3
                    },
                    {
                        "id": 12,
                        "post_id": 21,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 13,
                        "post_id": 21,
                        "tag_id": 8,
                        "sort_order": 1
                    },
                    {
                        "id": 14,
                        "post_id": 18,
                        "tag_id": 9,
                        "sort_order": 1
                    },
                    {
                        "id": 15,
                        "post_id": 19,
                        "tag_id": 5,
                        "sort_order": 0
                    },
                    {
                        "id": 16,
                        "post_id": 19,
                        "tag_id": 9,
                        "sort_order": 1
                    },
                    {
                        "id": 17,
                        "post_id": 17,
                        "tag_id": 9,
                        "sort_order": 1
                    },
                    {
                        "id": 18,
                        "post_id": 25,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 19,
                        "post_id": 26,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 20,
                        "post_id": 28,
                        "tag_id": 10,
                        "sort_order": 1
                    },
                    {
                        "id": 21,
                        "post_id": 28,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 22,
                        "post_id": 29,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 23,
                        "post_id": 32,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 24,
                        "post_id": 31,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 25,
                        "post_id": 31,
                        "tag_id": 11,
                        "sort_order": 1
                    },
                    {
                        "id": 26,
                        "post_id": 33,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 27,
                        "post_id": 33,
                        "tag_id": 11,
                        "sort_order": 1
                    },
                    {
                        "id": 28,
                        "post_id": 35,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 29,
                        "post_id": 39,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 30,
                        "post_id": 39,
                        "tag_id": 10,
                        "sort_order": 1
                    },
                    {
                        "id": 32,
                        "post_id": 41,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 33,
                        "post_id": 40,
                        "tag_id": 13,
                        "sort_order": 1
                    },
                    {
                        "id": 34,
                        "post_id": 40,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 35,
                        "post_id": 40,
                        "tag_id": 14,
                        "sort_order": 2
                    },
                    {
                        "id": 36,
                        "post_id": 42,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 37,
                        "post_id": 42,
                        "tag_id": 14,
                        "sort_order": 1
                    },
                    {
                        "id": 38,
                        "post_id": 44,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 39,
                        "post_id": 46,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 40,
                        "post_id": 46,
                        "tag_id": 10,
                        "sort_order": 1
                    },
                    {
                        "id": 41,
                        "post_id": 46,
                        "tag_id": 15,
                        "sort_order": 2
                    },
                    {
                        "id": 42,
                        "post_id": 48,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 43,
                        "post_id": 51,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 44,
                        "post_id": 52,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 45,
                        "post_id": 53,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 46,
                        "post_id": 54,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 47,
                        "post_id": 60,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 48,
                        "post_id": 60,
                        "tag_id": 8,
                        "sort_order": 1
                    },
                    {
                        "id": 49,
                        "post_id": 60,
                        "tag_id": 16,
                        "sort_order": 2
                    },
                    {
                        "id": 50,
                        "post_id": 62,
                        "tag_id": 17,
                        "sort_order": 1
                    },
                    {
                        "id": 51,
                        "post_id": 62,
                        "tag_id": 18,
                        "sort_order": 2
                    },
                    {
                        "id": 52,
                        "post_id": 62,
                        "tag_id": 5,
                        "sort_order": 0
                    },
                    {
                        "id": 53,
                        "post_id": 63,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 54,
                        "post_id": 63,
                        "tag_id": 5,
                        "sort_order": 1
                    },
                    {
                        "id": 55,
                        "post_id": 55,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 56,
                        "post_id": 64,
                        "tag_id": 19,
                        "sort_order": 0
                    },
                    {
                        "id": 57,
                        "post_id": 64,
                        "tag_id": 5,
                        "sort_order": 1
                    },
                    {
                        "id": 58,
                        "post_id": 65,
                        "tag_id": 19,
                        "sort_order": 0
                    },
                    {
                        "id": 59,
                        "post_id": 65,
                        "tag_id": 5,
                        "sort_order": 1
                    },
                    {
                        "id": 60,
                        "post_id": 66,
                        "tag_id": 19,
                        "sort_order": 0
                    },
                    {
                        "id": 61,
                        "post_id": 66,
                        "tag_id": 5,
                        "sort_order": 1
                    },
                    {
                        "id": 62,
                        "post_id": 67,
                        "tag_id": 19,
                        "sort_order": 0
                    },
                    {
                        "id": 63,
                        "post_id": 67,
                        "tag_id": 5,
                        "sort_order": 1
                    },
                    {
                        "id": 64,
                        "post_id": 61,
                        "tag_id": 5,
                        "sort_order": 0
                    },
                    {
                        "id": 65,
                        "post_id": 61,
                        "tag_id": 2,
                        "sort_order": 1
                    },
                    {
                        "id": 66,
                        "post_id": 61,
                        "tag_id": 20,
                        "sort_order": 2
                    },
                    {
                        "id": 67,
                        "post_id": 61,
                        "tag_id": 21,
                        "sort_order": 3
                    },
                    {
                        "id": 68,
                        "post_id": 61,
                        "tag_id": 22,
                        "sort_order": 4
                    },
                    {
                        "id": 69,
                        "post_id": 68,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 70,
                        "post_id": 68,
                        "tag_id": 23,
                        "sort_order": 1
                    },
                    {
                        "id": 71,
                        "post_id": 71,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 73,
                        "post_id": 72,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 74,
                        "post_id": 72,
                        "tag_id": 10,
                        "sort_order": 1
                    },
                    {
                        "id": 75,
                        "post_id": 72,
                        "tag_id": 15,
                        "sort_order": 2
                    },
                    {
                        "id": 76,
                        "post_id": 72,
                        "tag_id": 8,
                        "sort_order": 3
                    },
                    {
                        "id": 77,
                        "post_id": 73,
                        "tag_id": 2,
                        "sort_order": 0
                    },
                    {
                        "id": 78,
                        "post_id": 75,
                        "tag_id": 20,
                        "sort_order": 0
                    },
                    {
                        "id": 79,
                        "post_id": 75,
                        "tag_id": 21,
                        "sort_order": 1
                    },
                    {
                        "id": 80,
                        "post_id": 75,
                        "tag_id": 5,
                        "sort_order": 2
                    },
                    {
                        "id": 81,
                        "post_id": 75,
                        "tag_id": 22,
                        "sort_order": 3
                    },
                    {
                        "id": 82,
                        "post_id": 75,
                        "tag_id": 24,
                        "sort_order": 4
                    },
                    {
                        "id": 83,
                        "post_id": 75,
                        "tag_id": 13,
                        "sort_order": 5
                    },
                    {
                        "id": 84,
                        "post_id": 75,
                        "tag_id": 8,
                        "sort_order": 6
                    },
                    {
                        "id": 85,
                        "post_id": 75,
                        "tag_id": 9,
                        "sort_order": 7
                    },
                    {
                        "id": 86,
                        "post_id": 78,
                        "tag_id": 13,
                        "sort_order": 0
                    },
                    {
                        "id": 87,
                        "post_id": 78,
                        "tag_id": 18,
                        "sort_order": 1
                    },
                    {
                        "id": 88,
                        "post_id": 78,
                        "tag_id": 25,
                        "sort_order": 2
                    },
                    {
                        "id": 89,
                        "post_id": 78,
                        "tag_id": 26,
                        "sort_order": 3
                    },
                    {
                        "id": 90,
                        "post_id": 78,
                        "tag_id": 27,
                        "sort_order": 4
                    },
                    {
                        "id": 91,
                        "post_id": 79,
                        "tag_id": 13,
                        "sort_order": 0
                    },
                    {
                        "id": 92,
                        "post_id": 79,
                        "tag_id": 25,
                        "sort_order": 1
                    },
                    {
                        "id": 93,
                        "post_id": 79,
                        "tag_id": 18,
                        "sort_order": 2
                    }
                ],
                "apps": [],
                "app_settings": [],
                "app_fields": [],
                "tags": [
                    {
                        "id": 2,
                        "uuid": "3f36f603-0e68-4d85-96cf-a1cae556346a",
                        "name": "Powershell",
                        "slug": "powershell",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-02 12:52:29",
                        "created_by": 2,
                        "updated_at": "2016-05-02 12:52:29",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 3,
                        "uuid": "d84348c7-f4e8-413e-a4fd-b8cb9b021176",
                        "name": "AWS",
                        "slug": "aws",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-02 12:52:29",
                        "created_by": 2,
                        "updated_at": "2016-10-19 14:33:25",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 4,
                        "uuid": "5999755f-e646-441d-ad53-2129cd7b3d84",
                        "name": "Chef",
                        "slug": "chef",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-02 21:53:34",
                        "created_by": 1,
                        "updated_at": "2016-05-02 21:53:34",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 5,
                        "uuid": "10239024-30da-464f-b6ba-f6813be198c6",
                        "name": "Windows",
                        "slug": "windows",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-05 22:32:39",
                        "created_by": 1,
                        "updated_at": "2016-05-05 22:32:39",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 6,
                        "uuid": "dccd2077-503a-4f1c-ab59-9638699bf199",
                        "name": "Containers",
                        "slug": "containers",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-07 16:30:24",
                        "created_by": 1,
                        "updated_at": "2016-10-19 14:33:21",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 7,
                        "uuid": "680eedcf-f1c7-4744-ae2d-471faf9d83c5",
                        "name": "Docker",
                        "slug": "docker",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-07 16:30:24",
                        "created_by": 1,
                        "updated_at": "2016-10-19 14:33:37",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 8,
                        "uuid": "cc62a6f8-9f5e-412f-9fbb-7a266ae32370",
                        "name": "Automation",
                        "slug": "automation",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-10 08:59:40",
                        "created_by": 1,
                        "updated_at": "2016-10-19 14:33:42",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 9,
                        "uuid": "2270e5cf-c46c-4136-a340-ea0284ab018a",
                        "name": "Server2016",
                        "slug": "server2016",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-05-10 09:00:13",
                        "created_by": 1,
                        "updated_at": "2016-05-10 09:00:13",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 10,
                        "uuid": "9c3c8b2f-7a1d-457d-91ae-b83901025f9c",
                        "name": "Pester",
                        "slug": "pester",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-06-13 08:25:07",
                        "created_by": 1,
                        "updated_at": "2016-06-13 08:25:07",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 11,
                        "uuid": "31730cd0-d65a-49e0-aa42-c81029a08f3a",
                        "name": "PRTG",
                        "slug": "prtg",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-09-08 21:03:12",
                        "created_by": 1,
                        "updated_at": "2016-09-08 21:03:12",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 13,
                        "uuid": "58ae3002-24b7-48fa-9116-236954b1ee03",
                        "name": "Azure",
                        "slug": "azure",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-11-12 07:21:24",
                        "created_by": 1,
                        "updated_at": "2016-11-12 07:21:24",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 14,
                        "uuid": "f97ff040-6114-4df4-a57a-067e1ffb396e",
                        "name": "Slack",
                        "slug": "slack",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2016-11-12 07:21:24",
                        "created_by": 1,
                        "updated_at": "2016-11-12 07:21:24",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 15,
                        "uuid": "c8c92e8d-3a3c-4db9-9430-3c26a3a679a7",
                        "name": "AppVeyor",
                        "slug": "appveyor",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2017-01-20 13:45:33",
                        "created_by": 1,
                        "updated_at": "2017-01-20 13:45:33",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 16,
                        "uuid": "6bb09d79-4ae2-4c3e-a262-f40e02a51bcc",
                        "name": "SSRS",
                        "slug": "ssrs",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2017-10-04 17:06:32",
                        "created_by": 1,
                        "updated_at": "2017-10-04 17:06:32",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 17,
                        "uuid": "40838834-4e20-4cd7-a423-1931039d408e",
                        "name": "Isilon",
                        "slug": "isilon",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-01-05 21:58:04",
                        "created_by": 1,
                        "updated_at": "2018-01-05 21:58:04",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 18,
                        "uuid": "3ed3b8b0-e6ef-4306-8995-fb4136661577",
                        "name": "Troubleshooting",
                        "slug": "troubleshooting",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-01-05 21:58:04",
                        "created_by": 1,
                        "updated_at": "2018-01-22 15:24:23",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 19,
                        "uuid": "186534ff-47d2-4b52-b9ce-f0e60d803f62",
                        "name": "Puppet",
                        "slug": "puppet",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-01-19 16:43:54",
                        "created_by": 1,
                        "updated_at": "2018-01-22 15:24:17",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 20,
                        "uuid": "2c95ee97-cef4-44cd-b8f6-6a5fdedb1f53",
                        "name": "Influx",
                        "slug": "influx",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-02-19 15:24:04",
                        "created_by": 1,
                        "updated_at": "2018-02-19 15:24:04",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 21,
                        "uuid": "21325c17-4c4f-4328-9f32-7123b8e2fe6a",
                        "name": "Grafana",
                        "slug": "grafana",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-02-19 15:24:04",
                        "created_by": 1,
                        "updated_at": "2018-02-19 15:24:04",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 22,
                        "uuid": "89d58db7-9963-41fb-a912-4b5fa227cc1a",
                        "name": "DevOps",
                        "slug": "devops",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-02-19 15:24:04",
                        "created_by": 1,
                        "updated_at": "2018-02-19 15:24:04",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 23,
                        "uuid": "670d6240-1509-4832-aa27-b21616c9af92",
                        "name": "Chocolatey",
                        "slug": "chocolatey",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-02-28 15:01:17",
                        "created_by": 1,
                        "updated_at": "2018-02-28 15:01:17",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 24,
                        "uuid": "5ac6df46-dd9f-41ca-92d3-a1c8f2c56d69",
                        "name": "Terraform",
                        "slug": "terraform",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2018-10-31 13:54:24",
                        "created_by": 1,
                        "updated_at": "2018-10-31 13:54:24",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 25,
                        "uuid": "b305e795-bc80-4c38-af87-5c91335b0e56",
                        "name": "ARM",
                        "slug": "arm",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2020-08-10 11:12:48",
                        "created_by": 1,
                        "updated_at": "2020-08-10 11:12:48",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 26,
                        "uuid": "66eed6ad-f3b7-4aa3-b59e-466d60e2614e",
                        "name": "IaC",
                        "slug": "iac",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2020-08-10 11:12:48",
                        "created_by": 1,
                        "updated_at": "2020-08-10 11:12:48",
                        "updated_by": 1,
                        "visibility": "public"
                    },
                    {
                        "id": 27,
                        "uuid": "3e7ec169-973d-493e-aee5-3edd04205d1f",
                        "name": "Infrastructure",
                        "slug": "infrastructure",
                        "description": null,
                        "image": null,
                        "parent_id": null,
                        "meta_title": null,
                        "meta_description": null,
                        "created_at": "2020-08-10 11:12:48",
                        "created_by": 1,
                        "updated_at": "2020-08-10 11:12:48",
                        "updated_by": 1,
                        "visibility": "public"
                    }
                ],
                "subscribers": []
            }
        }
    ]
}